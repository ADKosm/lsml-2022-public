{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Алгоритмы на потоках данных\n",
    "\n",
    "На этом семинаре посмотрим на то, что можно сделать с большими данными, когда наши вычислительные возможности несколько ограничены. \n",
    "\n",
    "Ограниченость может быть вызвана различными факторами\n",
    "* У нас просто нет вычислительных ресурсов для hadoop кластера - есть только одна тачка для вычислений с жесткий диском\n",
    "* У нас все таки есть хадуп кластер, но при этом некоторые задачи все равно решаются на нем мучительно долго\n",
    "* У нас есть хадуп кластер, однако данные в огромных количествах прилетают каждую секунду (например сообщения из кафки или логи веб-серверов)\n",
    "* У нас есть только \"умная\" кофеварка и тонны данных для анализа - например мы строим Internet-of-things и хотим встроить какую-то аналитику в систему. В таком раскладе у нас есть небольшое устройство, которое подключено в гигантской сети из таких же устройств, каждое из которых непрерывно шлет показания с датчиков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далеко не любую задачу можно решить за приемлимое время с приемлимым качеством в таких условиях, однако существует целый класс алгоритмов, которые умеют выдавать разумные результаты в подобых сценариях - стриминговые алгоритмы.\n",
    "\n",
    "Основные отличия стриминговых алгоритмов следующие:\n",
    "* Память у алгоритма ограничена и много меньше размера входных данных\n",
    "* Алгоритм может посмотреть на данные только 1 раз"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Датасет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет на сегодняшний семинар - данные с сенсоров \"умного\" города в Денмарке. Сенсоры снимают показания о разруженности дорог по городу и сливают их в единый поток данных.\n",
    "\n",
    "Информация по датасету - http://iot.ee.surrey.ac.uk:8080/datasets.html#traffic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-03-26 12:06:16--  http://iot.ee.surrey.ac.uk:8080/datasets/traffic/traffic_oct_nov/citypulse_traffic_raw_data_aarhus_oct_nov_2014.zip\n",
      "Resolving iot.ee.surrey.ac.uk (iot.ee.surrey.ac.uk)... 131.227.92.114\n",
      "Connecting to iot.ee.surrey.ac.uk (iot.ee.surrey.ac.uk)|131.227.92.114|:8080... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 38520470 (37M) [application/zip]\n",
      "Saving to: ‘citypulse_traffic_raw_data_aarhus_oct_nov_2014.zip’\n",
      "\n",
      "citypulse_traffic_r 100%[===================>]  36.74M  33.7MB/s    in 1.1s    \n",
      "\n",
      "2022-03-26 12:06:17 (33.7 MB/s) - ‘citypulse_traffic_raw_data_aarhus_oct_nov_2014.zip’ saved [38520470/38520470]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget http://iot.ee.surrey.ac.uk:8080/datasets/traffic/traffic_oct_nov/citypulse_traffic_raw_data_aarhus_oct_nov_2014.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  citypulse_traffic_raw_data_aarhus_oct_nov_2014.zip\n",
      "  inflating: dataset/trafficData158324.csv  \n",
      "  inflating: dataset/trafficData158355.csv  \n",
      "  inflating: dataset/trafficData158386.csv  \n",
      "  inflating: dataset/trafficData158415.csv  \n",
      "  inflating: dataset/trafficData158446.csv  \n",
      "  inflating: dataset/trafficData158475.csv  \n",
      "  inflating: dataset/trafficData158505.csv  \n",
      "  inflating: dataset/trafficData158536.csv  \n",
      "  inflating: dataset/trafficData158565.csv  \n",
      "  inflating: dataset/trafficData158595.csv  \n",
      "  inflating: dataset/trafficData158624.csv  \n",
      "  inflating: dataset/trafficData158655.csv  \n",
      "  inflating: dataset/trafficData158684.csv  \n",
      "  inflating: dataset/trafficData158715.csv  \n",
      "  inflating: dataset/trafficData158744.csv  \n",
      "  inflating: dataset/trafficData158776.csv  \n",
      "  inflating: dataset/trafficData158805.csv  \n",
      "  inflating: dataset/trafficData158836.csv  \n",
      "  inflating: dataset/trafficData158865.csv  \n",
      "  inflating: dataset/trafficData158895.csv  \n",
      "  inflating: dataset/trafficData158924.csv  \n",
      "  inflating: dataset/trafficData158954.csv  \n",
      "  inflating: dataset/trafficData158983.csv  \n",
      "  inflating: dataset/trafficData159014.csv  \n",
      "  inflating: dataset/trafficData159043.csv  \n",
      "  inflating: dataset/trafficData171572.csv  \n",
      "  inflating: dataset/trafficData171969.csv  \n",
      "  inflating: dataset/trafficData172156.csv  \n",
      "  inflating: dataset/trafficData172329.csv  \n",
      "  inflating: dataset/trafficData172602.csv  \n",
      "  inflating: dataset/trafficData173011.csv  \n",
      "  inflating: dataset/trafficData173118.csv  \n",
      "  inflating: dataset/trafficData173225.csv  \n",
      "  inflating: dataset/trafficData178548.csv  \n",
      "  inflating: dataset/trafficData178600.csv  \n",
      "  inflating: dataset/trafficData178713.csv  \n",
      "  inflating: dataset/trafficData178739.csv  \n",
      "  inflating: dataset/trafficData178767.csv  \n",
      "  inflating: dataset/trafficData178793.csv  \n",
      "  inflating: dataset/trafficData178821.csv  \n",
      "  inflating: dataset/trafficData178847.csv  \n",
      "  inflating: dataset/trafficData178875.csv  \n",
      "  inflating: dataset/trafficData178901.csv  \n",
      "  inflating: dataset/trafficData178929.csv  \n",
      "  inflating: dataset/trafficData178955.csv  \n",
      "  inflating: dataset/trafficData178983.csv  \n",
      "  inflating: dataset/trafficData179009.csv  \n",
      "  inflating: dataset/trafficData179038.csv  \n",
      "  inflating: dataset/trafficData179064.csv  \n",
      "  inflating: dataset/trafficData179093.csv  \n",
      "  inflating: dataset/trafficData179119.csv  \n",
      "  inflating: dataset/trafficData179148.csv  \n",
      "  inflating: dataset/trafficData179174.csv  \n",
      "  inflating: dataset/trafficData179202.csv  \n",
      "  inflating: dataset/trafficData179228.csv  \n",
      "  inflating: dataset/trafficData179256.csv  \n",
      "  inflating: dataset/trafficData179282.csv  \n",
      "  inflating: dataset/trafficData179310.csv  \n",
      "  inflating: dataset/trafficData179336.csv  \n",
      "  inflating: dataset/trafficData179364.csv  \n",
      "  inflating: dataset/trafficData179390.csv  \n",
      "  inflating: dataset/trafficData179418.csv  \n",
      "  inflating: dataset/trafficData179444.csv  \n",
      "  inflating: dataset/trafficData180547.csv  \n",
      "  inflating: dataset/trafficData180573.csv  \n",
      "  inflating: dataset/trafficData180601.csv  \n",
      "  inflating: dataset/trafficData180627.csv  \n",
      "  inflating: dataset/trafficData180655.csv  \n",
      "  inflating: dataset/trafficData180681.csv  \n",
      "  inflating: dataset/trafficData180709.csv  \n",
      "  inflating: dataset/trafficData180735.csv  \n",
      "  inflating: dataset/trafficData180764.csv  \n",
      "  inflating: dataset/trafficData180790.csv  \n",
      "  inflating: dataset/trafficData180818.csv  \n",
      "  inflating: dataset/trafficData180844.csv  \n",
      "  inflating: dataset/trafficData180872.csv  \n",
      "  inflating: dataset/trafficData180898.csv  \n",
      "  inflating: dataset/trafficData180926.csv  \n",
      "  inflating: dataset/trafficData180952.csv  \n",
      "  inflating: dataset/trafficData180980.csv  \n",
      "  inflating: dataset/trafficData181006.csv  \n",
      "  inflating: dataset/trafficData181034.csv  \n",
      "  inflating: dataset/trafficData181060.csv  \n",
      "  inflating: dataset/trafficData181088.csv  \n",
      "  inflating: dataset/trafficData181114.csv  \n",
      "  inflating: dataset/trafficData181142.csv  \n",
      "  inflating: dataset/trafficData181168.csv  \n",
      "  inflating: dataset/trafficData181197.csv  \n",
      "  inflating: dataset/trafficData181223.csv  \n",
      "  inflating: dataset/trafficData181251.csv  \n",
      "  inflating: dataset/trafficData181277.csv  \n",
      "  inflating: dataset/trafficData181305.csv  \n",
      "  inflating: dataset/trafficData181331.csv  \n",
      "  inflating: dataset/trafficData182657.csv  \n",
      "  inflating: dataset/trafficData182683.csv  \n",
      "  inflating: dataset/trafficData182712.csv  \n",
      "  inflating: dataset/trafficData182738.csv  \n",
      "  inflating: dataset/trafficData182766.csv  \n",
      "  inflating: dataset/trafficData182792.csv  \n",
      "  inflating: dataset/trafficData182820.csv  \n",
      "  inflating: dataset/trafficData182846.csv  \n",
      "  inflating: dataset/trafficData182875.csv  \n",
      "  inflating: dataset/trafficData182901.csv  \n",
      "  inflating: dataset/trafficData182929.csv  \n",
      "  inflating: dataset/trafficData182955.csv  \n",
      "  inflating: dataset/trafficData182983.csv  \n",
      "  inflating: dataset/trafficData183009.csv  \n",
      "  inflating: dataset/trafficData183037.csv  \n",
      "  inflating: dataset/trafficData183063.csv  \n",
      "  inflating: dataset/trafficData183091.csv  \n",
      "  inflating: dataset/trafficData183117.csv  \n",
      "  inflating: dataset/trafficData184595.csv  \n",
      "  inflating: dataset/trafficData184621.csv  \n",
      "  inflating: dataset/trafficData184649.csv  \n",
      "  inflating: dataset/trafficData184675.csv  \n",
      "  inflating: dataset/trafficData184703.csv  \n",
      "  inflating: dataset/trafficData184729.csv  \n",
      "  inflating: dataset/trafficData184758.csv  \n",
      "  inflating: dataset/trafficData184784.csv  \n",
      "  inflating: dataset/trafficData184813.csv  \n",
      "  inflating: dataset/trafficData184839.csv  \n",
      "  inflating: dataset/trafficData184866.csv  \n",
      "  inflating: dataset/trafficData184892.csv  \n",
      "  inflating: dataset/trafficData184919.csv  \n",
      "  inflating: dataset/trafficData184945.csv  \n",
      "  inflating: dataset/trafficData184972.csv  \n",
      "  inflating: dataset/trafficData184998.csv  \n",
      "  inflating: dataset/trafficData185025.csv  \n",
      "  inflating: dataset/trafficData185051.csv  \n",
      "  inflating: dataset/trafficData185078.csv  \n",
      "  inflating: dataset/trafficData185104.csv  \n",
      "  inflating: dataset/trafficData185131.csv  \n",
      "  inflating: dataset/trafficData185157.csv  \n",
      "  inflating: dataset/trafficData185184.csv  \n",
      "  inflating: dataset/trafficData185210.csv  \n",
      "  inflating: dataset/trafficData185237.csv  \n",
      "  inflating: dataset/trafficData185263.csv  \n",
      "  inflating: dataset/trafficData185290.csv  \n",
      "  inflating: dataset/trafficData185316.csv  \n",
      "  inflating: dataset/trafficData185343.csv  \n",
      "  inflating: dataset/trafficData185369.csv  \n",
      "  inflating: dataset/trafficData185396.csv  \n",
      "  inflating: dataset/trafficData185422.csv  \n",
      "  inflating: dataset/trafficData186953.csv  \n",
      "  inflating: dataset/trafficData186979.csv  \n",
      "  inflating: dataset/trafficData187006.csv  \n",
      "  inflating: dataset/trafficData187032.csv  \n",
      "  inflating: dataset/trafficData187059.csv  \n",
      "  inflating: dataset/trafficData187085.csv  \n",
      "  inflating: dataset/trafficData187112.csv  \n",
      "  inflating: dataset/trafficData187138.csv  \n",
      "  inflating: dataset/trafficData187165.csv  \n",
      "  inflating: dataset/trafficData187191.csv  \n",
      "  inflating: dataset/trafficData187218.csv  \n",
      "  inflating: dataset/trafficData187244.csv  \n",
      "  inflating: dataset/trafficData187271.csv  \n",
      "  inflating: dataset/trafficData187297.csv  \n",
      "  inflating: dataset/trafficData187324.csv  \n",
      "  inflating: dataset/trafficData187350.csv  \n",
      "  inflating: dataset/trafficData187377.csv  \n",
      "  inflating: dataset/trafficData187403.csv  \n",
      "  inflating: dataset/trafficData187430.csv  \n",
      "  inflating: dataset/trafficData187456.csv  \n",
      "  inflating: dataset/trafficData187483.csv  \n",
      "  inflating: dataset/trafficData187509.csv  \n",
      "  inflating: dataset/trafficData187536.csv  \n",
      "  inflating: dataset/trafficData187562.csv  \n",
      "  inflating: dataset/trafficData187589.csv  \n",
      "  inflating: dataset/trafficData187615.csv  \n",
      "  inflating: dataset/trafficData187642.csv  \n",
      "  inflating: dataset/trafficData187668.csv  \n",
      "  inflating: dataset/trafficData187695.csv  \n",
      "  inflating: dataset/trafficData187721.csv  \n",
      "  inflating: dataset/trafficData187748.csv  \n",
      "  inflating: dataset/trafficData187774.csv  \n",
      "  inflating: dataset/trafficData187801.csv  \n",
      "  inflating: dataset/trafficData187827.csv  \n",
      "  inflating: dataset/trafficData187854.csv  \n",
      "  inflating: dataset/trafficData187880.csv  \n",
      "  inflating: dataset/trafficData187907.csv  \n",
      "  inflating: dataset/trafficData187933.csv  \n",
      "  inflating: dataset/trafficData187960.csv  \n",
      "  inflating: dataset/trafficData187986.csv  \n",
      "  inflating: dataset/trafficData188013.csv  \n",
      "  inflating: dataset/trafficData188039.csv  \n",
      "  inflating: dataset/trafficData188066.csv  \n",
      "  inflating: dataset/trafficData188092.csv  \n",
      "  inflating: dataset/trafficData188119.csv  \n",
      "  inflating: dataset/trafficData188145.csv  \n",
      "  inflating: dataset/trafficData188172.csv  \n",
      "  inflating: dataset/trafficData188198.csv  \n",
      "  inflating: dataset/trafficData188225.csv  \n",
      "  inflating: dataset/trafficData188251.csv  \n",
      "  inflating: dataset/trafficData189941.csv  \n",
      "  inflating: dataset/trafficData189967.csv  \n",
      "  inflating: dataset/trafficData189994.csv  \n",
      "  inflating: dataset/trafficData190020.csv  \n",
      "  inflating: dataset/trafficData190047.csv  \n",
      "  inflating: dataset/trafficData190073.csv  \n",
      "  inflating: dataset/trafficData190100.csv  \n",
      "  inflating: dataset/trafficData190126.csv  \n",
      "  inflating: dataset/trafficData190153.csv  \n",
      "  inflating: dataset/trafficData190179.csv  \n",
      "  inflating: dataset/trafficData190206.csv  \n",
      "  inflating: dataset/trafficData190232.csv  \n",
      "  inflating: dataset/trafficData190259.csv  \n",
      "  inflating: dataset/trafficData190285.csv  \n",
      "  inflating: dataset/trafficData190312.csv  \n",
      "  inflating: dataset/trafficData190338.csv  \n",
      "  inflating: dataset/trafficData190367.csv  \n",
      "  inflating: dataset/trafficData190393.csv  \n",
      "  inflating: dataset/trafficData190421.csv  \n",
      "  inflating: dataset/trafficData190447.csv  \n",
      "  inflating: dataset/trafficData190475.csv  \n",
      "  inflating: dataset/trafficData190501.csv  \n",
      "  inflating: dataset/trafficData190529.csv  \n",
      "  inflating: dataset/trafficData190555.csv  \n",
      "  inflating: dataset/trafficData190583.csv  \n",
      "  inflating: dataset/trafficData190609.csv  \n",
      "  inflating: dataset/trafficData190637.csv  \n",
      "  inflating: dataset/trafficData190663.csv  \n",
      "  inflating: dataset/trafficData190691.csv  \n",
      "  inflating: dataset/trafficData190717.csv  \n",
      "  inflating: dataset/trafficData190744.csv  \n",
      "  inflating: dataset/trafficData190770.csv  \n",
      "  inflating: dataset/trafficData190799.csv  \n",
      "  inflating: dataset/trafficData190825.csv  \n",
      "  inflating: dataset/trafficData190853.csv  \n",
      "  inflating: dataset/trafficData190879.csv  \n",
      "  inflating: dataset/trafficData190908.csv  \n",
      "  inflating: dataset/trafficData190934.csv  \n",
      "  inflating: dataset/trafficData190963.csv  \n",
      "  inflating: dataset/trafficData192438.csv  \n",
      "  inflating: dataset/trafficData192466.csv  \n",
      "  inflating: dataset/trafficData192492.csv  \n",
      "  inflating: dataset/trafficData192520.csv  \n",
      "  inflating: dataset/trafficData192546.csv  \n",
      "  inflating: dataset/trafficData192574.csv  \n",
      "  inflating: dataset/trafficData192600.csv  \n",
      "  inflating: dataset/trafficData192627.csv  \n",
      "  inflating: dataset/trafficData192653.csv  \n",
      "  inflating: dataset/trafficData192681.csv  \n",
      "  inflating: dataset/trafficData192707.csv  \n",
      "  inflating: dataset/trafficData192734.csv  \n",
      "  inflating: dataset/trafficData192760.csv  \n",
      "  inflating: dataset/trafficData192787.csv  \n",
      "  inflating: dataset/trafficData192813.csv  \n",
      "  inflating: dataset/trafficData192840.csv  \n",
      "  inflating: dataset/trafficData192866.csv  \n",
      "  inflating: dataset/trafficData192893.csv  \n",
      "  inflating: dataset/trafficData192919.csv  \n",
      "  inflating: dataset/trafficData192946.csv  \n",
      "  inflating: dataset/trafficData192972.csv  \n",
      "  inflating: dataset/trafficData193000.csv  \n",
      "  inflating: dataset/trafficData193026.csv  \n",
      "  inflating: dataset/trafficData193053.csv  \n",
      "  inflating: dataset/trafficData193079.csv  \n",
      "  inflating: dataset/trafficData193106.csv  \n",
      "  inflating: dataset/trafficData193132.csv  \n",
      "  inflating: dataset/trafficData193159.csv  \n",
      "  inflating: dataset/trafficData193185.csv  \n",
      "  inflating: dataset/trafficData193213.csv  \n",
      "  inflating: dataset/trafficData193239.csv  \n",
      "  inflating: dataset/trafficData193268.csv  \n",
      "  inflating: dataset/trafficData193294.csv  \n",
      "  inflating: dataset/trafficData193322.csv  \n",
      "  inflating: dataset/trafficData193348.csv  \n",
      "  inflating: dataset/trafficData193376.csv  \n",
      "  inflating: dataset/trafficData193402.csv  \n",
      "  inflating: dataset/trafficData193430.csv  \n",
      "  inflating: dataset/trafficData194878.csv  \n",
      "  inflating: dataset/trafficData194905.csv  \n",
      "  inflating: dataset/trafficData194931.csv  \n",
      "  inflating: dataset/trafficData194960.csv  \n",
      "  inflating: dataset/trafficData194986.csv  \n",
      "  inflating: dataset/trafficData195015.csv  \n",
      "  inflating: dataset/trafficData195041.csv  \n",
      "  inflating: dataset/trafficData195070.csv  \n",
      "  inflating: dataset/trafficData195096.csv  \n",
      "  inflating: dataset/trafficData195124.csv  \n",
      "  inflating: dataset/trafficData195150.csv  \n",
      "  inflating: dataset/trafficData195178.csv  \n",
      "  inflating: dataset/trafficData195204.csv  \n",
      "  inflating: dataset/trafficData195233.csv  \n",
      "  inflating: dataset/trafficData195259.csv  \n",
      "  inflating: dataset/trafficData195286.csv  \n",
      "  inflating: dataset/trafficData195312.csv  \n",
      "  inflating: dataset/trafficData195339.csv  \n",
      "  inflating: dataset/trafficData195365.csv  \n",
      "  inflating: dataset/trafficData195392.csv  \n",
      "  inflating: dataset/trafficData195418.csv  \n",
      "  inflating: dataset/trafficData195446.csv  \n",
      "  inflating: dataset/trafficData195472.csv  \n",
      "  inflating: dataset/trafficData195499.csv  \n",
      "  inflating: dataset/trafficData195525.csv  \n",
      "  inflating: dataset/trafficData195552.csv  \n",
      "  inflating: dataset/trafficData195578.csv  \n",
      "  inflating: dataset/trafficData195605.csv  \n",
      "  inflating: dataset/trafficData195631.csv  \n",
      "  inflating: dataset/trafficData195658.csv  \n",
      "  inflating: dataset/trafficData195684.csv  \n",
      "  inflating: dataset/trafficData195711.csv  \n",
      "  inflating: dataset/trafficData195737.csv  \n",
      "  inflating: dataset/trafficData195764.csv  \n",
      "  inflating: dataset/trafficData195790.csv  \n",
      "  inflating: dataset/trafficData195817.csv  \n",
      "  inflating: dataset/trafficData195843.csv  \n",
      "  inflating: dataset/trafficData195870.csv  \n",
      "  inflating: dataset/trafficData195896.csv  \n",
      "  inflating: dataset/trafficData195923.csv  \n",
      "  inflating: dataset/trafficData197274.csv  \n",
      "  inflating: dataset/trafficData197302.csv  \n",
      "  inflating: dataset/trafficData197328.csv  \n",
      "  inflating: dataset/trafficData197355.csv  \n",
      "  inflating: dataset/trafficData197381.csv  \n",
      "  inflating: dataset/trafficData197408.csv  \n",
      "  inflating: dataset/trafficData197434.csv  \n",
      "  inflating: dataset/trafficData197463.csv  \n",
      "  inflating: dataset/trafficData197489.csv  \n",
      "  inflating: dataset/trafficData197518.csv  \n",
      "  inflating: dataset/trafficData197544.csv  \n",
      "  inflating: dataset/trafficData197572.csv  \n",
      "  inflating: dataset/trafficData197598.csv  \n",
      "  inflating: dataset/trafficData197626.csv  \n",
      "  inflating: dataset/trafficData197652.csv  \n",
      "  inflating: dataset/trafficData197679.csv  \n",
      "  inflating: dataset/trafficData197705.csv  \n",
      "  inflating: dataset/trafficData197734.csv  \n",
      "  inflating: dataset/trafficData197760.csv  \n",
      "  inflating: dataset/trafficData197788.csv  \n",
      "  inflating: dataset/trafficData197814.csv  \n",
      "  inflating: dataset/trafficData197842.csv  \n",
      "  inflating: dataset/trafficData197868.csv  \n",
      "  inflating: dataset/trafficData197896.csv  \n",
      "  inflating: dataset/trafficData197922.csv  \n",
      "  inflating: dataset/trafficData197951.csv  \n",
      "  inflating: dataset/trafficData197977.csv  \n",
      "  inflating: dataset/trafficData198005.csv  \n",
      "  inflating: dataset/trafficData198031.csv  \n",
      "  inflating: dataset/trafficData198059.csv  \n",
      "  inflating: dataset/trafficData198085.csv  \n",
      "  inflating: dataset/trafficData198113.csv  \n",
      "  inflating: dataset/trafficData198139.csv  \n",
      "  inflating: dataset/trafficData198167.csv  \n",
      "  inflating: dataset/trafficData198193.csv  \n",
      "  inflating: dataset/trafficData198221.csv  \n",
      "  inflating: dataset/trafficData198247.csv  \n",
      "  inflating: dataset/trafficData198275.csv  \n",
      "  inflating: dataset/trafficData198301.csv  \n",
      "  inflating: dataset/trafficData198330.csv  \n",
      "  inflating: dataset/trafficData201183.csv  \n",
      "  inflating: dataset/trafficData201211.csv  \n",
      "  inflating: dataset/trafficData201237.csv  \n",
      "  inflating: dataset/trafficData201265.csv  \n",
      "  inflating: dataset/trafficData201291.csv  \n",
      "  inflating: dataset/trafficData201319.csv  \n",
      "  inflating: dataset/trafficData201345.csv  \n",
      "  inflating: dataset/trafficData201373.csv  \n",
      "  inflating: dataset/trafficData201399.csv  \n",
      "  inflating: dataset/trafficData201427.csv  \n",
      "  inflating: dataset/trafficData201453.csv  \n",
      "  inflating: dataset/trafficData201481.csv  \n",
      "  inflating: dataset/trafficData201507.csv  \n",
      "  inflating: dataset/trafficData201535.csv  \n",
      "  inflating: dataset/trafficData201561.csv  \n",
      "  inflating: dataset/trafficData201589.csv  \n",
      "  inflating: dataset/trafficData201615.csv  \n",
      "  inflating: dataset/trafficData201643.csv  \n",
      "  inflating: dataset/trafficData201669.csv  \n",
      "  inflating: dataset/trafficData201696.csv  \n",
      "  inflating: dataset/trafficData201722.csv  \n",
      "  inflating: dataset/trafficData201749.csv  \n",
      "  inflating: dataset/trafficData201775.csv  \n",
      "  inflating: dataset/trafficData201802.csv  \n",
      "  inflating: dataset/trafficData201828.csv  \n",
      "  inflating: dataset/trafficData201855.csv  \n",
      "  inflating: dataset/trafficData201881.csv  \n",
      "  inflating: dataset/trafficData201908.csv  \n",
      "  inflating: dataset/trafficData201934.csv  \n",
      "  inflating: dataset/trafficData201961.csv  \n",
      "  inflating: dataset/trafficData203530.csv  \n",
      "  inflating: dataset/trafficData203557.csv  \n",
      "  inflating: dataset/trafficData203583.csv  \n",
      "  inflating: dataset/trafficData203610.csv  \n",
      "  inflating: dataset/trafficData203636.csv  \n",
      "  inflating: dataset/trafficData203663.csv  \n",
      "  inflating: dataset/trafficData203689.csv  \n",
      "  inflating: dataset/trafficData203716.csv  \n",
      "  inflating: dataset/trafficData203742.csv  \n",
      "  inflating: dataset/trafficData203769.csv  \n",
      "  inflating: dataset/trafficData203795.csv  \n",
      "  inflating: dataset/trafficData203822.csv  \n",
      "  inflating: dataset/trafficData203848.csv  \n",
      "  inflating: dataset/trafficData203875.csv  \n",
      "  inflating: dataset/trafficData203901.csv  \n",
      "  inflating: dataset/trafficData203928.csv  \n",
      "  inflating: dataset/trafficData203954.csv  \n",
      "  inflating: dataset/trafficData203981.csv  \n",
      "  inflating: dataset/trafficData204007.csv  \n",
      "  inflating: dataset/trafficData204034.csv  \n",
      "  inflating: dataset/trafficData204060.csv  \n",
      "  inflating: dataset/trafficData204087.csv  \n",
      "  inflating: dataset/trafficData204113.csv  \n",
      "  inflating: dataset/trafficData204140.csv  \n",
      "  inflating: dataset/trafficData204166.csv  \n",
      "  inflating: dataset/trafficData204193.csv  \n",
      "  inflating: dataset/trafficData204219.csv  \n",
      "  inflating: dataset/trafficData204247.csv  \n",
      "  inflating: dataset/trafficData204273.csv  \n",
      "  inflating: dataset/trafficData204300.csv  \n",
      "  inflating: dataset/trafficData205997.csv  \n",
      "  inflating: dataset/trafficData206025.csv  \n",
      "  inflating: dataset/trafficData206051.csv  \n",
      "  inflating: dataset/trafficData206078.csv  \n",
      "  inflating: dataset/trafficData206104.csv  \n",
      "  inflating: dataset/trafficData206131.csv  \n",
      "  inflating: dataset/trafficData206157.csv  \n",
      "  inflating: dataset/trafficData206184.csv  \n",
      "  inflating: dataset/trafficData206210.csv  \n",
      "  inflating: dataset/trafficData206237.csv  \n",
      "  inflating: dataset/trafficData206263.csv  \n",
      "  inflating: dataset/trafficData206290.csv  \n",
      "  inflating: dataset/trafficData206316.csv  \n",
      "  inflating: dataset/trafficData206343.csv  \n",
      "  inflating: dataset/trafficData206369.csv  \n",
      "  inflating: dataset/trafficData206396.csv  \n",
      "  inflating: dataset/trafficData206422.csv  \n",
      "  inflating: dataset/trafficData206449.csv  \n",
      "  inflating: dataset/trafficData206475.csv  \n",
      "  inflating: dataset/trafficData206502.csv  \n",
      "  inflating: dataset/trafficData209721.csv  \n",
      "  inflating: dataset/trafficData209748.csv  \n",
      "  inflating: dataset/trafficData209774.csv  \n",
      "  inflating: dataset/trafficData209801.csv  \n",
      "  inflating: dataset/trafficData209827.csv  \n",
      "  inflating: dataset/trafficData209854.csv  \n",
      "  inflating: dataset/trafficData209880.csv  \n",
      "  inflating: dataset/trafficData209907.csv  \n",
      "  inflating: dataset/trafficData209933.csv  \n",
      "  inflating: dataset/trafficData209960.csv  \n",
      "  inflating: dataset/trafficData209986.csv  \n",
      "  inflating: dataset/trafficData210013.csv  \n",
      "  inflating: dataset/trafficData210040.csv  \n",
      "  inflating: dataset/trafficData210067.csv  \n",
      "  inflating: dataset/trafficData210093.csv  \n",
      "  inflating: dataset/trafficData210120.csv  \n",
      "  inflating: dataset/trafficData210146.csv  \n",
      "  inflating: dataset/trafficData210173.csv  \n",
      "  inflating: dataset/trafficData210199.csv  \n"
     ]
    }
   ],
   "source": [
    "! unzip citypulse_traffic_raw_data_aarhus_oct_nov_2014.zip -d dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sed -i '1d' dataset/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat dataset/* > traffic.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4382599\r\n"
     ]
    }
   ],
   "source": [
    "! cat traffic.csv | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! head traffic.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Колонки датасета следующие\n",
    "```\n",
    "status\tavgMeasuredTime\tavgSpeed\textID\tmedianMeasuredTime\tTIMESTAMP\tvehicleCount\t_id\tREPORT_ID\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Простые статистики"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Среднее"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одним из самых простых стриминговых алгоритмов, который писал скорее всего каждый - это подсчет среднего значения набора чисел. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mean-stream.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mean-stream.py\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "stream = map(lambda x: int(x[6]), csv.reader(iter(sys.stdin.readline, '')))\n",
    "\n",
    "vehicle_count = 0\n",
    "record_count = 0\n",
    "\n",
    "for current_vehicle_count in stream:\n",
    "    vehicle_count += current_vehicle_count\n",
    "    record_count += 1\n",
    "\n",
    "print(vehicle_count / record_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что алгоритм использует O(1) памяти (бкувально две переменные) и проходится по всем данных ровно один раз. \n",
    "Это практически эталонный пример того, как структурно выглядит стриминговый алгоритм и далее мы будем говорить именно о подобных алгоритмах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 4382599/4382599 [00:08<00:00, 520125.65it/s]\n",
      "3.0282777411303203\n"
     ]
    }
   ],
   "source": [
    "! cat traffic.csv | tqdm --total 4382599 | python3 mean-stream.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно заметить, что мы решили эту задачу точно (не приближенно). Аналогично мы можем посчитать и другие несложные статистики - количество, минимум, максимум, дисперсию и так далее.\n",
    "\n",
    "Однако не все статистики считаются с такой легкостью. Например есть большие проблемы с подсчетом медианы в один проход. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сложные статистики"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Медиана"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже предлагается к ознакомлению алгоритм для поиска медианы (Мунро-Патерсон).\n",
    "\n",
    "Идея крайне простая - возьмем T первых элементов из потока. Далее для всех следующих элементов будем подсчитывать, сколько из этих элементов больше (по значению), чем элементы из нашего множества и сколько элементов меньше. Если в конце окажется так, что и тех и тех (больших и меньших) элементов меньше, чем половина всех элементов в потоке (< N/2), то это означает, что наше множество содержит элементы как раз из середины упорядоченного ряда. А раз так, значит медиана - один из элементов нашего множества. Достаточно будет отсортировать наше множество и взять соответствующий элемент.\n",
    "\n",
    "Важно отметить, что будут элементы, которые не будут больше или меньше всех элементов нашего множества - они будут где-то между. В этот момент мы просто включим этот элемент в наше множество. Однако так как память у нас ограничена, то мы должны выкинуть какой-то элемент из множества, чтобы расход памяти не увеличивался. Легко заметить, что мы можем избавиться от минимального или максимального элемента нашего множества - если выкидываем максимум, то просто говорим, что на 1 увеличилось число элементов, больших чем наше (симметрично с минимумом). \n",
    "\n",
    "Осталось решить что выкинуть - минимум или максимум. Так как в конце мы бы хотели, чтобы больших и меньших элементов было примерно поровну, то тогда будем выкидывать элемент в соответствии с этим желанием - если меньших меньше, то выкидываем минимум, если больших - максимум.\n",
    "\n",
    "Более формальное описание алгоритма смотри здесь - https://www.cs.dartmouth.edu/~ac/Teach/data-streams-lecnotes.pdf\n",
    "\n",
    "Ниже - упрощенный схематичный пример работы алгоритма."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Munro-Paterson](https://github.com/ADKosm/lsml-seminars-2020-public/blob/master/img/munro-paterson.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing median-stream.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile median-stream.py\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "from itertools import chain\n",
    "\n",
    "MEMORY = 5000\n",
    "A = {}\n",
    "\n",
    "def add(element, number):\n",
    "    A[element] = A.get(element, 0) + number\n",
    "    if A[element] == 0:\n",
    "        A.pop(element)\n",
    "\n",
    "\n",
    "if len(sys.argv) > 1:\n",
    "    col = int(sys.argv[1])\n",
    "else:\n",
    "    col = 6\n",
    "    \n",
    "stream = map(lambda x: int(x[col]), csv.reader(iter(sys.stdin.readline, '')))\n",
    "\n",
    "for _ in range(MEMORY):\n",
    "    add(next(stream), 1)\n",
    "\n",
    "A_min = min(A)\n",
    "A_max = max(A)\n",
    "\n",
    "larger = 0\n",
    "less = 0\n",
    "N = len(A)\n",
    "\n",
    "for element in stream:\n",
    "    N += 1\n",
    "    if element > A_max:\n",
    "        larger += 1\n",
    "    elif element < A_min:\n",
    "        less += 1\n",
    "    else:\n",
    "        if less < larger:\n",
    "            add(A_min, -1)\n",
    "            add(element, +1)\n",
    "            A_min = min(A)\n",
    "            less += 1\n",
    "        else:\n",
    "            add(A_max, -1)\n",
    "            add(element, +1)\n",
    "            A_max = max(A)\n",
    "            larger += 1\n",
    "\n",
    "if less < N / 2 and larger < N / 2:\n",
    "    median_index = N // 2 - less\n",
    "#     A = sorted(list(A))\n",
    "    A = list(chain.from_iterable([[value] * count for value, count in A.items()]))\n",
    "    result = A[median_index]\n",
    "    print(result)\n",
    "else:\n",
    "    print(\"FAIL\")\n",
    "    print(N, less, larger, A_min, A_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 4382599/4382599 [00:09<00:00, 481414.07it/s]\n",
      "FAIL\n",
      "4377630 3051379 1326220 3 3\n"
     ]
    }
   ],
   "source": [
    "! cat traffic.csv | tqdm --total 4382599 | python3 median-stream.py 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 4382599/4382599 [00:10<00:00, 418702.63it/s]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "! cat traffic.csv | shuf | tqdm --total 4382599 | python3 median-stream.py 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 4382599/4382599 [00:10<00:00, 417793.46it/s]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "! cat traffic.csv | shuf | tqdm --total 4382599 | python3 median-stream.py 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 4382599/4382599 [00:08<00:00, 497303.89it/s]\n",
      "FAIL\n",
      "4377748 1560787 2816812 64 64\n"
     ]
    }
   ],
   "source": [
    "! cat traffic.csv | tqdm --total 4382599 | python3 median-stream.py 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 4382599/4382599 [00:10<00:00, 435877.34it/s]\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "! cat traffic.csv | shuf | tqdm --total 4382599 | python3 median-stream.py 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно заметить, что алгоритм работает только на потоках, которые хорошо перемешаны. Если же нам с данными не повезет, то все пропало."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно ли находить медиану всегда с ограничением по памяти? Кажется, что эта задача близка к невыполнимой. Есть продвинутые алгоримты, которые всегда выдают ответ, однако их результат - лишь статистическая оценка, а не точный ответ. \n",
    "\n",
    "Например алгоритм **P-Square**. Он позволяет оценивать любые квантили на потоке. Подробнее можно почитать про него в оригинальной статье - https://www.cse.wustl.edu/~jain/papers/ftp/psqr.pdf . Сейчас останавливаться на нем мы не будем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Скетчи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Принадлежность множеству"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо подсчета каких-то статистик, часто возникает задача построить структуру данных, которая бы смогла отвечать нам на какие-то запросы после обработки потока.\n",
    "\n",
    "Например - **присутствовал ли такой элемент в потоке.**\n",
    "\n",
    "Если бы у нас было O(N) памяти, то тогда эту задачу можно было бы решить честно. Однако нам такой расклад не подходит, поэтому будем строить алгоритм, который отвечает на вопрос правильно с некоторой информацией, но при этом используя гораздо меньше памяти.\n",
    "\n",
    "Здесь нам могут помочь хеш-функции. Самая простая идея, которая нам может прийти в голову - запоминать не сами увиденные значения, а их хеши.\n",
    "\n",
    "План действий следующий: заведем массив размера T - здесь будем отмечать элементы, которые мы видели.\n",
    "Будем хешировать все элементы, которые есть в потоке, в отрезок [0, T] (T выбирается исходя из размера доступной памяти) и отмечать соответствующий элемент в нашем массиве как увиденный. После того, как мы обработаем таким образом весь массив мы можем обабатывать входящие запросы.\n",
    "\n",
    "Для входящего запроса посчитаем хеш элемента, который нас спросили и проверим, есть ли он у нас в массиве.\n",
    "Если в массиве указано, что такой хеш мы не видели, значит и сам элемент мы точно не видели - ответ нет.\n",
    "Если же указано, что видели - тогда возможно, что такой элемент присутствовал в потоке, а может и нет. Такое может произойти, когда хеш другого элемента из потока совпал с хешом элемента, про который спросили. В данной ситуации мы отвечаем, что видели, однако нужно держать в голове, что этот ответ может быть неверным с некоторой вероятностью.\n",
    "\n",
    "Для того, чтобы уменьшить вероятность ошибки, мы можешь применить следующий трюк - возьмем сразу P различных случайных хеш-функций. Будем отмечать в нашем массиве сразу все значения хешей, как увиденные.\n",
    "\n",
    "При ответе также возьмем все P хешей от элемента. Если хотя бы один из значений хеша отсутствует в нашем массиве, то значит такой элемент мы не видели.\n",
    "Если же все хеши присутствуют в массиве, значит или мы видели этот элемент, или нам очень не повезло и у нас случилось сразу P коллизий (что менее вероятно, чем в первом случае в одной хеш функцией).\n",
    "\n",
    "Структура данных, которую мы только что описали, называется **Bloom Filter**.\n",
    "\n",
    "Более подробное описание можно посмотреть здесь - https://shodhganga.inflibnet.ac.in/bitstream/10603/11703/9/09_chapter%204.pdf\n",
    "В статье также можно найти псевдокод работы алгоритма.\n",
    "\n",
    "Ниже - схема принципа работы струткуры.\n",
    "\n",
    "![Bloom-filter](https://github.com/ADKosm/lsml-seminars-2020-public/blob/master/img/bloom-filter.png?raw=true)\n",
    "\n",
    "<sub><sup>Картинка взята из https://en.wikipedia.org/wiki/Bloom_filter</sup></sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Может возникнуть справедливый вопрос - где взять много случайных хеш-функций. Самый простой вариант - взять в качестве хеш-функции парамеризуемую функцию и случайно выставлять параметры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_hash(size):\n",
    "    p1, p2, p3 = random.randint(10, 10**8), random.randint(10, 10**8), random.randint(10, 10**8)\n",
    "    \n",
    "    def _hash(value):\n",
    "        return (p1 + value * p2 + value ** 2 * p3) % size\n",
    "    \n",
    "    return _hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = generate_hash(20)\n",
    "h2 = generate_hash(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "print(h1(10))\n",
    "print(h2(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(h1(10))\n",
    "print(h1(21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T19:35:21.845349Z",
     "start_time": "2020-02-19T19:35:21.724412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing bloom-filter.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile bloom-filter.py\n",
    "import argparse\n",
    "import csv\n",
    "import random\n",
    "import sys\n",
    "\n",
    "\n",
    "MEMORY = 5000\n",
    "HASHES_COUNT = 5\n",
    "\n",
    "\n",
    "def generate_hash(size, seed=None):\n",
    "    random.seed(seed)\n",
    "    p1, p2, p3 = random.randint(10, 10**8), random.randint(10, 10**8), random.randint(10, 10**8)\n",
    "\n",
    "    def _hash(value):\n",
    "        return (p1 + value * p2 + value ** 2 * p3) % size\n",
    "\n",
    "    return _hash\n",
    "\n",
    "\n",
    "def main(query_file_path):\n",
    "    stream = map(lambda x: int(x[8]), csv.reader(iter(sys.stdin.readline, '')))\n",
    "\n",
    "    bit_array = [False]*MEMORY\n",
    "    hash_funcs = [generate_hash(MEMORY, i) for i in range(HASHES_COUNT)]\n",
    "\n",
    "    for element in stream:\n",
    "        for h in hash_funcs:\n",
    "            bit_array[h(element)] = True\n",
    "\n",
    "    with open(query_file_path, \"r\") as f:\n",
    "        for query in map(int, f):\n",
    "            if all(bit_array[h(query)] for h in hash_funcs):\n",
    "                print('YES')\n",
    "            else:\n",
    "                print('NO')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Process some integers.')\n",
    "    parser.add_argument('queries', metavar='query', type=str, nargs='?',\n",
    "                        help='path to queries file')\n",
    "    args = parser.parse_args()\n",
    "    main(args.queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing bloom-filter-query.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile bloom-filter-query.txt\n",
    "158324\n",
    "203546\n",
    "158776\n",
    "23\n",
    "894\n",
    "180926\n",
    "182984\n",
    "81511"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 4382599/4382599 [00:18<00:00, 239549.83it/s]\n",
      "YES\n",
      "NO\n",
      "YES\n",
      "NO\n",
      "NO\n",
      "YES\n",
      "NO\n",
      "NO\n"
     ]
    }
   ],
   "source": [
    "! cat traffic.csv | tqdm --total 4382599 | python3 bloom-filter.py bloom-filter-query.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подсчет частоты "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададимся немного более сложным вопросом - **сколько раз заданный элемент встречался в потоке.**\n",
    "\n",
    "Как и с просто проверкой наличия элемента в потоке - эту задачу не получится решать честно в заданных условиях. Ответ опять будет примерный.\n",
    "\n",
    "Попробуем продолжить идею использования хешей для решения этой задачи. Опять возьмем массив размера T в который будем записывать, сколько раз мы увидели тот или иной хеш. Для каждого нового элемента считаем хеш и увеличиваем соответствующий счетчик в массиве.\n",
    "\n",
    "Когда нам придет запрос - посчитаем хеш и посмотрим в массив. Число будет скорее всего больше, чем правильный ответ, так как из-за коллизий в соответствующую ячейку добавились результаты от элементов, которые имеют одинаковый хеш.\n",
    "\n",
    "Для того, чтобы уменьшить масштаб трагедии из-за этих коллизий опять воспользуемся приемом с несколькими хеш-функциями. Возьмем теперь сразу несколько массивов и для каждого массива возьмем свою случайную хеш-функцию.\n",
    "Для каждого массива будем проделывать такие же операции.\n",
    "\n",
    "Теперь когда к нам придет запрос - посчитаем всех хеши от элемента и посмотрим во все соответствующие ячейки в массивах. Все эти значения очевидно не меньше чем правильный ответ, а значит минимум из этих чисел - наиболее точная оценка того, сколько на самом деле раз мы видели этот элемент в потоке. Его и дадим в качестве ответа.\n",
    "\n",
    "Та конструкция, которую мы только что построили, называется **Count Min Sketch**.\n",
    "\n",
    "Более подробное описание можно посмотреть здесь - http://resources.mpi-inf.mpg.de/departments/d1/teaching/ss13/gitcs/lecture5.pdf.\n",
    "\n",
    "Ниже - схема принципа работы структуры.\n",
    "\n",
    "![count-min-sketch](https://raw.githubusercontent.com/ADKosm/lsml-seminars-2020-public/master/img/count-min-sketch.png)\n",
    "\n",
    "<sub><sup>Картика взята из https://github.com/gopalkrushnapattanaik/SystemDesign/wiki/Count-Min-Sketch</sup></sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T19:35:32.237040Z",
     "start_time": "2020-02-19T19:35:32.119815Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing count-min-sketch.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile count-min-sketch.py\n",
    "import argparse\n",
    "import csv\n",
    "import random\n",
    "import sys\n",
    "\n",
    "TABLE_SIZE = 5000\n",
    "HASHES_COUNT = 10\n",
    "\n",
    "\n",
    "def generate_hash(size, seed=None):\n",
    "    random.seed(seed)\n",
    "    p1, p2, p3 = random.randint(10, 10**8), random.randint(10, 10**8), random.randint(10, 10**8)\n",
    "\n",
    "    def _hash(value):\n",
    "        return (p1 + value * p2 + value ** 2 * p3) % size\n",
    "\n",
    "    return _hash\n",
    "\n",
    "\n",
    "def main(query_file_path):\n",
    "    stream = map(lambda x: int(x[8]), csv.reader(iter(sys.stdin.readline, '')))\n",
    "\n",
    "    cnt_table = [[0 for _ in range(TABLE_SIZE)] for _ in range(HASHES_COUNT)]\n",
    "    hash_funcs = [generate_hash(TABLE_SIZE, i) for i in range(HASHES_COUNT)]\n",
    "\n",
    "    for element in stream:\n",
    "        for i, h in enumerate(hash_funcs):\n",
    "            cnt_table[i][h(element)] = cnt_table[i][h(element)] + 1\n",
    "\n",
    "    with open(query_file_path, \"r\") as f:\n",
    "        for query in map(int, f):\n",
    "            query_cnt = min(cnt_table[i][h(query)] for i, h in enumerate(hash_funcs))\n",
    "            print(query_cnt)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Process some integers.')\n",
    "    parser.add_argument('queries', metavar='query', type=str, nargs='?',\n",
    "                        help='path to queries file')\n",
    "    args = parser.parse_args()\n",
    "    main(args.queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing count-min-sketch-query.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile count-min-sketch-query.txt\n",
    "158324\n",
    "203546\n",
    "158776\n",
    "23\n",
    "894\n",
    "180926\n",
    "182984\n",
    "81511\n",
    "187774\n",
    "201855\n",
    "190100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 4382599/4382599 [00:55<00:00, 78897.72it/s]\n",
      "9305\n",
      "0\n",
      "9958\n",
      "0\n",
      "0\n",
      "9919\n",
      "0\n",
      "0\n",
      "9993\n",
      "9756\n",
      "9451\n"
     ]
    }
   ],
   "source": [
    "! cat traffic.csv | tqdm --total 4382599 | python3 count-min-sketch.py count-min-sketch-query.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Количество уникальных элементов в потоке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одной из важный задач является подсчет **количества уникальных элементов в потоке.** В этот раз так просто составить табличку и просто хешировать в нее не получится. \n",
    "\n",
    "Самыми продвинутыми алгоритмами в этом классе считаются LogLog алгоритмы (LogLog, HyperLogLog, HLL++ и другие). Про них есть много литературы (например http://algo.inria.fr/flajolet/Publications/FlMa85.pdf ), но они используют зубодробительный тервер и разбирать их здесь мы не будем. Но важно знать, что такие алгоритмы есть и понимать, какую задачу они решают. \n",
    "\n",
    "На семинаре предлагается рассмотреть более простой, но все еще работающий способ решения задачи.\n",
    "\n",
    "Давайте посмотрим на двоичную запись хеша от элемента."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0b1100111\n",
      "0b10000100101010\n",
      "0b100101001\n",
      "0b1010110010100\n",
      "0b1000001001011\n",
      "0b1100001011110\n",
      "0b11010111101\n",
      "0b1001111000\n",
      "0b101110001111\n",
      "0b10001000000010\n"
     ]
    }
   ],
   "source": [
    "hash_function = generate_hash(10**4)\n",
    "\n",
    "for number in range(10):\n",
    "    print(bin(hash_function(number)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим на количество нулей в конце двоичной записи.\n",
    "Если хеш случайный, то вероятность того, что на конце будет 1 = 1/2. \n",
    "Вероятность того, что на конце будет 10 = 1/4. 100 - 1/8 и так далее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0b1100111 0\n",
      "0b10000100101010 1\n",
      "0b100101001 0\n",
      "0b1010110010100 2\n",
      "0b1000001001011 0\n",
      "0b1100001011110 1\n",
      "0b11010111101 0\n",
      "0b1001111000 3\n",
      "0b101110001111 0\n",
      "0b10001000000010 1\n"
     ]
    }
   ],
   "source": [
    "def zeros(number):\n",
    "    result = 0\n",
    "    while number and number & 1 == 0:\n",
    "        result += 1\n",
    "        number = number >> 1\n",
    "    return result\n",
    "\n",
    "for number in range(10):\n",
    "    print(bin(hash_function(number)), zeros(hash_function(number)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это так же означает, что в множестве элементов примерно половина будет с нулем 0 на конце их хеш-функции, примерно четверть с одним 0 на конце, восьмая часть с двумя 0 на конце и тд. Этот факт нас очень скоро потребуется.\n",
    "\n",
    "Итак, если бы у нас было неограниченно памяти, мы бы в таком случае просто складывали все элементы из потока в множество и в конце просто бы посмотрели на размер этого множества - это был бы честный ответ в данной задаче.\n",
    "\n",
    "Учитывая, что память у нас ограничена, нам придется каким-то образом ужимать это множество. План следующий:\n",
    "\n",
    "* Возмем пустое множество B и отдельный счетчик z = 0\n",
    "* Для входящих элементов из потока будет добавлять в множество этот элемент.\n",
    "* Как только мы увидим, что размер множества превзошел определенный порог (лимит по памяти) производим следующую операцию\n",
    "  * Из множества удаляем все элементы у которых ровно z нулей на конце хеша (в первый раз будет 0 нулей, то есть те, у которых хеш оканчивается на 1)\n",
    "  * Увеличиваем z на 1\n",
    "* Далее все следующие элементы добавляем в множество, только если количество нулей на конце хеша не меньше z. \n",
    "* Как только в следующий раз у нас опять множество \"переполняется\", вновь повторяем процедуру с очисткой множества и увеличения z\n",
    "* В конце необходимо по нашему получившемуся множеству и z восстановить, сколько же различных элементов мы видели\n",
    "\n",
    "Каждая операция очистки множества удаляла из него примерно половину элементов (это мы увидели выше). Таким образом, исходное количество различных элементов в множестве будет равно примерно |B| * 2^z.\n",
    "\n",
    "Это и будет нашим ответом.\n",
    "\n",
    "Описанный алгоритм называется BJKST.\n",
    "\n",
    "Более подробное описание можно посмотреть здесь - http://resources.mpi-inf.mpg.de/departments/d1/teaching/ss13/gitcs/lecture5.pdf. В статье также можно найти псевдокод работы алгоритма.\n",
    "\n",
    "Ниже - схема принципа работы струткуры.\n",
    "\n",
    "![bjkst](https://github.com/ADKosm/lsml-seminars-2020-public/blob/master/img/bjkst.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T19:35:43.579368Z",
     "start_time": "2020-02-19T19:35:43.462118Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing bjkst.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile bjkst.py\n",
    "import csv\n",
    "import random\n",
    "import sys\n",
    "\n",
    "\n",
    "MEMORY = 100\n",
    "\n",
    "\n",
    "def generate_hash(size, seed=None):\n",
    "    random.seed(seed)\n",
    "    p1, p2, p3 = random.randint(10, 10**8), random.randint(10, 10**8), random.randint(10, 10**8)\n",
    "\n",
    "    def _hash(value):\n",
    "        return (p1 + value * p2 + value ** 2 * p3) % size\n",
    "\n",
    "    return _hash\n",
    "\n",
    "\n",
    "def zeros(number):\n",
    "    result = 0\n",
    "    while number and number & 1 == 0:\n",
    "        result += 1\n",
    "        number = number >> 1\n",
    "    return result\n",
    "\n",
    "\n",
    "def main():\n",
    "    stream = map(lambda x: int(x[8]), csv.reader(iter(sys.stdin.readline, '')))\n",
    "\n",
    "    z = 0\n",
    "    B = set()\n",
    "\n",
    "    for element in stream:\n",
    "        if len(B) > MEMORY:\n",
    "            B = {b for b in B if zeros(b) == z}\n",
    "            z += 1\n",
    "\n",
    "        if zeros(element) >= z:\n",
    "            B.add(element)\n",
    "\n",
    "    result = len(B) * 2**z\n",
    "    print(result)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 4382599/4382599 [00:09<00:00, 477975.46it/s]\n",
      "312\n"
     ]
    }
   ],
   "source": [
    "! cat traffic.csv | tqdm --total 4382599 | python3 bjkst.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
