{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop and MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пара слов про формат работы с кластером\n",
    "\n",
    "Яндекс.Облако предлагает сервис Data Sphere как удобный способ работы с ноутбуками и в том числе с кластером. Это действительно удобно, если для вам уже все-все приготовили, однако мы с вами сейчас в позиции Full Stack Data Engineer и поэтому нам ноутбука из Data Sphere будет не достаточно - нам потребуется доступ до головной машины, чтобы исполнять bash команды на ней.\n",
    "\n",
    "Это можно делать в простом формате - вы открываете терминал и браузер одновременно и переключаетесь между ними по мере необходимости.\n",
    "\n",
    "Я же предлагаю сделать более удобное \"рабочее место\", потратив 5 минут на его настройку. Организуем себе доступ до Jupyter прямо с головной машины!\n",
    "\n",
    "#### Организуем запуск Jupyter на головной машине\n",
    "\n",
    "Подключимся и установим сам Jupyter.\n",
    "\n",
    "```bash\n",
    "ssh lsml-head\n",
    "pip3 install notebook\n",
    "```\n",
    "\n",
    "Кластер мы будем время от времени включать/выключать, поэтому сразу настроим Jupyter на то, чтобы он включался при старте машины. Для этого воспользуемся инструментом `systemd`.\n",
    "\n",
    "Для этого создаем файл в специальной директории с описанием параметров запуска\n",
    "\n",
    "```bash \n",
    "sudo nano /etc/systemd/system/jupyter.service\n",
    "```\n",
    "\n",
    "Вставляем туда нашу конфигурацию\n",
    "\n",
    "```\n",
    "[Unit]\n",
    "After=network.service\n",
    "\n",
    "[Service]\n",
    "ExecStart=/opt/conda/bin/jupyter notebook --no-browser --ip 0.0.0.0 --port 8888 --NotebookApp.token=\"\" --notebook-dir=/home/ubuntu\n",
    "User=ubuntu\n",
    "\n",
    "[Install]\n",
    "WantedBy=default.target\n",
    "```\n",
    "\n",
    "`Ctrl+O`, `Ctrl+X` для того чтобы сохранить и выйти из `nano`\n",
    "\n",
    "Проверим, что точно записалось - `cat /etc/systemd/system/jupyter.service`\n",
    "\n",
    "Активируем и стартуем наш сервис\n",
    "\n",
    "```bash\n",
    "sudo systemctl enable jupyter.service\n",
    "sudo systemctl start jupyter.service\n",
    "```\n",
    "\n",
    "Отлично, Jupyter стартанул! Можно заметить, что мы запустили его без всякой авторизации, но это нормально, потому что головная машина не торчит в интернет (по этой причине у нас собственно есть прокси-машина).\n",
    "\n",
    "Осталось научиться до нее добираться из браузера. Для этого нам опять потребуется наша прокси-машина.\n",
    "\n",
    "Следующей командой мы создадим прокси от нашего локального порта `9999` до порта `8888` головной машины, где собственно крутится наш жупитер.\n",
    "\n",
    "```bash\n",
    "ssh -N yc-user@84.201.156.102 -L 9999:rc1a-dataproc-m-f5opzfioh9mln7bi.mdb.yandexcloud.net:8888\n",
    "```\n",
    "\n",
    "Пробуем открыть `http://localhost:9999/` - вуаля!\n",
    "\n",
    "И еще один небольшой штрих для удобства - вместо этой громоздкой команды, можно просто подправить конфиг `~/.ssh/config`:\n",
    "\n",
    "```\n",
    "...\n",
    "\n",
    "Host lsml-proxy\n",
    "    HostName 84.201.156.102\n",
    "    User yc-user\n",
    "    IdentityFile ~/.ssh/id_ed25519\n",
    "    LocalForward 9999 rc1a-dataproc-m-f5opzfioh9mln7bi.mdb.yandexcloud.net:8888\n",
    "```\n",
    "\n",
    "И тогда для запуска нужно всего лишь\n",
    "\n",
    "```bash\n",
    "ssh -N lsml-proxy\n",
    "```\n",
    "\n",
    "Поздравляю, если вы справились с этой настройкой, то теперь вы начинаете представлять, что же такое настоящий дата-инжиниринг :) \n",
    "\n",
    "Дальше я предполагаю, что я закинул этот ноутбук на головную машину и запускаю команды именно оттуда"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu\r\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Основы MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В своей сути MapRedcue это очень простая парадигма. Допустим у нас есть датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T20:25:44.158742Z",
     "start_time": "2021-01-25T20:25:41.796781Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 89.9M  100 89.9M    0     0  16.8M      0  0:00:05  0:00:05 --:--:-- 21.7M\n"
     ]
    }
   ],
   "source": [
    "! curl https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_1.csv > tweets_1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! head -n 2 tweets_1.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы хотим в этом датасете что-нибудь найти. Например (сюрприз-сюрприз), посчитать количество уникальных слов. Мы могли бы сделать что-то такое:\n",
    "\n",
    "#### Вариант 1 \n",
    "Используем исключительно питон и наивный алгоритм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T20:30:27.659125Z",
     "start_time": "2021-01-25T20:30:25.819459Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t\t268703\n",
      "co\t250375\n",
      "https\t221366\n",
      "the\t69350\n",
      "to\t55972\n",
      "a\t43420\n",
      "in\t37099\n",
      "s\t36085\n",
      "of\t33579\n",
      "http\t28661\n",
      "CPU times: user 4.88 s, sys: 88.8 ms, total: 4.96 s\n",
      "Wall time: 4.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from collections import Counter\n",
    "import csv\n",
    "import re\n",
    "import sys\n",
    "\n",
    "counter = Counter()\n",
    "pattern = re.compile(r\"[a-z]+\")\n",
    "\n",
    "with open('tweets_1.csv', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    for row in reader:\n",
    "        content = row[2]\n",
    "        for match in pattern.finditer(content.lower()):\n",
    "            word = match.group(0)\n",
    "            counter[word] += 1\n",
    "\n",
    "for word, count in counter.most_common(10):\n",
    "    print(f\"{word}\\t{count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T20:30:53.751130Z",
     "start_time": "2021-01-25T20:30:53.746887Z"
    }
   },
   "source": [
    "Такое сработает только если у нас не очень много данных и они все вмещаются в оперативную память"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91M\ttweets_1.csv\r\n"
     ]
    }
   ],
   "source": [
    "! du -h tweets_1.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вариант 2\n",
    "Используем парадигму Map Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом примере у нас всего 90 мегабайт данных, и на моем компьютере они обрабатываются за примерно 30 секунд с помощью питона. Теперь представим (это достаточно несложно), что у нас приходит новых данных приходит _десятки терабайт_ в сутки. Такое уже не поместится ни в один сервер, поэтому нам нужно придумать что-нибудь похитрее.\n",
    "\n",
    "MapReduce как раз является парадигмой, помогающей обрабатывать большие объемы данных, за счет простоты своего устройства.\n",
    "\n",
    "Приятная новость - для того, чтобы понять и научиться программировать программы в парадигме MapReduce вам потребуется... **5 секунд!**\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ADKosm/lsml-2021-public/main/imgs/you-know-mapreduce.png\" width=\"400\">\n",
    "\n",
    "Все потому что вы уже прошли семинар по Bash и научились составлять большие программы в виде компоновки небольших  программ, соединенных пайпами. По своей сути программа на MapReduce - это хорошо отмасштабированная программа вида\n",
    "\n",
    "```bash\n",
    "cat data.txt | map | sort | reduce\n",
    "```\n",
    "\n",
    "Сортировку за вас выполняет сам фреймворк (и ее вы можете дополнительно настроить точно такое как и команду sort). А также он сам разбивает данные на части и параллельно запускает операции map и reduce. \n",
    "\n",
    "Таким образом на самом деле Hadoop - это всего лишь гигантская машина сортировки, которая дополнительно дает вам некоторые гарантии:\n",
    "\n",
    "* Для всех данных параллельно будет применена операция map\n",
    "* Данные будут отсортированы по указанному вами ключу\n",
    "* Каждый ключ будет целиком передан на один и только один reduce\n",
    "\n",
    "Программисту остается реализовать программу, которая состоит из двух компонент: `map` и `reduce`. \n",
    "\n",
    "Операция `map` -- это просто функция из одного элемента в другой элемент, у которого есть первичный ключ. \n",
    "\n",
    "Операция `reduce` -- это коммутативная и ассоциативная агрегация всех элементов по ключу. Чтобы эти операции совершить, надо разбить весь вход на куски данных и отправить их на машины, чтобы они выполнялись в параллель, а весь выход операции map идёт в операцию shuffle, которая по одним и тем же ключам определяет записи на одинаковые хосты. \n",
    "\n",
    "В итоге получается, что мы можем спокойно увеличивать количество worker'ов для map операций и с увеличением количества данных мы лишь будем линейно утилизировать количество машин, то же самое с операцией reduce -- мы можем добавлять машины с ростом увеличения количества ключей линейно, не боясь того, что мы не можем позволить на одной какой-то машине больше памяти или диска.\n",
    "\n",
    "Давайте напишем маппер и редьюсер на питоне для этой задачи:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
      "\u001b[K     |████████████████████████████████| 76 kB 1.2 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.62.3\n"
     ]
    }
   ],
   "source": [
    "! sudo pip3 install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T20:45:38.075819Z",
     "start_time": "2021-01-25T20:45:38.069953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordcount.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount.py\n",
    "import sys\n",
    "import csv\n",
    "import re\n",
    "\n",
    "\n",
    "def mapper():\n",
    "    pattern = re.compile(r\"[a-z]+\")\n",
    "    for row in csv.reader(iter(sys.stdin.readline, '')):\n",
    "        content = row[2]\n",
    "        for match in pattern.finditer(content.lower()):\n",
    "            word = match.group(0)\n",
    "            print(\"{}\\t{}\".format(word, 1))\n",
    "\n",
    "\n",
    "def reducer():\n",
    "    word, number = next(sys.stdin).split('\\t')\n",
    "    number = int(number)\n",
    "    for line in sys.stdin:\n",
    "        current_word, current_number = line.split('\\t')\n",
    "        current_number = int(current_number)\n",
    "        if current_word != word:\n",
    "            print(\"{}\\t{}\".format(word, number))\n",
    "            word = current_word\n",
    "            number = current_number\n",
    "        else:\n",
    "            number += current_number\n",
    "    print(\"{}\\t{}\".format(word, number))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mr_command = sys.argv[1]\n",
    "    {\n",
    "        'map': mapper,\n",
    "        'reduce': reducer\n",
    "    }[mr_command]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важно еще удалить голову у таблицы, иначе подсчеты могут быть некоректными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sed -i -e '1'd tweets_1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\t1\r\n",
      "a\t1\r\n",
      "a\t1\r\n",
      "a\t1\r\n",
      "a\t1\r\n",
      "a\t1\r\n",
      "a\t1\r\n",
      "a\t1\r\n",
      "a\t1\r\n",
      "a\t1\r\n",
      "sort: write failed: 'standard output': Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "! cat tweets_1.csv | python wordcount.py map | sort -k1,1 | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T20:47:23.728893Z",
     "start_time": "2021-01-25T20:46:51.895208Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 243891/243891 [00:09<00:00, 26536.92it/s]\n",
      "CPU times: user 211 ms, sys: 29.1 ms, total: 240 ms\n",
      "Wall time: 12.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "! cat tweets_1.csv | \\\n",
    "    tqdm --total $(cat tweets_1.csv | wc -l)| \\\n",
    "    python wordcount.py map | \\\n",
    "    sort -k1,1 | \\\n",
    "    python wordcount.py reduce > result.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\t43420\r\n",
      "aa\t151\r\n",
      "aaa\t13\r\n",
      "aaaaaa\t1\r\n",
      "aaaaaaaaaaaaand\t1\r\n",
      "aaaaaaaaaall\t1\r\n",
      "aaaaaaaamen\t1\r\n",
      "aaaaaaaand\t2\r\n",
      "aaaaaaargh\t1\r\n",
      "aaaaaand\t2\r\n"
     ]
    }
   ],
   "source": [
    "! head result.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T20:50:28.963822Z",
     "start_time": "2021-01-25T20:50:28.265568Z"
    }
   },
   "source": [
    "Отлично! Слова есть, осталось только найти top-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing top10.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top10.py\n",
    "import sys\n",
    "\n",
    "\n",
    "def _rewind_stream(stream):\n",
    "    for _ in stream:\n",
    "        pass\n",
    "\n",
    "\n",
    "def mapper():\n",
    "    for row in sys.stdin:\n",
    "        key, value = row.split('\\t')\n",
    "        print(\"{}+{}\\t\".format(key, value.strip()))\n",
    "\n",
    "\n",
    "def reducer():\n",
    "    for _ in range(10):\n",
    "        key, _ = next(sys.stdin).split('\\t')\n",
    "        word, count = key.split(\"+\")\n",
    "        print(\"{}\\t{}\".format(word, count))\n",
    "    _rewind_stream(sys.stdin)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mr_command = sys.argv[1]\n",
    "    {\n",
    "        'map': mapper,\n",
    "        'reduce': reducer\n",
    "    }[mr_command]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 346613/346613 [00:00<00:00, 615506.93it/s]\n"
     ]
    }
   ],
   "source": [
    "! cat result.txt | \\\n",
    "    tqdm --total $(cat result.txt | wc -l) | \\\n",
    "    python top10.py map | \\\n",
    "    sort -t'+' -k2,2nr -k1,1 | \\\n",
    "    python top10.py reduce > top-10.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t\t268703\r\n",
      "co\t250375\r\n",
      "https\t221366\r\n",
      "the\t69350\r\n",
      "to\t55972\r\n",
      "a\t43420\r\n",
      "in\t37099\r\n",
      "s\t36085\r\n",
      "of\t33579\r\n",
      "http\t28661\r\n"
     ]
    }
   ],
   "source": [
    "! cat top-10.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На MapReduce мы задачу переписали, однако быстрее работать она пока не стала. Все дело в том, что мы это еще не на кластере запускали! Время запускать все на настоящем кластере!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загружаем данные в HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T20:52:22.492958Z",
     "start_time": "2021-01-25T20:52:22.382757Z"
    }
   },
   "source": [
    "При работе с HDFS нужно понимать, что есть два места, где хранятся данные\n",
    "\n",
    "1. На локальных жестких дисках машин кластера - это деволтная система, на нее можно посмотреть через `hdfs dfs -ls /`\n",
    "\n",
    "2. В Object Storage - для работы с ней, нужно указывать путь до бакета - `hdfs dfs -ls s3a://lsml2022alexius/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_1.csv --> IRAhandle_tweets_1.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_1.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 89.9M  100 89.9M    0     0  50.5M      0  0:00:01  0:00:01 --:--:-- 50.5M\n",
      "\n",
      "[2/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_2.csv --> IRAhandle_tweets_2.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_2.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 90.0M  100 90.0M    0     0  13.5M      0  0:00:06  0:00:06 --:--:-- 19.3M\n",
      "\n",
      "[3/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_3.csv --> IRAhandle_tweets_3.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_3.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 90.0M  100 90.0M    0     0  11.6M      0  0:00:07  0:00:07 --:--:-- 24.0M\n",
      "\n",
      "[4/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_4.csv --> IRAhandle_tweets_4.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_4.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 90.0M  100 90.0M    0     0  19.0M      0  0:00:04  0:00:04 --:--:-- 24.1M   68 61.7M    0     0  14.3M      0  0:00:06  0:00:04  0:00:02 14.3M\n",
      "\n",
      "[5/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_5.csv --> IRAhandle_tweets_5.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_5.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 90.0M  100 90.0M    0     0  18.9M      0  0:00:04  0:00:04 --:--:-- 23.9M     0  0:00:20  0:00:03  0:00:17 4551k\n",
      "\n",
      "[6/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_6.csv --> IRAhandle_tweets_6.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_6.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 90.0M  100 90.0M    0     0  16.2M      0  0:00:05  0:00:05 --:--:-- 19.8M\n",
      "\n",
      "[7/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_7.csv --> IRAhandle_tweets_7.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_7.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 90.0M  100 90.0M    0     0  17.0M      0  0:00:05  0:00:05 --:--:-- 21.0M\n",
      "\n",
      "[8/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_8.csv --> IRAhandle_tweets_8.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_8.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 90.0M  100 90.0M    0     0  17.1M      0  0:00:05  0:00:05 --:--:-- 27.8M\n",
      "\n",
      "[9/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_9.csv --> IRAhandle_tweets_9.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_9.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 89.9M  100 89.9M    0     0  18.0M      0  0:00:04  0:00:04 --:--:-- 22.5M\n",
      "\n",
      "[10/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_10.csv --> IRAhandle_tweets_10.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_10.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 90.0M  100 90.0M    0     0  20.2M      0  0:00:04  0:00:04 --:--:-- 20.2M\n",
      "\n",
      "[11/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_11.csv --> IRAhandle_tweets_11.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_11.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 89.9M  100 89.9M    0     0  17.4M      0  0:00:05  0:00:05 --:--:-- 21.6M\n",
      "\n",
      "[12/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_12.csv --> IRAhandle_tweets_12.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_12.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 90.0M  100 90.0M    0     0  17.1M      0  0:00:05  0:00:05 --:--:-- 27.7M\n",
      "\n",
      "[13/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_13.csv --> IRAhandle_tweets_13.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_13.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 8045k  100 8045k    0     0  13.5M      0 --:--:-- --:--:-- --:--:-- 13.5M\n"
     ]
    }
   ],
   "source": [
    "! curl -O https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_{`seq -s , 1 13`}.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подформатируем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish 1\n",
      "Finish 2\n",
      "Finish 3\n",
      "Finish 4\n",
      "Finish 5\n",
      "Finish 6\n",
      "Finish 7\n",
      "Finish 8\n",
      "Finish 9\n",
      "Finish 10\n",
      "Finish 11\n",
      "Finish 12\n",
      "Finish 13\n"
     ]
    }
   ],
   "source": [
    "! for i in {1..13}; do sed IRAhandle_tweets_$i.csv -i -e '1'd && echo \"Finish $i\" ; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим отдельную папку для этих данных в HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "drwx------   - mapred hadoop          0 2022-01-24 09:44 /hadoop\r\n",
      "drwxrwxrwt   - hdfs   hadoop          0 2022-01-24 09:43 /tmp\r\n",
      "drwxrwxrwt   - hdfs   hadoop          0 2022-01-30 10:44 /user\r\n",
      "drwxrwxrwt   - hdfs   hadoop          0 2022-01-24 09:44 /var\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/tweets/data': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -rm -r /user/tweets/data\n",
    "! hdfs dfs -mkdir -p /user/tweets/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: все команды для hdfs смотреть здесь - https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/FileSystemShell.html\n",
    "\n",
    "Заливаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -put IRAhandle_tweets_* /user/tweets/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-30 12:33:19,998 INFO balancer.Balancer: namenodes  = [hdfs://rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net:8020]\n",
      "2022-01-30 12:33:20,005 INFO balancer.Balancer: parameters = Balancer.BalancerParameters [BalancingPolicy.Node, threshold = 10.0, max idle iteration = 5, #excluded nodes = 0, #included nodes = 0, #source nodes = 0, #blockpools = 0, run during upgrade = false]\n",
      "2022-01-30 12:33:20,005 INFO balancer.Balancer: included nodes = []\n",
      "2022-01-30 12:33:20,005 INFO balancer.Balancer: excluded nodes = []\n",
      "2022-01-30 12:33:20,005 INFO balancer.Balancer: source nodes = []\n",
      "Time Stamp               Iteration#  Bytes Already Moved  Bytes Left To Move  Bytes Being Moved  NameNode\n",
      "2022-01-30 12:33:20,010 INFO balancer.NameNodeConnector: getBlocks calls for hdfs://rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net:8020 will be rate-limited to 20 per second\n",
      "2022-01-30 12:33:21,104 INFO balancer.Balancer: dfs.namenode.get-blocks.max-qps = 20 (default=20)\n",
      "2022-01-30 12:33:21,104 INFO balancer.Balancer: dfs.balancer.movedWinWidth = 5400000 (default=5400000)\n",
      "2022-01-30 12:33:21,104 INFO balancer.Balancer: dfs.balancer.moverThreads = 1000 (default=1000)\n",
      "2022-01-30 12:33:21,104 INFO balancer.Balancer: dfs.balancer.dispatcherThreads = 200 (default=200)\n",
      "2022-01-30 12:33:21,104 INFO balancer.Balancer: dfs.balancer.getBlocks.size = 2147483648 (default=2147483648)\n",
      "2022-01-30 12:33:21,104 INFO balancer.Balancer: dfs.balancer.getBlocks.min-block-size = 10485760 (default=10485760)\n",
      "2022-01-30 12:33:21,104 INFO balancer.Balancer: dfs.datanode.balance.max.concurrent.moves = 50 (default=50)\n",
      "2022-01-30 12:33:21,104 INFO balancer.Balancer: dfs.datanode.balance.bandwidthPerSec = 10485760 (default=10485760)\n",
      "2022-01-30 12:33:21,111 INFO balancer.Balancer: dfs.balancer.max-size-to-move = 10737418240 (default=10737418240)\n",
      "2022-01-30 12:33:21,111 INFO balancer.Balancer: dfs.blocksize = 268435456 (default=134217728)\n",
      "2022-01-30 12:33:21,120 INFO net.NetworkTopology: Adding a new node: /default-rack/10.128.0.18:9866\n",
      "2022-01-30 12:33:21,121 INFO net.NetworkTopology: Adding a new node: /default-rack/10.128.0.5:9866\n",
      "2022-01-30 12:33:21,121 INFO net.NetworkTopology: Adding a new node: /default-rack/10.128.0.14:9866\n",
      "2022-01-30 12:33:21,123 INFO balancer.Balancer: 0 over-utilized: []\n",
      "2022-01-30 12:33:21,123 INFO balancer.Balancer: 0 underutilized: []\n",
      "The cluster is balanced. Exiting...\n",
      "Jan 30, 2022 12:33:21 PM          0                  0 B                 0 B                0 B  hdfs://rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net:8020\n",
      "Jan 30, 2022 12:33:21 PM Balancing took 1.725 seconds\n"
     ]
    }
   ],
   "source": [
    "! sudo chmod 0777 /usr/lib/hadoop/logs\n",
    "! sudo -u hdfs hdfs balancer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13 items\n",
      "-rw-r--r--   1 ubuntu hadoop   94371561 2022-01-30 12:13 /user/tweets/data/IRAhandle_tweets_1.csv\n",
      "-rw-r--r--   1 ubuntu hadoop   94371615 2022-01-30 12:13 /user/tweets/data/IRAhandle_tweets_10.csv\n",
      "-rw-r--r--   1 ubuntu hadoop   94371552 2022-01-30 12:13 /user/tweets/data/IRAhandle_tweets_11.csv\n",
      "-rw-r--r--   1 ubuntu hadoop   94371703 2022-01-30 12:13 /user/tweets/data/IRAhandle_tweets_12.csv\n",
      "-rw-r--r--   1 ubuntu hadoop    8238864 2022-01-30 12:13 /user/tweets/data/IRAhandle_tweets_13.csv\n",
      "-rw-r--r--   1 ubuntu hadoop   94371748 2022-01-30 12:13 /user/tweets/data/IRAhandle_tweets_2.csv\n",
      "-rw-r--r--   1 ubuntu hadoop   94371796 2022-01-30 12:13 /user/tweets/data/IRAhandle_tweets_3.csv\n",
      "-rw-r--r--   1 ubuntu hadoop   94371606 2022-01-30 12:13 /user/tweets/data/IRAhandle_tweets_4.csv\n",
      "-rw-r--r--   1 ubuntu hadoop   94371616 2022-01-30 12:13 /user/tweets/data/IRAhandle_tweets_5.csv\n",
      "-rw-r--r--   1 ubuntu hadoop   94371646 2022-01-30 12:13 /user/tweets/data/IRAhandle_tweets_6.csv\n",
      "-rw-r--r--   1 ubuntu hadoop   94371711 2022-01-30 12:13 /user/tweets/data/IRAhandle_tweets_7.csv\n",
      "-rw-r--r--   1 ubuntu hadoop   94371727 2022-01-30 12:13 /user/tweets/data/IRAhandle_tweets_8.csv\n",
      "-rw-r--r--   1 ubuntu hadoop   94371542 2022-01-30 12:13 /user/tweets/data/IRAhandle_tweets_9.csv\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /user/tweets/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Запускаем MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяем, что скрипты на головной машине"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import sys\r\n",
      "import csv\r\n",
      "import re\r\n",
      "\r\n",
      "\r\n",
      "def mapper():\r\n",
      "    pattern = re.compile(r\"[a-z]+\")\r\n",
      "    for row in csv.reader(iter(sys.stdin.readline, '')):\r\n",
      "        content = row[2]\r\n",
      "        for match in pattern.finditer(content.lower()):\r\n",
      "            word = match.group(0)\r\n",
      "            print(\"{}\\t{}\".format(word, 1))\r\n",
      "\r\n",
      "\r\n",
      "def reducer():\r\n",
      "    word, number = next(sys.stdin).split('\\t')\r\n",
      "    number = int(number)\r\n",
      "    for line in sys.stdin:\r\n",
      "        current_word, current_number = line.split('\\t')\r\n",
      "        current_number = int(current_number)\r\n",
      "        if current_word != word:\r\n",
      "            print(\"{}\\t{}\".format(word, number))\r\n",
      "            word = current_word\r\n",
      "            number = current_number\r\n",
      "        else:\r\n",
      "            number += current_number\r\n",
      "    print(\"{}\\t{}\".format(word, number))\r\n",
      "\r\n",
      "\r\n",
      "if __name__ == '__main__':\r\n",
      "    mr_command = sys.argv[1]\r\n",
      "    {\r\n",
      "        'map': mapper,\r\n",
      "        'reduce': reducer\r\n",
      "    }[mr_command]()\r\n"
     ]
    }
   ],
   "source": [
    "! cat wordcount.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import sys\r\n",
      "\r\n",
      "\r\n",
      "def _rewind_stream(stream):\r\n",
      "    for _ in stream:\r\n",
      "        pass\r\n",
      "\r\n",
      "\r\n",
      "def mapper():\r\n",
      "    for row in sys.stdin:\r\n",
      "        key, value = row.split('\\t')\r\n",
      "        print(\"{}+{}\\t\".format(key, value.strip()))\r\n",
      "\r\n",
      "\r\n",
      "def reducer():\r\n",
      "    for _ in range(10):\r\n",
      "        key, _ = next(sys.stdin).split('\\t')\r\n",
      "        word, count = key.split(\"+\")\r\n",
      "        print(\"{}\\t{}\".format(word, count))\r\n",
      "    _rewind_stream(sys.stdin)\r\n",
      "\r\n",
      "if __name__ == '__main__':\r\n",
      "    mr_command = sys.argv[1]\r\n",
      "    {\r\n",
      "        'map': mapper,\r\n",
      "        'reduce': reducer\r\n",
      "    }[mr_command]()\r\n"
     ]
    }
   ],
   "source": [
    "! cat top10.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собираем команду на запуск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar: cannot open `/usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar' (No such file or directory)\r\n"
     ]
    }
   ],
   "source": [
    "! file /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/tweets/result': No such file or directory\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-3.2.2.jar] /tmp/streamjob7074396873421902954.jar tmpDir=null\n",
      "2022-01-30 12:21:34,703 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net/10.128.0.26:8032\n",
      "2022-01-30 12:21:34,911 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net/10.128.0.26:10200\n",
      "2022-01-30 12:21:34,947 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net/10.128.0.26:8032\n",
      "2022-01-30 12:21:34,948 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net/10.128.0.26:10200\n",
      "2022-01-30 12:21:35,206 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1643538737352_0002\n",
      "2022-01-30 12:21:35,923 INFO mapred.FileInputFormat: Total input files to process : 13\n",
      "2022-01-30 12:21:36,522 INFO mapreduce.JobSubmitter: number of splits:37\n",
      "2022-01-30 12:21:36,682 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1643538737352_0002\n",
      "2022-01-30 12:21:36,683 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2022-01-30 12:21:36,894 INFO conf.Configuration: resource-types.xml not found\n",
      "2022-01-30 12:21:36,895 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2022-01-30 12:21:36,995 INFO impl.YarnClientImpl: Submitted application application_1643538737352_0002\n",
      "2022-01-30 12:21:37,095 INFO mapreduce.Job: The url to track the job: http://rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net:8088/proxy/application_1643538737352_0002/\n",
      "2022-01-30 12:21:37,097 INFO mapreduce.Job: Running job: job_1643538737352_0002\n",
      "2022-01-30 12:21:42,179 INFO mapreduce.Job: Job job_1643538737352_0002 running in uber mode : false\n",
      "2022-01-30 12:21:42,180 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2022-01-30 12:21:52,492 INFO mapreduce.Job:  map 5% reduce 0%\n",
      "2022-01-30 12:21:58,576 INFO mapreduce.Job:  map 8% reduce 0%\n",
      "2022-01-30 12:21:59,600 INFO mapreduce.Job:  map 19% reduce 0%\n",
      "2022-01-30 12:22:00,608 INFO mapreduce.Job:  map 24% reduce 0%\n",
      "2022-01-30 12:22:01,639 INFO mapreduce.Job:  map 30% reduce 0%\n",
      "2022-01-30 12:22:02,657 INFO mapreduce.Job:  map 32% reduce 0%\n",
      "2022-01-30 12:22:06,679 INFO mapreduce.Job:  map 35% reduce 0%\n",
      "2022-01-30 12:22:07,696 INFO mapreduce.Job:  map 41% reduce 0%\n",
      "2022-01-30 12:22:09,736 INFO mapreduce.Job:  map 43% reduce 0%\n",
      "2022-01-30 12:22:12,787 INFO mapreduce.Job:  map 49% reduce 0%\n",
      "2022-01-30 12:22:13,796 INFO mapreduce.Job:  map 57% reduce 0%\n",
      "2022-01-30 12:22:14,819 INFO mapreduce.Job:  map 59% reduce 0%\n",
      "2022-01-30 12:22:17,857 INFO mapreduce.Job:  map 59% reduce 7%\n",
      "2022-01-30 12:22:20,905 INFO mapreduce.Job:  map 62% reduce 7%\n",
      "2022-01-30 12:22:21,951 INFO mapreduce.Job:  map 65% reduce 7%\n",
      "2022-01-30 12:22:22,999 INFO mapreduce.Job:  map 68% reduce 7%\n",
      "2022-01-30 12:22:24,007 INFO mapreduce.Job:  map 73% reduce 15%\n",
      "2022-01-30 12:22:25,015 INFO mapreduce.Job:  map 76% reduce 15%\n",
      "2022-01-30 12:22:27,049 INFO mapreduce.Job:  map 81% reduce 15%\n",
      "2022-01-30 12:22:30,182 INFO mapreduce.Job:  map 84% reduce 18%\n",
      "2022-01-30 12:22:31,188 INFO mapreduce.Job:  map 86% reduce 18%\n",
      "2022-01-30 12:22:32,193 INFO mapreduce.Job:  map 95% reduce 18%\n",
      "2022-01-30 12:22:36,217 INFO mapreduce.Job:  map 97% reduce 21%\n",
      "2022-01-30 12:22:37,222 INFO mapreduce.Job:  map 100% reduce 21%\n",
      "2022-01-30 12:22:42,245 INFO mapreduce.Job:  map 100% reduce 42%\n",
      "2022-01-30 12:22:48,275 INFO mapreduce.Job:  map 100% reduce 87%\n",
      "2022-01-30 12:22:54,298 INFO mapreduce.Job:  map 100% reduce 99%\n",
      "2022-01-30 12:22:55,302 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2022-01-30 12:22:57,320 INFO mapreduce.Job: Job job_1643538737352_0002 completed successfully\n",
      "2022-01-30 12:22:57,394 INFO mapreduce.Job: Counters: 56\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=16556486\n",
      "\t\tFILE: Number of bytes written=56960232\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1143550744\n",
      "\t\tHDFS: Number of bytes written=30359819\n",
      "\t\tHDFS: Number of read operations=126\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=9\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=38\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tData-local map tasks=35\n",
      "\t\tRack-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=1013085\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=665946\n",
      "\t\tTotal time spent by all map tasks (ms)=337695\n",
      "\t\tTotal time spent by all reduce tasks (ms)=110991\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=337695\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=110991\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=1037399040\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=681928704\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2946207\n",
      "\t\tMap output records=41946754\n",
      "\t\tMap output bytes=313767903\n",
      "\t\tMap output materialized bytes=30752668\n",
      "\t\tInput split bytes=5597\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=2831736\n",
      "\t\tReduce shuffle bytes=30752668\n",
      "\t\tReduce input records=41946754\n",
      "\t\tReduce output records=2831736\n",
      "\t\tSpilled Records=83893508\n",
      "\t\tShuffled Maps =111\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=111\n",
      "\t\tGC time elapsed (ms)=5267\n",
      "\t\tCPU time spent (ms)=201910\n",
      "\t\tPhysical memory (bytes) snapshot=14061375488\n",
      "\t\tVirtual memory (bytes) snapshot=181574209536\n",
      "\t\tTotal committed heap usage (bytes)=13717995520\n",
      "\t\tPeak Map Physical memory (bytes)=360402944\n",
      "\t\tPeak Map Virtual memory (bytes)=4340711424\n",
      "\t\tPeak Reduce Physical memory (bytes)=528461824\n",
      "\t\tPeak Reduce Virtual memory (bytes)=7033843712\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1143545147\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=30359819\n",
      "2022-01-30 12:22:57,394 INFO streaming.StreamJob: Output directory: /user/tweets/result/\n",
      "CPU times: user 1.37 s, sys: 333 ms, total: 1.71 s\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "! hdfs dfs -rm -r /user/tweets/result || true\n",
    "! yarn jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.name=\"word-count\" \\\n",
    "-D mapreduce.job.reduces=3 \\\n",
    "-files ~/wordcount.py \\\n",
    "-mapper \"python3 wordcount.py map\" \\\n",
    "-reducer \"python3 wordcount.py reduce\" \\\n",
    "-input /user/tweets/data/ \\\n",
    "-output /user/tweets/result/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо того, что можно следить здесь в терминале, за выполнением можно наблюдать через UI-proxy в интерфейсе облака"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смотрим результат\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "-rw-r--r--   1 ubuntu hadoop          0 2022-01-30 12:22 /user/tweets/result/_SUCCESS\r\n",
      "-rw-r--r--   1 ubuntu hadoop   10107405 2022-01-30 12:22 /user/tweets/result/part-00000\r\n",
      "-rw-r--r--   1 ubuntu hadoop   10134121 2022-01-30 12:22 /user/tweets/result/part-00001\r\n",
      "-rw-r--r--   1 ubuntu hadoop   10118293 2022-01-30 12:22 /user/tweets/result/part-00002\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /user/tweets/result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\t1726\r\n",
      "aaaaa\t7\r\n",
      "aaaaaaaaaaaaaa\t3\r\n",
      "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaannnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnddddddddddddddddddddddddddddddddddddd\t1\r\n",
      "aaaaaaaaaaaaaaaaaaah\t1\r\n",
      "aaaaaaaaaaaaand\t1\r\n",
      "aaaaaaaaannnnnnnnnnnddddddddddddd\t1\r\n",
      "aaaaaaaah\t1\r\n",
      "aaaaaaaamen\t1\r\n",
      "aaaaaaagh\t2\r\n",
      "cat: Unable to write to output stream.\r\n",
      "cat: Unable to write to output stream.\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /user/tweets/result/part-* | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим вторую задачу "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/tweets/top10/': No such file or directory\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-3.2.2.jar] /tmp/streamjob1782456448161600928.jar tmpDir=null\n",
      "2022-01-30 12:25:30,007 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net/10.128.0.26:8032\n",
      "2022-01-30 12:25:30,242 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net/10.128.0.26:10200\n",
      "2022-01-30 12:25:30,282 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net/10.128.0.26:8032\n",
      "2022-01-30 12:25:30,284 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net/10.128.0.26:10200\n",
      "2022-01-30 12:25:30,515 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1643538737352_0003\n",
      "2022-01-30 12:25:31,242 INFO mapred.FileInputFormat: Total input files to process : 3\n",
      "2022-01-30 12:25:31,363 INFO mapreduce.JobSubmitter: number of splits:30\n",
      "2022-01-30 12:25:32,041 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1643538737352_0003\n",
      "2022-01-30 12:25:32,043 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2022-01-30 12:25:32,208 INFO conf.Configuration: resource-types.xml not found\n",
      "2022-01-30 12:25:32,208 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2022-01-30 12:25:32,277 INFO impl.YarnClientImpl: Submitted application application_1643538737352_0003\n",
      "2022-01-30 12:25:32,307 INFO mapreduce.Job: The url to track the job: http://rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net:8088/proxy/application_1643538737352_0003/\n",
      "2022-01-30 12:25:32,309 INFO mapreduce.Job: Running job: job_1643538737352_0003\n",
      "2022-01-30 12:25:38,380 INFO mapreduce.Job: Job job_1643538737352_0003 running in uber mode : false\n",
      "2022-01-30 12:25:38,381 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2022-01-30 12:25:45,511 INFO mapreduce.Job:  map 7% reduce 0%\n",
      "2022-01-30 12:25:47,536 INFO mapreduce.Job:  map 10% reduce 0%\n",
      "2022-01-30 12:25:48,558 INFO mapreduce.Job:  map 23% reduce 0%\n",
      "2022-01-30 12:25:49,569 INFO mapreduce.Job:  map 27% reduce 0%\n",
      "2022-01-30 12:25:50,580 INFO mapreduce.Job:  map 30% reduce 0%\n",
      "2022-01-30 12:25:52,612 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2022-01-30 12:25:54,652 INFO mapreduce.Job:  map 37% reduce 0%\n",
      "2022-01-30 12:25:55,680 INFO mapreduce.Job:  map 40% reduce 0%\n",
      "2022-01-30 12:25:56,690 INFO mapreduce.Job:  map 47% reduce 0%\n",
      "2022-01-30 12:25:57,700 INFO mapreduce.Job:  map 57% reduce 0%\n",
      "2022-01-30 12:25:59,755 INFO mapreduce.Job:  map 60% reduce 0%\n",
      "2022-01-30 12:26:00,767 INFO mapreduce.Job:  map 63% reduce 0%\n",
      "2022-01-30 12:26:02,805 INFO mapreduce.Job:  map 70% reduce 0%\n",
      "2022-01-30 12:26:03,813 INFO mapreduce.Job:  map 77% reduce 0%\n",
      "2022-01-30 12:26:04,820 INFO mapreduce.Job:  map 80% reduce 0%\n",
      "2022-01-30 12:26:05,840 INFO mapreduce.Job:  map 83% reduce 27%\n",
      "2022-01-30 12:26:06,847 INFO mapreduce.Job:  map 87% reduce 27%\n",
      "2022-01-30 12:26:07,858 INFO mapreduce.Job:  map 90% reduce 27%\n",
      "2022-01-30 12:26:09,902 INFO mapreduce.Job:  map 93% reduce 27%\n",
      "2022-01-30 12:26:10,908 INFO mapreduce.Job:  map 97% reduce 27%\n",
      "2022-01-30 12:26:11,914 INFO mapreduce.Job:  map 100% reduce 32%\n",
      "2022-01-30 12:26:16,940 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2022-01-30 12:26:17,961 INFO mapreduce.Job: Job job_1643538737352_0003 completed successfully\n",
      "2022-01-30 12:26:18,047 INFO mapreduce.Job: Counters: 56\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=13564266\n",
      "\t\tFILE: Number of bytes written=35383416\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=31346018\n",
      "\t\tHDFS: Number of bytes written=106\n",
      "\t\tHDFS: Number of read operations=95\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=3\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=30\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=22\n",
      "\t\tRack-local map tasks=8\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=434277\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=150672\n",
      "\t\tTotal time spent by all map tasks (ms)=144759\n",
      "\t\tTotal time spent by all reduce tasks (ms)=25112\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=144759\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=25112\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=444699648\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=154288128\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2831736\n",
      "\t\tMap output records=2831736\n",
      "\t\tMap output bytes=33191556\n",
      "\t\tMap output materialized bytes=14323416\n",
      "\t\tInput split bytes=4230\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=2831736\n",
      "\t\tReduce shuffle bytes=14323416\n",
      "\t\tReduce input records=2831736\n",
      "\t\tReduce output records=10\n",
      "\t\tSpilled Records=5663472\n",
      "\t\tShuffled Maps =30\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=30\n",
      "\t\tGC time elapsed (ms)=3991\n",
      "\t\tCPU time spent (ms)=91690\n",
      "\t\tPhysical memory (bytes) snapshot=13613690880\n",
      "\t\tVirtual memory (bytes) snapshot=137221320704\n",
      "\t\tTotal committed heap usage (bytes)=13523484672\n",
      "\t\tPeak Map Physical memory (bytes)=472252416\n",
      "\t\tPeak Map Virtual memory (bytes)=4352278528\n",
      "\t\tPeak Reduce Physical memory (bytes)=471699456\n",
      "\t\tPeak Reduce Virtual memory (bytes)=7014068224\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=31341788\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=106\n",
      "2022-01-30 12:26:18,049 INFO streaming.StreamJob: Output directory: /user/tweets/top10/\n",
      "CPU times: user 839 ms, sys: 201 ms, total: 1.04 s\n",
      "Wall time: 52.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "! hdfs dfs -rm -r /user/tweets/top10/\n",
    "! yarn jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.name=\"top-10\" \\\n",
    "-D mapreduce.job.reduces=1 \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "-D mapreduce.map.output.key.field.separator='+' \\\n",
    "-files top10.py \\\n",
    "-mapper \"python top10.py map\" \\\n",
    "-reducer \"python top10.py reduce\" \\\n",
    "-input /user/tweets/result/ \\\n",
    "-output /user/tweets/top10/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   1 ubuntu hadoop          0 2022-01-30 12:26 /user/tweets/top10/_SUCCESS\r\n",
      "-rw-r--r--   1 ubuntu hadoop        106 2022-01-30 12:26 /user/tweets/top10/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /user/tweets/top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t\t3015051\r\n",
      "co\t2833375\r\n",
      "https\t2454132\r\n",
      "the\t591885\r\n",
      "to\t589004\r\n",
      "in\t457433\r\n",
      "a\t412888\r\n",
      "s\t397889\r\n",
      "http\t375299\r\n",
      "of\t350983\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /user/tweets/top10/part-*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed cache\n",
    "\n",
    "Помимо самого скрипта, мы можем положить в MapReduce любой другой файл, который может пригодиться для работы программы. Например при подсчете количества слов мы бы хотели выкинуть \"стоп-слова\". Их количество скорее всего не очень большое поэтому смело может передавать их обычным файлом. Hadoop гарантирует, что доставит все файлы ко всем машинам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -cat /user/tweets/top10/part-* > stop-words.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Хозяйке на заметку\n",
    "\n",
    "В питоне уже есть хорошая стандартная библиотека, которая позволяет вам гораздо удобнее работать с такимим стримовыми данными. Давайте напишем новую задачу со стоп словами, чтобы они смотрелись поприличнее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing wordcount2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount2.py\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "import re\n",
    "from itertools import groupby\n",
    "\n",
    "\n",
    "def csv_stream():\n",
    "    return csv.reader(iter(sys.stdin.readline, ''))\n",
    "\n",
    "def kv_stream(sep=\"\\t\"):\n",
    "    return map(lambda x: x.split(sep), sys.stdin)\n",
    "\n",
    "\n",
    "def mapper():\n",
    "    pattern = re.compile(r\"[a-z]+\")\n",
    "    for row in csv_stream():\n",
    "        content = row[2]\n",
    "        for match in pattern.finditer(content.lower()):\n",
    "            word = match.group(0)\n",
    "            print(\"{}\\t{}\".format(word, 1))\n",
    "\n",
    "\n",
    "def reducer():\n",
    "    for key, group in groupby(kv_stream(), lambda x: x[0]):\n",
    "        word = key\n",
    "        number = sum(int(x) for _, x in group)\n",
    "        print(\"{}\\t{}\".format(word, number))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mr_command = sys.argv[1]\n",
    "    {\n",
    "        'map': mapper,\n",
    "        'reduce': reducer\n",
    "    }[mr_command]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing top10-2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top10-2.py\n",
    "\n",
    "import sys\n",
    "import collections\n",
    "from itertools import islice\n",
    "\n",
    "def build_stop_words():\n",
    "    with open('stop-words.txt', 'r') as f:\n",
    "        stop_words = {x.split('\\t')[0] for x in f}\n",
    "    return stop_words\n",
    "\n",
    "def kv_stream(sep=\"\\t\"):\n",
    "    return map(lambda x: x.split(sep), sys.stdin)\n",
    "\n",
    "def rewind():\n",
    "    collections.deque(sys.stdin, maxlen=0)\n",
    "\n",
    "def mapper():\n",
    "    for key, value in kv_stream():\n",
    "        print(\"{}+{}\\t\".format(key, value.strip()))\n",
    "\n",
    "def reducer():\n",
    "    stop_words = build_stop_words()\n",
    "    first_10_stream = islice(filter(lambda x: x[0] not in stop_words, kv_stream('+')), 10)\n",
    "    \n",
    "    for word, count in first_10_stream:\n",
    "        print(\"{}\\t{}\".format(word, count.strip()))\n",
    "    rewind()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mr_command = sys.argv[1]\n",
    "    {\n",
    "        'map': mapper,\n",
    "        'reduce': reducer\n",
    "    }[mr_command]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t\t3015051\r\n",
      "co\t2833375\r\n",
      "https\t2454132\r\n",
      "the\t591885\r\n",
      "to\t589004\r\n",
      "in\t457433\r\n",
      "a\t412888\r\n",
      "s\t397889\r\n",
      "http\t375299\r\n",
      "of\t350983\r\n"
     ]
    }
   ],
   "source": [
    "! cat stop-words.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/tweets/top10-stop-words/': No such file or directory\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-3.2.2.jar] /tmp/streamjob7790193568073565258.jar tmpDir=null\n",
      "2022-01-30 12:37:32,981 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net/10.128.0.26:8032\n",
      "2022-01-30 12:37:33,199 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net/10.128.0.26:10200\n",
      "2022-01-30 12:37:33,252 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net/10.128.0.26:8032\n",
      "2022-01-30 12:37:33,255 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net/10.128.0.26:10200\n",
      "2022-01-30 12:37:33,467 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1643538737352_0004\n",
      "2022-01-30 12:37:34,595 INFO mapred.FileInputFormat: Total input files to process : 3\n",
      "2022-01-30 12:37:35,091 INFO mapreduce.JobSubmitter: number of splits:30\n",
      "2022-01-30 12:37:35,653 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1643538737352_0004\n",
      "2022-01-30 12:37:35,655 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2022-01-30 12:37:35,826 INFO conf.Configuration: resource-types.xml not found\n",
      "2022-01-30 12:37:35,827 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2022-01-30 12:37:35,896 INFO impl.YarnClientImpl: Submitted application application_1643538737352_0004\n",
      "2022-01-30 12:37:35,934 INFO mapreduce.Job: The url to track the job: http://rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net:8088/proxy/application_1643538737352_0004/\n",
      "2022-01-30 12:37:35,936 INFO mapreduce.Job: Running job: job_1643538737352_0004\n",
      "2022-01-30 12:37:42,010 INFO mapreduce.Job: Job job_1643538737352_0004 running in uber mode : false\n",
      "2022-01-30 12:37:42,012 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2022-01-30 12:37:48,168 INFO mapreduce.Job:  map 3% reduce 0%\n",
      "2022-01-30 12:37:49,175 INFO mapreduce.Job:  map 7% reduce 0%\n",
      "2022-01-30 12:37:51,282 INFO mapreduce.Job:  map 23% reduce 0%\n",
      "2022-01-30 12:37:54,389 INFO mapreduce.Job:  map 30% reduce 0%\n",
      "2022-01-30 12:37:57,524 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2022-01-30 12:37:59,575 INFO mapreduce.Job:  map 43% reduce 0%\n",
      "2022-01-30 12:38:00,630 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "2022-01-30 12:38:03,761 INFO mapreduce.Job:  map 53% reduce 0%\n",
      "2022-01-30 12:38:04,768 INFO mapreduce.Job:  map 60% reduce 0%\n",
      "2022-01-30 12:38:05,775 INFO mapreduce.Job:  map 63% reduce 0%\n",
      "2022-01-30 12:38:06,784 INFO mapreduce.Job:  map 70% reduce 0%\n",
      "2022-01-30 12:38:07,842 INFO mapreduce.Job:  map 73% reduce 23%\n",
      "2022-01-30 12:38:08,885 INFO mapreduce.Job:  map 77% reduce 23%\n",
      "2022-01-30 12:38:09,891 INFO mapreduce.Job:  map 80% reduce 23%\n",
      "2022-01-30 12:38:10,899 INFO mapreduce.Job:  map 83% reduce 23%\n",
      "2022-01-30 12:38:11,915 INFO mapreduce.Job:  map 87% reduce 23%\n",
      "2022-01-30 12:38:13,976 INFO mapreduce.Job:  map 90% reduce 29%\n",
      "2022-01-30 12:38:14,981 INFO mapreduce.Job:  map 97% reduce 29%\n",
      "2022-01-30 12:38:16,991 INFO mapreduce.Job:  map 100% reduce 29%\n",
      "2022-01-30 12:38:20,008 INFO mapreduce.Job:  map 100% reduce 47%\n",
      "2022-01-30 12:38:26,044 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2022-01-30 12:38:26,053 INFO mapreduce.Job: Job job_1643538737352_0004 completed successfully\n",
      "2022-01-30 12:38:26,141 INFO mapreduce.Job: Counters: 56\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=13564266\n",
      "\t\tFILE: Number of bytes written=35393181\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=31346018\n",
      "\t\tHDFS: Number of bytes written=109\n",
      "\t\tHDFS: Number of read operations=95\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=3\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=30\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=22\n",
      "\t\tRack-local map tasks=8\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=432831\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=190956\n",
      "\t\tTotal time spent by all map tasks (ms)=144277\n",
      "\t\tTotal time spent by all reduce tasks (ms)=31826\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=144277\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=31826\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=443218944\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=195538944\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2831736\n",
      "\t\tMap output records=2831736\n",
      "\t\tMap output bytes=33191556\n",
      "\t\tMap output materialized bytes=14323416\n",
      "\t\tInput split bytes=4230\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=2831736\n",
      "\t\tReduce shuffle bytes=14323416\n",
      "\t\tReduce input records=2831736\n",
      "\t\tReduce output records=10\n",
      "\t\tSpilled Records=5663472\n",
      "\t\tShuffled Maps =30\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=30\n",
      "\t\tGC time elapsed (ms)=4000\n",
      "\t\tCPU time spent (ms)=90000\n",
      "\t\tPhysical memory (bytes) snapshot=13258067968\n",
      "\t\tVirtual memory (bytes) snapshot=137192054784\n",
      "\t\tTotal committed heap usage (bytes)=13457948672\n",
      "\t\tPeak Map Physical memory (bytes)=499060736\n",
      "\t\tPeak Map Virtual memory (bytes)=4346630144\n",
      "\t\tPeak Reduce Physical memory (bytes)=571691008\n",
      "\t\tPeak Reduce Virtual memory (bytes)=7014494208\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=31341788\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=109\n",
      "2022-01-30 12:38:26,141 INFO streaming.StreamJob: Output directory: /user/tweets/top10-stop-words/\n",
      "CPU times: user 927 ms, sys: 221 ms, total: 1.15 s\n",
      "Wall time: 57.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "! hdfs dfs -rm -r /user/tweets/top10-stop-words/ || true\n",
    "! yarn jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.name=\"top-10-stop-words\" \\\n",
    "-D mapreduce.job.reduces=1 \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "-D mapreduce.map.output.key.field.separator='+' \\\n",
    "-files top10-2.py,stop-words.txt \\\n",
    "-mapper \"python top10-2.py map\" \\\n",
    "-reducer \"python top10-2.py reduce\" \\\n",
    "-input /user/tweets/result/ \\\n",
    "-output /user/tweets/top10-stop-words/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\t287232\r\n",
      "for\t272995\r\n",
      "and\t247749\r\n",
      "is\t246856\r\n",
      "on\t210172\r\n",
      "you\t196950\r\n",
      "trump\t169520\r\n",
      "news\t156101\r\n",
      "it\t152816\r\n",
      "with\t134178\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /user/tweets/top10-stop-words/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ускоряем вычисления \n",
    "\n",
    "Несмотря на все оптимизации внутри Hadoop, самое узкое место - это передача данных от mapper к reducer. Таким образом если у нас получиться ускорить выполнение ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing wordcount3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount3.py\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "import re\n",
    "from itertools import groupby\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def csv_stream():\n",
    "    return csv.reader(iter(sys.stdin.readline, ''))\n",
    "\n",
    "def kv_stream(sep=\"\\t\"):\n",
    "    return map(lambda x: x.split(sep), sys.stdin)\n",
    "\n",
    "\n",
    "def mapper():\n",
    "    counter = Counter()\n",
    "    pattern = re.compile(r\"[a-z]+\")\n",
    "    for row in csv_stream():\n",
    "        content = row[2]\n",
    "        for match in pattern.finditer(content.lower()):\n",
    "            word = match.group(0)\n",
    "            counter[word] += 1\n",
    "    \n",
    "    for word, number in counter.items():\n",
    "        print(\"{}\\t{}\".format(word, number))\n",
    "\n",
    "\n",
    "def reducer():\n",
    "    for key, group in groupby(kv_stream(), lambda x: x[0]):\n",
    "        word = key\n",
    "        number = sum(int(x) for _, x in group)\n",
    "        print(\"{}\\t{}\".format(word, number))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mr_command = sys.argv[1]\n",
    "    {\n",
    "        'map': mapper,\n",
    "        'reduce': reducer\n",
    "    }[mr_command]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/tweets/result-fast1': No such file or directory\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-3.2.2.jar] /tmp/streamjob3815495773780317704.jar tmpDir=null\n",
      "2022-01-30 12:40:21,324 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net/10.128.0.26:8032\n",
      "2022-01-30 12:40:21,551 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net/10.128.0.26:10200\n",
      "2022-01-30 12:40:21,585 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net/10.128.0.26:8032\n",
      "2022-01-30 12:40:21,586 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net/10.128.0.26:10200\n",
      "2022-01-30 12:40:21,805 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1643538737352_0005\n",
      "2022-01-30 12:40:22,949 INFO mapred.FileInputFormat: Total input files to process : 13\n",
      "2022-01-30 12:40:23,439 INFO mapreduce.JobSubmitter: number of splits:37\n",
      "2022-01-30 12:40:23,995 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1643538737352_0005\n",
      "2022-01-30 12:40:23,997 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2022-01-30 12:40:24,159 INFO conf.Configuration: resource-types.xml not found\n",
      "2022-01-30 12:40:24,160 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2022-01-30 12:40:24,242 INFO impl.YarnClientImpl: Submitted application application_1643538737352_0005\n",
      "2022-01-30 12:40:24,279 INFO mapreduce.Job: The url to track the job: http://rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net:8088/proxy/application_1643538737352_0005/\n",
      "2022-01-30 12:40:24,280 INFO mapreduce.Job: Running job: job_1643538737352_0005\n",
      "2022-01-30 12:40:29,427 INFO mapreduce.Job: Job job_1643538737352_0005 running in uber mode : false\n",
      "2022-01-30 12:40:29,428 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2022-01-30 12:40:38,746 INFO mapreduce.Job:  map 5% reduce 0%\n",
      "2022-01-30 12:40:40,762 INFO mapreduce.Job:  map 16% reduce 0%\n",
      "2022-01-30 12:40:42,776 INFO mapreduce.Job:  map 27% reduce 0%\n",
      "2022-01-30 12:40:44,803 INFO mapreduce.Job:  map 32% reduce 0%\n",
      "2022-01-30 12:40:48,852 INFO mapreduce.Job:  map 35% reduce 0%\n",
      "2022-01-30 12:40:49,858 INFO mapreduce.Job:  map 38% reduce 0%\n",
      "2022-01-30 12:40:50,886 INFO mapreduce.Job:  map 43% reduce 0%\n",
      "2022-01-30 12:40:51,891 INFO mapreduce.Job:  map 54% reduce 0%\n",
      "2022-01-30 12:40:58,930 INFO mapreduce.Job:  map 59% reduce 0%\n",
      "2022-01-30 12:40:59,937 INFO mapreduce.Job:  map 68% reduce 0%\n",
      "2022-01-30 12:41:01,947 INFO mapreduce.Job:  map 68% reduce 8%\n",
      "2022-01-30 12:41:02,952 INFO mapreduce.Job:  map 70% reduce 8%\n",
      "2022-01-30 12:41:06,971 INFO mapreduce.Job:  map 78% reduce 15%\n",
      "2022-01-30 12:41:07,976 INFO mapreduce.Job:  map 81% reduce 16%\n",
      "2022-01-30 12:41:09,987 INFO mapreduce.Job:  map 84% reduce 16%\n",
      "2022-01-30 12:41:11,996 INFO mapreduce.Job:  map 86% reduce 16%\n",
      "2022-01-30 12:41:13,001 INFO mapreduce.Job:  map 89% reduce 17%\n",
      "2022-01-30 12:41:14,005 INFO mapreduce.Job:  map 92% reduce 19%\n",
      "2022-01-30 12:41:16,014 INFO mapreduce.Job:  map 95% reduce 19%\n",
      "2022-01-30 12:41:18,023 INFO mapreduce.Job:  map 100% reduce 19%\n",
      "2022-01-30 12:41:19,028 INFO mapreduce.Job:  map 100% reduce 20%\n",
      "2022-01-30 12:41:20,033 INFO mapreduce.Job:  map 100% reduce 26%\n",
      "2022-01-30 12:41:23,050 INFO mapreduce.Job:  map 100% reduce 49%\n",
      "2022-01-30 12:41:24,055 INFO mapreduce.Job:  map 100% reduce 67%\n",
      "2022-01-30 12:41:25,060 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2022-01-30 12:41:25,067 INFO mapreduce.Job: Job job_1643538737352_0005 completed successfully\n",
      "2022-01-30 12:41:25,130 INFO mapreduce.Job: Counters: 56\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=17763203\n",
      "\t\tFILE: Number of bytes written=55429269\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1143550744\n",
      "\t\tHDFS: Number of bytes written=30359819\n",
      "\t\tHDFS: Number of read operations=126\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=9\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=37\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tData-local map tasks=28\n",
      "\t\tRack-local map tasks=9\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=782112\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=461130\n",
      "\t\tTotal time spent by all map tasks (ms)=260704\n",
      "\t\tTotal time spent by all reduce tasks (ms)=76855\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=260704\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=76855\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=800882688\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=472197120\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2946207\n",
      "\t\tMap output records=5204737\n",
      "\t\tMap output bytes=53033060\n",
      "\t\tMap output materialized bytes=28014548\n",
      "\t\tInput split bytes=5597\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=2831736\n",
      "\t\tReduce shuffle bytes=28014548\n",
      "\t\tReduce input records=5204737\n",
      "\t\tReduce output records=2831736\n",
      "\t\tSpilled Records=10409474\n",
      "\t\tShuffled Maps =111\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=111\n",
      "\t\tGC time elapsed (ms)=4991\n",
      "\t\tCPU time spent (ms)=97570\n",
      "\t\tPhysical memory (bytes) snapshot=13305065472\n",
      "\t\tVirtual memory (bytes) snapshot=181584760832\n",
      "\t\tTotal committed heap usage (bytes)=13201047552\n",
      "\t\tPeak Map Physical memory (bytes)=355721216\n",
      "\t\tPeak Map Virtual memory (bytes)=4343631872\n",
      "\t\tPeak Reduce Physical memory (bytes)=276492288\n",
      "\t\tPeak Reduce Virtual memory (bytes)=7016419328\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1143545147\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=30359819\n",
      "2022-01-30 12:41:25,132 INFO streaming.StreamJob: Output directory: /user/tweets/result-fast1/\n",
      "CPU times: user 1.11 s, sys: 235 ms, total: 1.35 s\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "! hdfs dfs -rm -r /user/tweets/result-fast1 || true\n",
    "! yarn jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.name=\"word-count\" \\\n",
    "-D mapreduce.job.reduces=3 \\\n",
    "-files ~/wordcount3.py \\\n",
    "-mapper \"python3 wordcount3.py map\" \\\n",
    "-reducer \"python3 wordcount3.py reduce\" \\\n",
    "-input /user/tweets/data/ \\\n",
    "-output /user/tweets/result-fast1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\t1726\r\n",
      "aaaaa\t7\r\n",
      "aaaaaaaaaaaaaa\t3\r\n",
      "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaannnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnddddddddddddddddddddddddddddddddddddd\t1\r\n",
      "aaaaaaaaaaaaaaaaaaah\t1\r\n",
      "aaaaaaaaaaaaand\t1\r\n",
      "aaaaaaaaannnnnnnnnnnddddddddddddd\t1\r\n",
      "aaaaaaaah\t1\r\n",
      "aaaaaaaamen\t1\r\n",
      "aaaaaaagh\t2\r\n",
      "cat: Unable to write to output stream.\r\n",
      "cat: Unable to write to output stream.\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /user/tweets/result-fast1/* | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако у этого решения есть **очень большой минус** - сложность по памяти **O(n)**. Это означает, что вычисление может упасть если данные попадутся неудачные. \n",
    "\n",
    "Важный принцип работы с большими данными - все алгоритмы должны работать меньше чем за O(n). Это относится не только к MapReduce, а в целом почти к любым инструментам обработки больших данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\r\n",
      "drwxr-xr-x   - hive   hadoop          0 2022-01-24 09:44 /user/hive\r\n",
      "drwxr-xr-x   - livy   hadoop          0 2022-01-30 10:44 /user/livy\r\n",
      "drwxr-xr-x   - ubuntu hadoop          0 2022-01-30 12:47 /user/tweets\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /user/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Используем комбайнер\n",
    "\n",
    "Чтобы побороться с этой бедой, воспользуемся дополнительным инструментом в Hadoop - Combiner. По сути это маленький Reduce, который запускается после маппера. Это позволяет уменьшить количество выходных данных с Map стадии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://habrastorage.org/getpro/habr/post_images/587/2d2/dfe/5872d2dfe12643665370708d225bc1d4.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/tweets/result-fast2': No such file or directory\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-3.2.2.jar] /tmp/streamjob8175970712716338649.jar tmpDir=null\n",
      "2022-01-30 12:43:12,573 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net/10.128.0.26:8032\n",
      "2022-01-30 12:43:12,796 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net/10.128.0.26:10200\n",
      "2022-01-30 12:43:12,831 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net/10.128.0.26:8032\n",
      "2022-01-30 12:43:12,832 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net/10.128.0.26:10200\n",
      "2022-01-30 12:43:13,045 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1643538737352_0006\n",
      "2022-01-30 12:43:13,774 INFO mapred.FileInputFormat: Total input files to process : 13\n",
      "2022-01-30 12:43:13,890 INFO mapreduce.JobSubmitter: number of splits:37\n",
      "2022-01-30 12:43:14,046 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1643538737352_0006\n",
      "2022-01-30 12:43:14,048 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2022-01-30 12:43:14,253 INFO conf.Configuration: resource-types.xml not found\n",
      "2022-01-30 12:43:14,254 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2022-01-30 12:43:14,323 INFO impl.YarnClientImpl: Submitted application application_1643538737352_0006\n",
      "2022-01-30 12:43:14,361 INFO mapreduce.Job: The url to track the job: http://rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net:8088/proxy/application_1643538737352_0006/\n",
      "2022-01-30 12:43:14,363 INFO mapreduce.Job: Running job: job_1643538737352_0006\n",
      "2022-01-30 12:43:19,440 INFO mapreduce.Job: Job job_1643538737352_0006 running in uber mode : false\n",
      "2022-01-30 12:43:19,441 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2022-01-30 12:43:30,683 INFO mapreduce.Job:  map 5% reduce 0%\n",
      "2022-01-30 12:43:34,713 INFO mapreduce.Job:  map 8% reduce 0%\n",
      "2022-01-30 12:43:35,728 INFO mapreduce.Job:  map 19% reduce 0%\n",
      "2022-01-30 12:43:37,759 INFO mapreduce.Job:  map 24% reduce 0%\n",
      "2022-01-30 12:43:38,766 INFO mapreduce.Job:  map 32% reduce 0%\n",
      "2022-01-30 12:43:44,880 INFO mapreduce.Job:  map 35% reduce 0%\n",
      "2022-01-30 12:43:45,886 INFO mapreduce.Job:  map 38% reduce 0%\n",
      "2022-01-30 12:43:46,897 INFO mapreduce.Job:  map 41% reduce 0%\n",
      "2022-01-30 12:43:48,967 INFO mapreduce.Job:  map 46% reduce 0%\n",
      "2022-01-30 12:43:50,011 INFO mapreduce.Job:  map 54% reduce 0%\n",
      "2022-01-30 12:43:55,112 INFO mapreduce.Job:  map 57% reduce 0%\n",
      "2022-01-30 12:43:56,128 INFO mapreduce.Job:  map 62% reduce 6%\n",
      "2022-01-30 12:43:58,203 INFO mapreduce.Job:  map 65% reduce 6%\n",
      "2022-01-30 12:43:59,209 INFO mapreduce.Job:  map 68% reduce 6%\n",
      "2022-01-30 12:44:00,216 INFO mapreduce.Job:  map 70% reduce 6%\n",
      "2022-01-30 12:44:01,284 INFO mapreduce.Job:  map 70% reduce 8%\n",
      "2022-01-30 12:44:02,290 INFO mapreduce.Job:  map 73% reduce 8%\n",
      "2022-01-30 12:44:03,306 INFO mapreduce.Job:  map 76% reduce 8%\n",
      "2022-01-30 12:44:04,339 INFO mapreduce.Job:  map 78% reduce 16%\n",
      "2022-01-30 12:44:05,344 INFO mapreduce.Job:  map 81% reduce 16%\n",
      "2022-01-30 12:44:07,395 INFO mapreduce.Job:  map 81% reduce 17%\n",
      "2022-01-30 12:44:08,424 INFO mapreduce.Job:  map 86% reduce 17%\n",
      "2022-01-30 12:44:10,495 INFO mapreduce.Job:  map 89% reduce 19%\n",
      "2022-01-30 12:44:11,548 INFO mapreduce.Job:  map 92% reduce 19%\n",
      "2022-01-30 12:44:13,586 INFO mapreduce.Job:  map 95% reduce 20%\n",
      "2022-01-30 12:44:14,594 INFO mapreduce.Job:  map 100% reduce 20%\n",
      "2022-01-30 12:44:16,603 INFO mapreduce.Job:  map 100% reduce 26%\n",
      "2022-01-30 12:44:19,617 INFO mapreduce.Job:  map 100% reduce 49%\n",
      "2022-01-30 12:44:20,621 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2022-01-30 12:44:21,640 INFO mapreduce.Job: Job job_1643538737352_0006 completed successfully\n",
      "2022-01-30 12:44:21,725 INFO mapreduce.Job: Counters: 56\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=17763203\n",
      "\t\tFILE: Number of bytes written=55444349\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1143550744\n",
      "\t\tHDFS: Number of bytes written=30359819\n",
      "\t\tHDFS: Number of read operations=126\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=9\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=37\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tData-local map tasks=33\n",
      "\t\tRack-local map tasks=4\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=995754\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=456660\n",
      "\t\tTotal time spent by all map tasks (ms)=331918\n",
      "\t\tTotal time spent by all reduce tasks (ms)=76110\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=331918\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=76110\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=1019652096\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=467619840\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2946207\n",
      "\t\tMap output records=41946754\n",
      "\t\tMap output bytes=313767903\n",
      "\t\tMap output materialized bytes=28014548\n",
      "\t\tInput split bytes=5597\n",
      "\t\tCombine input records=41946754\n",
      "\t\tCombine output records=5204737\n",
      "\t\tReduce input groups=2831736\n",
      "\t\tReduce shuffle bytes=28014548\n",
      "\t\tReduce input records=5204737\n",
      "\t\tReduce output records=2831736\n",
      "\t\tSpilled Records=10409474\n",
      "\t\tShuffled Maps =111\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=111\n",
      "\t\tGC time elapsed (ms)=4179\n",
      "\t\tCPU time spent (ms)=179710\n",
      "\t\tPhysical memory (bytes) snapshot=13608513536\n",
      "\t\tVirtual memory (bytes) snapshot=181591515136\n",
      "\t\tTotal committed heap usage (bytes)=13443268608\n",
      "\t\tPeak Map Physical memory (bytes)=373661696\n",
      "\t\tPeak Map Virtual memory (bytes)=4341764096\n",
      "\t\tPeak Reduce Physical memory (bytes)=296763392\n",
      "\t\tPeak Reduce Virtual memory (bytes)=7017275392\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1143545147\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=30359819\n",
      "2022-01-30 12:44:21,726 INFO streaming.StreamJob: Output directory: /user/tweets/result-fast2/\n",
      "CPU times: user 1.24 s, sys: 255 ms, total: 1.5 s\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "! hdfs dfs -rm -r /user/tweets/result-fast2 || true\n",
    "! yarn jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.name=\"word-count\" \\\n",
    "-D mapreduce.job.reduces=3 \\\n",
    "-files ~/wordcount2.py \\\n",
    "-mapper \"python3 wordcount2.py map\" \\\n",
    "-combiner \"python3 wordcount2.py reduce\" \\\n",
    "-reducer \"python3 wordcount2.py reduce\" \\\n",
    "-input /user/tweets/data/ \\\n",
    "-output /user/tweets/result-fast2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\t1726\n",
      "aaaaa\t7\n",
      "aaaaaaaaaaaaaa\t3\n",
      "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaannnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnddddddddddddddddddddddddddddddddddddd\t1\n",
      "aaaaaaaaaaaaaaaaaaah\t1\n",
      "aaaaaaaaaaaaand\t1\n",
      "aaaaaaaaannnnnnnnnnnddddddddddddd\t1\n",
      "aaaaaaaah\t1\n",
      "aaaaaaaamen\t1\n",
      "aaaaaaagh\t2\n",
      "cat: Unable to write to output stream.\n",
      "cat: Unable to write to output stream.\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /user/tweets/result-fast1/* | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто combiner может просто совпадать с reducer однако это не всегда так по следующей причине - combiner не имеет права менять формат вывода после стадии map.\n",
    "\n",
    "Hadoop самостоятельно опеределяет целесообразность запуска combiner и может его не запускать вовсе.\n",
    "Или например задача может вообще не подходить под такую модель запуска. Если мы ищем среднее, то нельзя заранее подсчитывать среднее на стадии combiner - макмимум, что мы там можем запустить - это подсчет количество и суммы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting top10-3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top10-3.py\n",
    "\n",
    "import sys\n",
    "import collections\n",
    "from itertools import islice\n",
    "\n",
    "def build_stop_words():\n",
    "    with open('stop-words.txt', 'r') as f:\n",
    "        stop_words = {x.split('\\t')[0] for x in f}\n",
    "    return stop_words\n",
    "\n",
    "def kv_stream(sep=\"\\t\"):\n",
    "    return map(lambda x: x.split(sep), sys.stdin)\n",
    "\n",
    "def rewind():\n",
    "    collections.deque(sys.stdin, maxlen=0)\n",
    "\n",
    "def mapper():\n",
    "    for key, value in kv_stream():\n",
    "        print(\"{}+{}\\t\".format(key, value.strip()))\n",
    "\n",
    "def reducer():\n",
    "    stop_words = build_stop_words()\n",
    "    first_10_stream = islice(filter(lambda x: x[0] not in stop_words, kv_stream('+')), 10)\n",
    "    \n",
    "    for word, count in first_10_stream:\n",
    "        print(\"{}\\t{}\".format(word, count.strip()))\n",
    "    rewind()\n",
    "    \n",
    "def combiner():\n",
    "    stop_words = build_stop_words()\n",
    "    first_10_stream = islice(filter(lambda x: x[0] not in stop_words, kv_stream('+')), 10)\n",
    "    \n",
    "    for word, count in first_10_stream:\n",
    "        print(\"{}+{}\\t\".format(word, count.strip()))\n",
    "    rewind()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mr_command = sys.argv[1]\n",
    "    {\n",
    "        'map': mapper,\n",
    "        'reduce': reducer,\n",
    "        'combiner': combiner\n",
    "    }[mr_command]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/tweets/top10-fast': No such file or directory\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-3.2.2.jar] /tmp/streamjob9119716366048105986.jar tmpDir=null\n",
      "2022-01-30 12:47:15,994 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net/10.128.0.26:8032\n",
      "2022-01-30 12:47:16,213 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net/10.128.0.26:10200\n",
      "2022-01-30 12:47:16,248 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net/10.128.0.26:8032\n",
      "2022-01-30 12:47:16,249 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net/10.128.0.26:10200\n",
      "2022-01-30 12:47:16,445 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1643538737352_0007\n",
      "2022-01-30 12:47:17,196 INFO mapred.FileInputFormat: Total input files to process : 3\n",
      "2022-01-30 12:47:17,306 INFO mapreduce.JobSubmitter: number of splits:30\n",
      "2022-01-30 12:47:17,861 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1643538737352_0007\n",
      "2022-01-30 12:47:17,862 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2022-01-30 12:47:18,048 INFO conf.Configuration: resource-types.xml not found\n",
      "2022-01-30 12:47:18,049 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2022-01-30 12:47:18,123 INFO impl.YarnClientImpl: Submitted application application_1643538737352_0007\n",
      "2022-01-30 12:47:18,153 INFO mapreduce.Job: The url to track the job: http://rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net:8088/proxy/application_1643538737352_0007/\n",
      "2022-01-30 12:47:18,155 INFO mapreduce.Job: Running job: job_1643538737352_0007\n",
      "2022-01-30 12:47:23,243 INFO mapreduce.Job: Job job_1643538737352_0007 running in uber mode : false\n",
      "2022-01-30 12:47:23,244 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2022-01-30 12:47:29,397 INFO mapreduce.Job:  map 3% reduce 0%\n",
      "2022-01-30 12:47:30,405 INFO mapreduce.Job:  map 7% reduce 0%\n",
      "2022-01-30 12:47:31,416 INFO mapreduce.Job:  map 10% reduce 0%\n",
      "2022-01-30 12:47:33,483 INFO mapreduce.Job:  map 13% reduce 0%\n",
      "2022-01-30 12:47:34,495 INFO mapreduce.Job:  map 30% reduce 0%\n",
      "2022-01-30 12:47:36,511 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2022-01-30 12:47:39,544 INFO mapreduce.Job:  map 40% reduce 0%\n",
      "2022-01-30 12:47:41,562 INFO mapreduce.Job:  map 43% reduce 0%\n",
      "2022-01-30 12:47:42,573 INFO mapreduce.Job:  map 47% reduce 0%\n",
      "2022-01-30 12:47:44,637 INFO mapreduce.Job:  map 63% reduce 0%\n",
      "2022-01-30 12:47:48,696 INFO mapreduce.Job:  map 70% reduce 0%\n",
      "2022-01-30 12:47:49,710 INFO mapreduce.Job:  map 73% reduce 0%\n",
      "2022-01-30 12:47:50,716 INFO mapreduce.Job:  map 73% reduce 23%\n",
      "2022-01-30 12:47:52,783 INFO mapreduce.Job:  map 77% reduce 23%\n",
      "2022-01-30 12:47:53,788 INFO mapreduce.Job:  map 87% reduce 23%\n",
      "2022-01-30 12:47:54,793 INFO mapreduce.Job:  map 90% reduce 23%\n",
      "2022-01-30 12:47:55,801 INFO mapreduce.Job:  map 93% reduce 23%\n",
      "2022-01-30 12:47:56,806 INFO mapreduce.Job:  map 97% reduce 30%\n",
      "2022-01-30 12:47:57,810 INFO mapreduce.Job:  map 100% reduce 30%\n",
      "2022-01-30 12:47:58,817 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2022-01-30 12:48:00,835 INFO mapreduce.Job: Job job_1643538737352_0007 completed successfully\n",
      "2022-01-30 12:48:00,921 INFO mapreduce.Job: Counters: 56\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2185\n",
      "\t\tFILE: Number of bytes written=7522858\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=31346198\n",
      "\t\tHDFS: Number of bytes written=109\n",
      "\t\tHDFS: Number of read operations=95\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=3\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=30\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=23\n",
      "\t\tRack-local map tasks=7\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=500229\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=134328\n",
      "\t\tTotal time spent by all map tasks (ms)=166743\n",
      "\t\tTotal time spent by all reduce tasks (ms)=22388\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=166743\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=22388\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=512234496\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=137551872\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2831736\n",
      "\t\tMap output records=2831736\n",
      "\t\tMap output bytes=33191556\n",
      "\t\tMap output materialized bytes=3766\n",
      "\t\tInput split bytes=4410\n",
      "\t\tCombine input records=2831736\n",
      "\t\tCombine output records=300\n",
      "\t\tReduce input groups=300\n",
      "\t\tReduce shuffle bytes=3766\n",
      "\t\tReduce input records=300\n",
      "\t\tReduce output records=10\n",
      "\t\tSpilled Records=600\n",
      "\t\tShuffled Maps =30\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=30\n",
      "\t\tGC time elapsed (ms)=4374\n",
      "\t\tCPU time spent (ms)=90860\n",
      "\t\tPhysical memory (bytes) snapshot=13220569088\n",
      "\t\tVirtual memory (bytes) snapshot=137242017792\n",
      "\t\tTotal committed heap usage (bytes)=13389791232\n",
      "\t\tPeak Map Physical memory (bytes)=493092864\n",
      "\t\tPeak Map Virtual memory (bytes)=4352643072\n",
      "\t\tPeak Reduce Physical memory (bytes)=256942080\n",
      "\t\tPeak Reduce Virtual memory (bytes)=7014510592\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=31341788\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=109\n",
      "2022-01-30 12:48:00,922 INFO streaming.StreamJob: Output directory: /user/tweets/top10-fast/\n",
      "CPU times: user 821 ms, sys: 197 ms, total: 1.02 s\n",
      "Wall time: 49.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "! hdfs dfs -rm -r /user/tweets/top10-fast || true\n",
    "! yarn jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.name=\"word-count\" \\\n",
    "-D mapreduce.job.reduces=1 \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "-D mapreduce.map.output.key.field.separator='+' \\\n",
    "-files ~/top10-3.py,stop-words.txt \\\n",
    "-mapper \"python3 top10-3.py map\" \\\n",
    "-combiner \"python3 top10-3.py combiner\" \\\n",
    "-reducer \"python3 top10-3.py reduce\" \\\n",
    "-input /user/tweets/result-fast1 \\\n",
    "-output /user/tweets/top10-fast/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\t287232\r\n",
      "for\t272995\r\n",
      "and\t247749\r\n",
      "is\t246856\r\n",
      "on\t210172\r\n",
      "you\t196950\r\n",
      "trump\t169520\r\n",
      "news\t156101\r\n",
      "it\t152816\r\n",
      "with\t134178\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /user/tweets/top10-fast/* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кастомный Partitioner\n",
    "\n",
    "В Hadoop MapReduce можно указывать свой кастомный партишенер, который будет определять, как разбивать данные по редюсерам.\n",
    "\n",
    "Это бывает важно, когда вы используете сложный ключ и много редюсеров - вполне возможно вы захотите, чтобы такие ключи определенным образом распределялись по редюс-задачам.\n",
    "\n",
    "Для наглядности давайте решим такую задачу - для каждого пользователя подсчитаем, на каких языках он писал твиты и в каком количестве"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lang-distribution.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lang-distribution.py\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "from collections import Counter\n",
    "from itertools import groupby\n",
    "\n",
    "def kv_stream(sep=\"\\t\"):\n",
    "    return map(lambda x: x.split(sep), sys.stdin)\n",
    "\n",
    "def csv_stream():\n",
    "    return csv.reader(iter(sys.stdin.readline, ''))\n",
    "\n",
    "def mapper():\n",
    "    for row in csv_stream():\n",
    "        author, lang = row[1], row[4]\n",
    "        print(\"{}+{}\\t1\".format(author.strip(), lang.strip()))\n",
    "\n",
    "def reducer():\n",
    "    for author, records in groupby(kv_stream('+'), lambda x: x[0]):\n",
    "        langs_stream = (x.split('\\t') for _, x in records)\n",
    "        for lang, group in groupby(langs_stream, lambda x: x[0]):\n",
    "            count = sum(int(x) for _, x in group)\n",
    "            print(\"{}+{}\\t{}\".format(author, lang, count))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mr_command = sys.argv[1]\n",
    "    {\n",
    "        'map': mapper,\n",
    "        'reduce': reducer,\n",
    "    }[mr_command]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметры\n",
    "\n",
    "```\n",
    "-D mapred.partitioner.class=org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "-D mapreduce.partition.keypartitioner.options=\"-k1,1\" \\\n",
    "```\n",
    "\n",
    "Указывают на то, что разделятся на редюсеры записи должны не по полному ключу, а только по первой его части"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/tweets/lang-dist\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-3.2.2.jar] /tmp/streamjob4237600768320552868.jar tmpDir=null\n",
      "2022-01-30 13:56:15,431 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net/10.128.0.26:8032\n",
      "2022-01-30 13:56:15,676 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net/10.128.0.26:10200\n",
      "2022-01-30 13:56:15,708 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net/10.128.0.26:8032\n",
      "2022-01-30 13:56:15,709 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net/10.128.0.26:10200\n",
      "2022-01-30 13:56:15,934 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1643547082499_0003\n",
      "2022-01-30 13:56:16,305 INFO mapred.FileInputFormat: Total input files to process : 13\n",
      "2022-01-30 13:56:16,835 INFO mapreduce.JobSubmitter: number of splits:37\n",
      "2022-01-30 13:56:17,413 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1643547082499_0003\n",
      "2022-01-30 13:56:17,415 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2022-01-30 13:56:17,584 INFO conf.Configuration: resource-types.xml not found\n",
      "2022-01-30 13:56:17,584 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2022-01-30 13:56:17,664 INFO impl.YarnClientImpl: Submitted application application_1643547082499_0003\n",
      "2022-01-30 13:56:17,714 INFO mapreduce.Job: The url to track the job: http://rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net:8088/proxy/application_1643547082499_0003/\n",
      "2022-01-30 13:56:17,716 INFO mapreduce.Job: Running job: job_1643547082499_0003\n",
      "2022-01-30 13:56:23,802 INFO mapreduce.Job: Job job_1643547082499_0003 running in uber mode : false\n",
      "2022-01-30 13:56:23,803 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2022-01-30 13:56:29,974 INFO mapreduce.Job:  map 3% reduce 0%\n",
      "2022-01-30 13:56:30,993 INFO mapreduce.Job:  map 5% reduce 0%\n",
      "2022-01-30 13:56:34,125 INFO mapreduce.Job:  map 27% reduce 0%\n",
      "2022-01-30 13:56:36,149 INFO mapreduce.Job:  map 32% reduce 0%\n",
      "2022-01-30 13:56:42,187 INFO mapreduce.Job:  map 41% reduce 0%\n",
      "2022-01-30 13:56:43,194 INFO mapreduce.Job:  map 54% reduce 0%\n",
      "2022-01-30 13:56:50,231 INFO mapreduce.Job:  map 59% reduce 6%\n",
      "2022-01-30 13:56:51,237 INFO mapreduce.Job:  map 70% reduce 6%\n",
      "2022-01-30 13:56:52,243 INFO mapreduce.Job:  map 76% reduce 6%\n",
      "2022-01-30 13:56:56,264 INFO mapreduce.Job:  map 81% reduce 8%\n",
      "2022-01-30 13:56:57,269 INFO mapreduce.Job:  map 84% reduce 8%\n",
      "2022-01-30 13:56:58,275 INFO mapreduce.Job:  map 89% reduce 8%\n",
      "2022-01-30 13:56:59,282 INFO mapreduce.Job:  map 95% reduce 8%\n",
      "2022-01-30 13:57:02,297 INFO mapreduce.Job:  map 95% reduce 11%\n",
      "2022-01-30 13:57:03,303 INFO mapreduce.Job:  map 100% reduce 11%\n",
      "2022-01-30 13:57:05,315 INFO mapreduce.Job:  map 100% reduce 44%\n",
      "2022-01-30 13:57:06,322 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2022-01-30 13:57:07,348 INFO mapreduce.Job: Job job_1643547082499_0003 completed successfully\n",
      "2022-01-30 13:57:07,433 INFO mapreduce.Job: Counters: 56\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=359106\n",
      "\t\tFILE: Number of bytes written=10416684\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1143550744\n",
      "\t\tHDFS: Number of bytes written=380326\n",
      "\t\tHDFS: Number of read operations=126\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=9\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=37\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tData-local map tasks=35\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=728682\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=293124\n",
      "\t\tTotal time spent by all map tasks (ms)=242894\n",
      "\t\tTotal time spent by all reduce tasks (ms)=48854\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=242894\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=48854\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=746170368\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=300158976\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2946207\n",
      "\t\tMap output records=2946207\n",
      "\t\tMap output bytes=67013403\n",
      "\t\tMap output materialized bytes=365980\n",
      "\t\tInput split bytes=5597\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=16091\n",
      "\t\tReduce shuffle bytes=365980\n",
      "\t\tReduce input records=2946207\n",
      "\t\tReduce output records=16091\n",
      "\t\tSpilled Records=5892414\n",
      "\t\tShuffled Maps =111\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=111\n",
      "\t\tGC time elapsed (ms)=7399\n",
      "\t\tCPU time spent (ms)=87130\n",
      "\t\tPhysical memory (bytes) snapshot=14597185536\n",
      "\t\tVirtual memory (bytes) snapshot=181627994112\n",
      "\t\tTotal committed heap usage (bytes)=15625879552\n",
      "\t\tPeak Map Physical memory (bytes)=384790528\n",
      "\t\tPeak Map Virtual memory (bytes)=4346363904\n",
      "\t\tPeak Reduce Physical memory (bytes)=404279296\n",
      "\t\tPeak Reduce Virtual memory (bytes)=7021330432\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1143545147\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=380326\n",
      "2022-01-30 13:57:07,433 INFO streaming.StreamJob: Output directory: /user/tweets/lang-dist\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -rm -r /user/tweets/lang-dist || true\n",
    "! yarn jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.name=\"lang-dist\" \\\n",
    "-D mapreduce.job.reduces=3 \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options=\"-k1,1 -k2,2\" \\\n",
    "-D mapreduce.map.output.key.field.separator='+' \\\n",
    "-D mapred.partitioner.class=org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "-D mapreduce.partition.keypartitioner.options=\"-k1,1\" \\\n",
    "-files ~/lang-distribution.py \\\n",
    "-mapper \"python3 lang-distribution.py map\" \\\n",
    "-reducer \"python3 lang-distribution.py reduce\" \\\n",
    "-input /user/tweets/data/ \\\n",
    "-output /user/tweets/lang-dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1488REASONS+Russian\t50\r\n",
      "1488REASONS+Serbian\t1\r\n",
      "1488REASONS+Ukrainian\t1\r\n",
      "1D_NICOLE_+Albanian\t1\r\n",
      "1D_NICOLE_+English\t41\r\n",
      "1D_NICOLE_+Tagalog (Filipino)\t2\r\n",
      "1ERIK_LEE+English\t2\r\n",
      "459JISALGE+Russian\t1\r\n",
      "4MYSQUAD+Arabic\t5\r\n",
      "4MYSQUAD+Catalan\t1\r\n",
      "cat: Unable to write to output stream.\r\n",
      "cat: Unable to write to output stream.\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /user/tweets/lang-dist/* | head "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как дебажить ошибки\n",
    "\n",
    "Через yarn можно выводить логи приложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mistake.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mistake.py\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "import re\n",
    "from itertools import groupby\n",
    "\n",
    "\n",
    "def csv_stream():\n",
    "    return csv.reader(iter(sys.stdin.readline, ''))\n",
    "\n",
    "def kv_stream(sep=\"\\t\"):\n",
    "    return map(lambda x: x.split(sep), sys.stdin)\n",
    "\n",
    "\n",
    "def mapper():\n",
    "    pattern = re.compile(r\"[a-z]+\")\n",
    "    for row in csv_stream():\n",
    "        content = row[2]\n",
    "        for match in pattern.finditer(content.lower()):\n",
    "            word = match.group(0)\n",
    "            strange_number = 1.0 / (len(word) - 1)\n",
    "            print(\"{}\\t{}\".format(word, strange_number))\n",
    "\n",
    "\n",
    "def reducer():\n",
    "    for key, group in groupby(kv_stream(), lambda x: x[0]):\n",
    "        word = key\n",
    "        number = sum(float(x) for _, x in group)\n",
    "        print(\"{}\\t{}\".format(word, number))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mr_command = sys.argv[1]\n",
    "    {\n",
    "        'map': mapper,\n",
    "        'reduce': reducer\n",
    "    }[mr_command]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/tweets/mistake\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-3.2.2.jar] /tmp/streamjob2385587628438507101.jar tmpDir=null\n",
      "2022-01-30 14:12:15,607 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net/10.128.0.26:8032\n",
      "2022-01-30 14:12:15,839 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net/10.128.0.26:10200\n",
      "2022-01-30 14:12:15,884 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net/10.128.0.26:8032\n",
      "2022-01-30 14:12:15,885 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net/10.128.0.26:10200\n",
      "2022-01-30 14:12:16,123 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1643547082499_0005\n",
      "2022-01-30 14:12:16,862 INFO mapred.FileInputFormat: Total input files to process : 13\n",
      "2022-01-30 14:12:16,948 INFO mapreduce.JobSubmitter: number of splits:37\n",
      "2022-01-30 14:12:17,113 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1643547082499_0005\n",
      "2022-01-30 14:12:17,115 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2022-01-30 14:12:17,325 INFO conf.Configuration: resource-types.xml not found\n",
      "2022-01-30 14:12:17,325 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2022-01-30 14:12:17,407 INFO impl.YarnClientImpl: Submitted application application_1643547082499_0005\n",
      "2022-01-30 14:12:17,462 INFO mapreduce.Job: The url to track the job: http://rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net:8088/proxy/application_1643547082499_0005/\n",
      "2022-01-30 14:12:17,463 INFO mapreduce.Job: Running job: job_1643547082499_0005\n",
      "2022-01-30 14:12:22,542 INFO mapreduce.Job: Job job_1643547082499_0005 running in uber mode : false\n",
      "2022-01-30 14:12:22,543 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2022-01-30 14:12:26,632 INFO mapreduce.Job: Task Id : attempt_1643547082499_0005_m_000004_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2022-01-30 14:12:26,647 INFO mapreduce.Job: Task Id : attempt_1643547082499_0005_m_000005_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2022-01-30 14:12:28,738 INFO mapreduce.Job: Task Id : attempt_1643547082499_0005_m_000000_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2022-01-30 14:12:28,755 INFO mapreduce.Job: Task Id : attempt_1643547082499_0005_m_000003_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2022-01-30 14:12:28,758 INFO mapreduce.Job: Task Id : attempt_1643547082499_0005_m_000002_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2022-01-30 14:12:28,762 INFO mapreduce.Job: Task Id : attempt_1643547082499_0005_m_000001_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2022-01-30 14:12:28,763 INFO mapreduce.Job: Task Id : attempt_1643547082499_0005_m_000011_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2022-01-30 14:12:28,766 INFO mapreduce.Job: Task Id : attempt_1643547082499_0005_m_000010_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2022-01-30 14:12:28,771 INFO mapreduce.Job: Task Id : attempt_1643547082499_0005_m_000022_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2022-01-30 14:12:28,774 INFO mapreduce.Job: Task Id : attempt_1643547082499_0005_m_000023_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2022-01-30 14:12:30,801 INFO mapreduce.Job: Task Id : attempt_1643547082499_0005_m_000005_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2022-01-30 14:12:30,809 INFO mapreduce.Job: Task Id : attempt_1643547082499_0005_m_000004_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2022-01-30 14:12:32,858 INFO mapreduce.Job: Task Id : attempt_1643547082499_0005_m_000024_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2022-01-30 14:12:32,864 INFO mapreduce.Job: Task Id : attempt_1643547082499_0005_m_000035_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2022-01-30 14:12:33,884 INFO mapreduce.Job: Task Id : attempt_1643547082499_0005_m_000015_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2022-01-30 14:12:33,887 INFO mapreduce.Job: Task Id : attempt_1643547082499_0005_m_000014_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2022-01-30 14:12:33,890 INFO mapreduce.Job: Task Id : attempt_1643547082499_0005_m_000000_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2022-01-30 14:12:34,909 INFO mapreduce.Job: Task Id : attempt_1643547082499_0005_m_000002_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2022-01-30 14:12:34,914 INFO mapreduce.Job: Task Id : attempt_1643547082499_0005_m_000003_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2022-01-30 14:12:34,919 INFO mapreduce.Job: Task Id : attempt_1643547082499_0005_m_000001_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2022-01-30 14:12:34,921 INFO mapreduce.Job: Task Id : attempt_1643547082499_0005_m_000011_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2022-01-30 14:12:36,970 INFO mapreduce.Job: Task Id : attempt_1643547082499_0005_m_000010_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2022-01-30 14:12:36,971 INFO mapreduce.Job: Task Id : attempt_1643547082499_0005_m_000024_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2022-01-30 14:12:37,983 INFO mapreduce.Job: Task Id : attempt_1643547082499_0005_m_000022_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2022-01-30 14:12:37,986 INFO mapreduce.Job: Task Id : attempt_1643547082499_0005_m_000035_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2022-01-30 14:12:39,002 INFO mapreduce.Job: Task Id : attempt_1643547082499_0005_m_000005_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2022-01-30 14:12:39,006 INFO mapreduce.Job: Task Id : attempt_1643547082499_0005_m_000023_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2022-01-30 14:12:39,017 INFO mapreduce.Job: Task Id : attempt_1643547082499_0005_m_000004_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2022-01-30 14:12:40,042 INFO mapreduce.Job: Task Id : attempt_1643547082499_0005_m_000014_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2022-01-30 14:12:40,044 INFO mapreduce.Job: Task Id : attempt_1643547082499_0005_m_000015_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2022-01-30 14:12:40,046 INFO mapreduce.Job: Task Id : attempt_1643547082499_0005_m_000003_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2022-01-30 14:12:40,049 INFO mapreduce.Job: Task Id : attempt_1643547082499_0005_m_000000_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2022-01-30 14:12:41,072 INFO mapreduce.Job: Task Id : attempt_1643547082499_0005_m_000011_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2022-01-30 14:12:42,095 INFO mapreduce.Job: Task Id : attempt_1643547082499_0005_m_000002_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2022-01-30 14:12:43,147 INFO mapreduce.Job: Task Id : attempt_1643547082499_0005_m_000001_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2022-01-30 14:12:43,159 INFO mapreduce.Job: Task Id : attempt_1643547082499_0005_m_000023_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2022-01-30 14:12:44,169 INFO mapreduce.Job: Task Id : attempt_1643547082499_0005_m_000022_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2022-01-30 14:12:44,171 INFO mapreduce.Job: Task Id : attempt_1643547082499_0005_m_000010_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2022-01-30 14:12:44,175 INFO mapreduce.Job: Task Id : attempt_1643547082499_0005_m_000024_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2022-01-30 14:12:44,176 INFO mapreduce.Job: Task Id : attempt_1643547082499_0005_m_000014_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2022-01-30 14:12:46,191 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2022-01-30 14:12:47,204 INFO mapreduce.Job: Job job_1643547082499_0005 failed with state FAILED due to: Task failed task_1643547082499_0005_m_000005\n",
      "Job failed as tasks failed. failedMaps:1 failedReduces:0 killedMaps:0 killedReduces: 0\n",
      "\n",
      "2022-01-30 14:12:47,275 INFO mapreduce.Job: Counters: 14\n",
      "\tJob Counters \n",
      "\t\tFailed map tasks=41\n",
      "\t\tKilled map tasks=36\n",
      "\t\tKilled reduce tasks=3\n",
      "\t\tLaunched map tasks=50\n",
      "\t\tOther local map tasks=36\n",
      "\t\tData-local map tasks=14\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=509280\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=169760\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=169760\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=521502720\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "2022-01-30 14:12:47,275 ERROR streaming.StreamJob: Job not successful!\n",
      "Streaming Command Failed!\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -rm -r /user/tweets/mistake || true\n",
    "! yarn jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.name=\"mistake\" \\\n",
    "-D mapreduce.job.reduces=3 \\\n",
    "-files ~/mistake.py \\\n",
    "-mapper \"python3 mistake.py map\" \\\n",
    "-reducer \"python3 mistake.py reduce\" \\\n",
    "-input /user/tweets/data/ \\\n",
    "-output /user/tweets/mistake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-30 14:13:23,022 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net/10.128.0.26:8032\n",
      "2022-01-30 14:13:23,269 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net/10.128.0.26:10200\n",
      "Container: container_1643547082499_0005_01_000026 on rc1a-dataproc-d-9r5dg188d90q4f2a.mdb.yandexcloud.net_44115\n",
      "LogAggregationType: AGGREGATED\n",
      "===============================================================================================================\n",
      "LogType:stderr\n",
      "LogLastModifiedTime:Sun Jan 30 14:12:53 +0000 2022\n",
      "LogLength:208\n",
      "LogContents:\n",
      "Traceback (most recent call last):\n",
      "  File \"mistake.py\", line 34, in <module>\n",
      "    {\n",
      "  File \"mistake.py\", line 21, in mapper\n",
      "    strange_number = 1.0 / (len(word) - 1)\n",
      "ZeroDivisionError: float division by zero\n",
      "\n",
      "End of LogType:stderr\n",
      "***********************************************************************\n",
      "\n",
      "Container: container_1643547082499_0005_01_000025 on rc1a-dataproc-d-9r5dg188d90q4f2a.mdb.yandexcloud.net_44115\n",
      "LogAggregationType: AGGREGATED\n",
      "===============================================================================================================\n",
      "LogType:stderr\n",
      "LogLastModifiedTime:Sun Jan 30 14:12:53 +0000 2022\n",
      "LogLength:208\n",
      "LogContents:\n",
      "Traceback (most recent call last):\n",
      "  File \"mistake.py\", line 34, in <module>\n",
      "    {\n",
      "  File \"mistake.py\", line 21, in mapper\n",
      "    strange_number = 1.0 / (len(word) - 1)\n",
      "ZeroDivisionError: float division by zero\n",
      "\n",
      "End of LogType:stderr\n",
      "***********************************************************************\n",
      "\n",
      "Container: container_1643547082499_0005_01_000031 on rc1a-dataproc-d-9r5dg188d90q4f2a.mdb.yandexcloud.net_44115\n",
      "LogAggregationType: AGGREGATED\n",
      "===============================================================================================================\n",
      "LogType:stderr\n",
      "LogLastModifiedTime:Sun Jan 30 14:12:53 +0000 2022\n",
      "LogLength:208\n",
      "LogContents:\n",
      "Traceback (most recent call last):\n",
      "  File \"mistake.py\", line 34, in <module>\n",
      "    {\n",
      "  File \"mistake.py\", line 21, in mapper\n",
      "    strange_number = 1.0 / (len(word) - 1)\n",
      "ZeroDivisionError: float division by zero\n",
      "\n",
      "End of LogType:stderr\n",
      "***********************************************************************\n"
     ]
    }
   ],
   "source": [
    "! yarn logs -applicationId application_1643547082499_0005 -log_files stderr | head -n 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идентификатор можно найти логах запуска, в интерфейсе или посмотреть список всех и найти там"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-30 14:15:27,368 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net/10.128.0.26:8032\n",
      "2022-01-30 14:15:27,611 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-mk7wm6vzosv08ggu.mdb.yandexcloud.net/10.128.0.26:10200\n",
      "Total number of applications (application-types: [], states: [SUBMITTED, ACCEPTED, RUNNING] and tags: []):0\n",
      "                Application-Id\t    Application-Name\t    Application-Type\t      User\t     Queue\t             State\t       Final-State\t       Progress\t                       Tracking-URL\n"
     ]
    }
   ],
   "source": [
    "! yarn application -list "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы заранее прикончить задачу, можно также использовать yarn\n",
    "\n",
    "```bash\n",
    "yarn application -kill <application_id>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Переносим результаты\n",
    "\n",
    "Все данные, с которыми мы работали только что, лежат на жестких дисках кластера и доступны только через hdfs\n",
    "\n",
    "Если мы готовы презентовать наш результат миру, нужно его переместить в s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   1 ubuntu hadoop          0 2022-01-30 12:38 /user/tweets/top10-stop-words/_SUCCESS\r\n",
      "-rw-r--r--   1 ubuntu hadoop        109 2022-01-30 12:38 /user/tweets/top10-stop-words/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /user/tweets/top10-stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-30 14:23:59,851 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2022-01-30 14:23:59,943 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2022-01-30 14:23:59,943 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\n",
      "2022-01-30 14:24:04,635 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\n",
      "2022-01-30 14:24:04,636 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\n",
      "2022-01-30 14:24:04,636 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cp /user/tweets/top10-stop-words/part-00000 s3a://lsml2022alexius/top10.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно проверять через интерфейс - файл оказался в бакете"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop жжет бабло\n",
    "\n",
    "<img src=\"http://vostokovod.ru/assets/images/blog/2013/000333.png\">\n",
    "\n",
    "Полноценный кластер - весьма дорогое удовольствие, поэтому отлючайте его после использования. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
