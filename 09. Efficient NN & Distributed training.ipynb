{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Запуск на устройствах пользователей\n",
    "\n",
    "Чтобы запустить стенд с TfJS , перейдите в директорию `tensorflowjs` и запустите\n",
    "\n",
    "```bash\n",
    "docker-compose up\n",
    "```\n",
    "\n",
    "После этого можно переходить по адресам\n",
    "* http://localhost:9090\n",
    "* http://localhost:9091"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9j81mzLx40eD"
   },
   "source": [
    "## PyTorch. DistributedDataParallel \n",
    "\n",
    "Это класс, скрывающий под капотом детали параллельного обучениия в PyTorch, применяется для обучения на кластере из нескольких компьютеров с несколькими GPU\n",
    "\n",
    "Внутри осуществляет разбивку данных по обработчикам, на backward шаге градиенты усредняются с использованием allreduce.\n",
    "\n",
    "Существует также `DataParallel` класс, но он работает в рамках одного процесса, используя потоки, тем самым он не рекомендуется к применению в силу ограничений GIL, `DistributedDataParallel` показывает себя лучше даже при обучении на одном компьютере."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A9i8Z9Ku4wGU",
    "outputId": "4b40cf10-d97e-4118-82ad-0fc6f5a5cacb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing launch_ddp_demo.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile launch_ddp_demo.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "\n",
    "    # инициализация группы процессов\n",
    "    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "class ToyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ToyModel, self).__init__()\n",
    "        self.net1 = nn.Linear(10, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.net2 = nn.Linear(10, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net2(self.relu(self.net1(x)))\n",
    "\n",
    "\n",
    "def demo_basic(rank, world_size):\n",
    "    print(f\"Базовый пример использованиия DDP с рангом {rank}.\")\n",
    "    setup(rank, world_size)\n",
    "\n",
    "    # создать модель и отправить её на GPU с id = rank\n",
    "    model = ToyModel()\n",
    "    ddp_model = DDP(model)\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = ddp_model(torch.randn(20, 10))\n",
    "    labels = torch.randn(20, 5)\n",
    "    loss_fn(outputs, labels).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    cleanup()\n",
    "\n",
    "\n",
    "def run_demo(demo_fn, world_size):\n",
    "    mp.spawn(demo_fn,\n",
    "             args=(world_size,),\n",
    "             nprocs=world_size,\n",
    "             join=True)\n",
    "    \n",
    "def demo_checkpoint(rank, world_size):\n",
    "    print(f\"Пример использованиия DDP с чекпоинтами с рангом {rank}.\")\n",
    "    setup(rank, world_size)\n",
    "\n",
    "    model = ToyModel().to(rank)\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n",
    "\n",
    "    CHECKPOINT_PATH = tempfile.gettempdir() + \"/model.checkpoint\"\n",
    "    if rank == 0:\n",
    "        # Все процессы должны начать с одних значений параметров и градиента,\n",
    "        # они также синхронизируются на backward шаге,\n",
    "        # поэтому достаточно сохранять модель в одном процессе\n",
    "        torch.save(ddp_model.state_dict(), CHECKPOINT_PATH)\n",
    "\n",
    "    # Используем barrier(), чтобы удостовериться,\n",
    "    # что процесс 1 загрузит модель после сохранения процессом 0\n",
    "    dist.barrier()\n",
    "    # опциии map_location\n",
    "    map_location = {'cuda:%d' % 0: 'cuda:%d' % rank}\n",
    "    ddp_model.load_state_dict(\n",
    "        torch.load(CHECKPOINT_PATH, map_location=map_location))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = ddp_model(torch.randn(20, 10))\n",
    "    labels = torch.randn(20, 5).to(rank)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    loss_fn(outputs, labels).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if rank == 0:\n",
    "        os.remove(CHECKPOINT_PATH)\n",
    "\n",
    "    cleanup()\n",
    "\n",
    "class ToyMpModel(nn.Module):\n",
    "    def __init__(self, dev0, dev1):\n",
    "        super(ToyMpModel, self).__init__()\n",
    "        self.dev0 = dev0\n",
    "        self.dev1 = dev1\n",
    "        self.net1 = torch.nn.Linear(10, 10).to(dev0)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.net2 = torch.nn.Linear(10, 5).to(dev1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.dev0)\n",
    "        x = self.relu(self.net1(x))\n",
    "        x = x.to(self.dev1)\n",
    "        return self.net2(x)\n",
    "\n",
    "def demo_model_parallel(rank, world_size):\n",
    "    print(f\"Пример использованиия DDP с параллельностью по модели с рангом {rank}.\")\n",
    "    setup(rank, world_size)\n",
    "\n",
    "    # настроим модель и устройства в этом процессе\n",
    "    dev0 = rank * 2\n",
    "    dev1 = rank * 2 + 1\n",
    "    mp_model = ToyMpModel(dev0, dev1)\n",
    "    ddp_mp_model = DDP(mp_model)\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.SGD(ddp_mp_model.parameters(), lr=0.001)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    # вывод будет на устройстве dev1\n",
    "    outputs = ddp_mp_model(torch.randn(20, 10))\n",
    "    labels = torch.randn(20, 5).to(dev1)\n",
    "    loss_fn(outputs, labels).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    cleanup()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    print(\"Количество GPU:\", n_gpus)\n",
    "    run_demo(demo_basic, 4)\n",
    "    run_demo(demo_checkpoint, n_gpus)\n",
    "    # run_demo(demo_model_parallel, n_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pTnDeykLA8wJ",
    "outputId": "cbc01f90-3501-48a3-f86e-a8fead68f44d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество GPU: 1\n",
      "Базовый пример использованиия DDP с рангом 2.\n",
      "Базовый пример использованиия DDP с рангом 0.\n",
      "Базовый пример использованиия DDP с рангом 3.\n",
      "Базовый пример использованиия DDP с рангом 1.\n",
      "Пример использованиия DDP с чекпоинтами с рангом 0.\n"
     ]
    }
   ],
   "source": [
    "! python3 launch_ddp_demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0YIfRC049CX"
   },
   "source": [
    "Для сохранениия и загрузки состояния обучения (checkpoint) используются функциии `torch.save()` и `torch.load()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJispqHd5C88"
   },
   "source": [
    "Model Parallel - использование нескольких устройств на одном обработчике."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_-HisF2HzVg"
   },
   "source": [
    "### Обучение MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oj92iYqL9wal",
    "outputId": "87d1a028-a42e-4762-82bb-3610ec518c16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting launch_ddp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile launch_ddp.py\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import torch.multiprocessing as mp\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-n', '--nodes', default=1, type=int, metavar='N',\n",
    "                        help='количество обработчиков (default: 1)')\n",
    "    parser.add_argument('-nr', '--nr', default=0, type=int,\n",
    "                        help='глобальный ранг')\n",
    "    parser.add_argument('--epochs', default=2, type=int, metavar='N',\n",
    "                        help='количество эпох обучениия')\n",
    "    args = parser.parse_args()\n",
    "    args.world_size = args.nodes\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '23334'\n",
    "    print(\"Run with args - {}\".format(str(args)))\n",
    "    mp.spawn(train, nprocs=args.nodes, args=(args,))\n",
    "\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(7*7*32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def train(rank, args):\n",
    "    dist.init_process_group(backend='gloo', init_method='env://', world_size=args.world_size, rank=rank)\n",
    "    torch.manual_seed(0)\n",
    "    \n",
    "    print(\"worker {}\".format(rank))\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = ConvNet()\n",
    "    torch.cuda.set_device(device)\n",
    "    model.cuda(device)\n",
    "    batch_size = 100\n",
    "    \n",
    "    # определим loss function и optimizer\n",
    "    criterion = nn.CrossEntropyLoss().cuda(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), 1e-4)\n",
    "    # Обернем модель в DDP\n",
    "    model = nn.parallel.DistributedDataParallel(model, device_ids=[device])\n",
    "    # Загрузка данных\n",
    "    train_dataset = torchvision.datasets.MNIST(root='./data',\n",
    "                                               train=True,\n",
    "                                               transform=transforms.ToTensor(),\n",
    "                                               download=True)\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset,\n",
    "                                                                    num_replicas=args.world_size,\n",
    "                                                                    rank=rank)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False,\n",
    "                                               num_workers=0,\n",
    "                                               pin_memory=True,\n",
    "                                               sampler=train_sampler)\n",
    "\n",
    "    start = datetime.now()\n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(args.epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.cuda(non_blocking=True)\n",
    "            labels = labels.cuda(non_blocking=True)\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print('[{}] Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(rank, epoch + 1, args.epochs, i + 1, total_step,\n",
    "                                                                         loss.item()))\n",
    "    if rank == 0:\n",
    "        print(\"Обучение завершено за: \" + str(datetime.now() - start))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z8gitSAujJC5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8WfZ2cl590da",
    "outputId": "a3cf9219-e045-4c81-cbe0-aa1434400275"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run with args - Namespace(epochs=2, nodes=2, nr=0, world_size=2)\n",
      "worker 0\n",
      "worker 1\n",
      "[0] Epoch [1/2], Step [100/300], Loss: 2.1675\n",
      "[1] Epoch [1/2], Step [100/300], Loss: 2.1775\n",
      "[0] Epoch [1/2], Step [200/300], Loss: 2.0043\n",
      "[1] Epoch [1/2], Step [200/300], Loss: 1.9923\n",
      "[1] Epoch [1/2], Step [300/300], Loss: 1.9118\n",
      "[0] Epoch [1/2], Step [300/300], Loss: 1.9282\n",
      "[0] Epoch [2/2], Step [100/300], Loss: 1.8029\n",
      "[1] Epoch [2/2], Step [100/300], Loss: 1.7870\n",
      "[0] Epoch [2/2], Step [200/300], Loss: 1.6588\n",
      "[1] Epoch [2/2], Step [200/300], Loss: 1.6494\n",
      "[1] Epoch [2/2], Step [300/300], Loss: 1.5918\n",
      "[0] Epoch [2/2], Step [300/300], Loss: 1.6320\n",
      "Обучение завершено за: 0:00:16.939810\n"
     ]
    }
   ],
   "source": [
    "! python3 launch_ddp.py --nodes 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nvs5y_2m-vyK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3j2XAdb88Ku"
   },
   "source": [
    "### Распределенное обучениеи с использованием PyTorch и Horovod для MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4B2UmzpUP51S",
    "outputId": "cf989d3a-a901-40ee-d9d3-3387c794a5de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting horovod[pytorch]\n",
      "  Downloading horovod-0.24.2.tar.gz (3.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.4 MB 5.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from horovod[pytorch]) (1.3.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from horovod[pytorch]) (5.4.8)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from horovod[pytorch]) (3.13)\n",
      "Requirement already satisfied: cffi>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from horovod[pytorch]) (1.15.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from horovod[pytorch]) (1.10.0+cu111)\n",
      "Collecting pytorch_lightning==1.3.8\n",
      "  Downloading pytorch_lightning-1.3.8-py3-none-any.whl (813 kB)\n",
      "\u001b[K     |████████████████████████████████| 813 kB 46.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: tensorboard!=2.5.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.3.8->horovod[pytorch]) (2.8.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.3.8->horovod[pytorch]) (21.3)\n",
      "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
      "  Downloading fsspec-2022.2.0-py3-none-any.whl (134 kB)\n",
      "\u001b[K     |████████████████████████████████| 134 kB 52.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.3.8->horovod[pytorch]) (1.21.5)\n",
      "Collecting torchmetrics>=0.2.0\n",
      "  Downloading torchmetrics-0.7.2-py3-none-any.whl (397 kB)\n",
      "\u001b[K     |████████████████████████████████| 397 kB 48.4 MB/s \n",
      "\u001b[?25hCollecting future>=0.17.1\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "\u001b[K     |████████████████████████████████| 829 kB 42.5 MB/s \n",
      "\u001b[?25hCollecting pyyaml\n",
      "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
      "\u001b[K     |████████████████████████████████| 636 kB 42.1 MB/s \n",
      "\u001b[?25hCollecting pyDeprecate==0.3.0\n",
      "  Downloading pyDeprecate-0.3.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.3.8->horovod[pytorch]) (4.63.0)\n",
      "Requirement already satisfied: pillow!=8.3.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.3.8->horovod[pytorch]) (7.1.2)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.4.0->horovod[pytorch]) (2.21)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 40.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.3.8->horovod[pytorch]) (2.23.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch_lightning==1.3.8->horovod[pytorch]) (3.0.7)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.3.8->horovod[pytorch]) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.3.8->horovod[pytorch]) (3.3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.3.8->horovod[pytorch]) (0.6.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.3.8->horovod[pytorch]) (3.17.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.3.8->horovod[pytorch]) (1.8.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.3.8->horovod[pytorch]) (57.4.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.3.8->horovod[pytorch]) (1.35.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.3.8->horovod[pytorch]) (1.0.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.3.8->horovod[pytorch]) (0.37.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.3.8->horovod[pytorch]) (1.44.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.3.8->horovod[pytorch]) (0.4.6)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.3.8->horovod[pytorch]) (1.15.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.3.8->horovod[pytorch]) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.3.8->horovod[pytorch]) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.3.8->horovod[pytorch]) (4.2.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.3.8->horovod[pytorch]) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.3.8->horovod[pytorch]) (4.11.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.3.8->horovod[pytorch]) (3.10.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.3.8->horovod[pytorch]) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.3.8->horovod[pytorch]) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.3.8->horovod[pytorch]) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.3.8->horovod[pytorch]) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.3.8->horovod[pytorch]) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.3.8->horovod[pytorch]) (3.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.3.8->horovod[pytorch]) (3.2.0)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
      "\u001b[K     |████████████████████████████████| 271 kB 47.9 MB/s \n",
      "\u001b[?25hCollecting asynctest==0.13.0\n",
      "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.3.8->horovod[pytorch]) (21.4.0)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
      "\u001b[K     |████████████████████████████████| 144 kB 51.2 MB/s \n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
      "\u001b[K     |████████████████████████████████| 94 kB 3.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.3.8->horovod[pytorch]) (2.0.12)\n",
      "Building wheels for collected packages: horovod, future\n",
      "  Building wheel for horovod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for horovod: filename=horovod-0.24.2-cp37-cp37m-linux_x86_64.whl size=26893748 sha256=d38bb7df1ffe169b6c9ecd9ba255a69277c1126d7428e2f77bb60d3fcb5d3c37\n",
      "  Stored in directory: /root/.cache/pip/wheels/81/fe/2e/4443f20cae201f993f9ab9f3ecc34715928029a0ade57563bc\n",
      "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=dc835486b05f6e9ee7850115c84df4f4771ff756a6f952c3140aa5bb9c1270f4\n",
      "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
      "Successfully built horovod future\n",
      "Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, pyDeprecate, fsspec, aiohttp, torchmetrics, pyyaml, future, pytorch-lightning, horovod\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "  Attempting uninstall: future\n",
      "    Found existing installation: future 0.16.0\n",
      "    Uninstalling future-0.16.0:\n",
      "      Successfully uninstalled future-0.16.0\n",
      "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 frozenlist-1.3.0 fsspec-2022.2.0 future-0.18.2 horovod-0.24.2 multidict-6.0.2 pyDeprecate-0.3.0 pytorch-lightning-1.3.8 pyyaml-5.4.1 torchmetrics-0.7.2 yarl-1.7.2\n"
     ]
    }
   ],
   "source": [
    "! pip install horovod[pytorch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PdyzttzL88Kv",
    "outputId": "85721184-59b2-4c31-a1b3-692fe06a1768"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting learn_hvd.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile learn_hvd.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "# Параметры обучения\n",
    "batch_size = 100\n",
    "num_epochs = 5\n",
    "momentum = 0.5\n",
    "log_interval = 100\n",
    "\n",
    "def train_one_epoch(model, device, data_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(data_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(data_loader) * len(data),\n",
    "                100. * batch_idx / len(data_loader), loss.item()))\n",
    "            \n",
    "from time import time\n",
    "import os\n",
    "\n",
    "LOG_DIR = os.path.join('./logs/', str(time()), 'MNISTDemo')\n",
    "os.makedirs(LOG_DIR)\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch):\n",
    "  filepath = LOG_DIR + '/checkpoint-{epoch}.pth.tar'.format(epoch=epoch)\n",
    "  state = {\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "  }\n",
    "  torch.save(state, filepath)\n",
    "\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def train(learning_rate):\n",
    "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "  train_dataset = datasets.MNIST(\n",
    "    'data', \n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "  data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "  model = Net().to(device)\n",
    "\n",
    "  optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "  for epoch in range(1, num_epochs + 1):\n",
    "    train_one_epoch(model, device, data_loader, optimizer, epoch)\n",
    "    save_checkpoint(model, optimizer, epoch)\n",
    "\n",
    "import horovod.torch as hvd\n",
    "\n",
    "def train_hvd(learning_rate):\n",
    "  hvd.init()  # Иницииализация\n",
    "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "  \n",
    "  if device.type == 'cuda':\n",
    "    torch.cuda.set_device(0)\n",
    "\n",
    "  train_dataset = datasets.MNIST(\n",
    "    root='data-%d'% hvd.rank(),  # каждый обработчики в своей папке\n",
    "    train=True, \n",
    "    download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "  )\n",
    "\n",
    "  from torch.utils.data.distributed import DistributedSampler\n",
    "  \n",
    "  train_sampler = DistributedSampler(train_dataset, num_replicas=hvd.size(), rank=hvd.rank())\n",
    "  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "\n",
    "  model = Net().to(device)\n",
    "  \n",
    "  optimizer = optim.SGD(model.parameters(), lr=learning_rate * hvd.size(), momentum=momentum)\n",
    "\n",
    "  # оборачиваем оптимизатор в Horovod DistributedOptimizer\n",
    "  optimizer = hvd.DistributedOptimizer(optimizer, named_parameters=model.named_parameters())\n",
    "  \n",
    "  # Ставим для всех моделей начальные параметры одинаковыми\n",
    "  hvd.broadcast_parameters(model.state_dict(), root_rank=0)\n",
    "\n",
    "  for epoch in range(1, num_epochs + 1):\n",
    "    train_one_epoch(model, device, train_loader, optimizer, epoch)\n",
    "    # Сохраняем только в одном обработчике\n",
    "    if hvd.rank() == 0:\n",
    "      save_checkpoint(model, optimizer, epoch)\n",
    "  \n",
    "\n",
    "if __name__ == '__main__':\n",
    "  train_hvd(0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "srAHAFohPwA1"
   },
   "outputs": [],
   "source": [
    "from learn_hvd import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxaKBq5v88Kw"
   },
   "source": [
    "### Обучение MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "YM7O3vaz88Kw",
    "outputId": "e4bcdb41-471c-4f61-f7b2-8490b6620cd9"
   },
   "outputs": [],
   "source": [
    "train(learning_rate = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PP5hSCOE88Kw"
   },
   "source": [
    "### Horovod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7NdO6SoaSOw6",
    "outputId": "f9c8d1b8-d5c7-4d35-9d67-3fad9d34e0af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: horovodrun [-h] [-v] -np NP [-cb] [--disable-cache]\n",
      "                  [--start-timeout START_TIMEOUT] [--network-interface NICS]\n",
      "                  [--output-filename OUTPUT_FILENAME] [--verbose]\n",
      "                  [--config-file CONFIG_FILE] [-p SSH_PORT]\n",
      "                  [-i SSH_IDENTITY_FILE]\n",
      "                  [--fusion-threshold-mb FUSION_THRESHOLD_MB]\n",
      "                  [--cycle-time-ms CYCLE_TIME_MS]\n",
      "                  [--cache-capacity CACHE_CAPACITY]\n",
      "                  [--hierarchical-allreduce | --no-hierarchical-allreduce]\n",
      "                  [--hierarchical-allgather | --no-hierarchical-allgather]\n",
      "                  [--autotune] [--autotune-log-file AUTOTUNE_LOG_FILE]\n",
      "                  [--autotune-warmup-samples AUTOTUNE_WARMUP_SAMPLES]\n",
      "                  [--autotune-steps-per-sample AUTOTUNE_STEPS_PER_SAMPLE]\n",
      "                  [--autotune-bayes-opt-max-samples AUTOTUNE_BAYES_OPT_MAX_SAMPLES]\n",
      "                  [--autotune-gaussian-process-noise AUTOTUNE_GAUSSIAN_PROCESS_NOISE]\n",
      "                  [--min-np MIN_NP] [--max-np MAX_NP] [--slots-per-host SLOTS]\n",
      "                  [--elastic-timeout ELASTIC_TIMEOUT]\n",
      "                  [--reset-limit RESET_LIMIT]\n",
      "                  [--blacklist-cooldown-range COOLDOWN_RANGE COOLDOWN_RANGE]\n",
      "                  [--timeline-filename TIMELINE_FILENAME]\n",
      "                  [--timeline-mark-cycles] [--no-stall-check]\n",
      "                  [--stall-check-warning-time-seconds STALL_CHECK_WARNING_TIME_SECONDS]\n",
      "                  [--stall-check-shutdown-time-seconds STALL_CHECK_SHUTDOWN_TIME_SECONDS]\n",
      "                  [--mpi-threads-disable] [--mpi-args MPI_ARGS] [--tcp]\n",
      "                  [--binding-args BINDING_ARGS]\n",
      "                  [--num-nccl-streams NUM_NCCL_STREAMS]\n",
      "                  [--thread-affinity THREAD_AFFINITY]\n",
      "                  [--gloo-timeout-seconds GLOO_TIMEOUT_SECONDS]\n",
      "                  [--log-level {TRACE,DEBUG,INFO,WARNING,ERROR,FATAL}]\n",
      "                  [--log-without-timestamp | -prefix-timestamp]\n",
      "                  [-H HOSTS | -hostfile HOSTFILE | --host-discovery-script HOST_DISCOVERY_SCRIPT]\n",
      "                  [--gloo | --mpi | --jsrun]\n",
      "                  ...\n",
      "horovodrun: error: the following arguments are required: -np/--num-proc\n"
     ]
    }
   ],
   "source": [
    "! horovodrun "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8H8zz3VXTBox",
    "outputId": "4150667c-75ca-4888-df48-9bb964631f48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,1]<stdout>:Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "[1,1]<stdout>:Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data-1/MNIST/raw/train-images-idx3-ubyte.gz\n",
      "9913344it [00:00, 34813387.63it/s]                             [1,1]<stdout>:Extracting data-1/MNIST/raw/train-images-idx3-ubyte.gz to data-1/MNIST/raw\n",
      "[1,1]<stdout>:\n",
      "[1,1]<stdout>:Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "[1,1]<stdout>:Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data-1/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "[1,1]<stderr>:\n",
      "29696it [00:00, 49230850.43it/s]         [1,1]<stdout>:Extracting data-1/MNIST/raw/train-labels-idx1-ubyte.gz to data-1/MNIST/raw\n",
      "[1,1]<stdout>:\n",
      "[1,1]<stdout>:Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "[1,1]<stdout>:Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data-1/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "[1,1]<stderr>:\n",
      "1649664it [00:00, 16115392.96it/s]                             [1,1]<stdout>:Extracting data-1/MNIST/raw/t10k-images-idx3-ubyte.gz to data-1/MNIST/raw\n",
      "[1,1]<stdout>:\n",
      "[1,1]<stdout>:Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "[1,1]<stdout>:Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data-1/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "[1,1]<stderr>:\n",
      "5120it [00:00, 22486739.77it/s]         [1,1]<stdout>:Extracting data-1/MNIST/raw/t10k-labels-idx1-ubyte.gz to data-1/MNIST/raw\n",
      "[1,1]<stdout>:\n",
      "[1,1]<stdout>:Train Epoch: 1 [0/30000 (0%)]\tLoss: 2.357181\n",
      "[1,0]<stdout>:Train Epoch: 1 [0/30000 (0%)]\tLoss: 2.328243\n",
      "[1,1]<stdout>:Train Epoch: 1 [10000/30000 (33%)]\tLoss: 2.272687\n",
      "[1,0]<stdout>:Train Epoch: 1 [10000/30000 (33%)]\tLoss: 2.264185\n",
      "[1,1]<stdout>:Train Epoch: 1 [20000/30000 (67%)]\tLoss: 2.254785\n",
      "[1,0]<stdout>:Train Epoch: 1 [20000/30000 (67%)]\tLoss: 2.251860\n",
      "[1,0]<stdout>:Train Epoch: 2 [0/30000 (0%)]\tLoss: 2.147922\n",
      "[1,1]<stdout>:Train Epoch: 2 [0/30000 (0%)]\tLoss: 2.183982\n",
      "[1,1]<stdout>:Train Epoch: 2 [10000/30000 (33%)]\tLoss: 2.045335\n",
      "[1,0]<stdout>:Train Epoch: 2 [10000/30000 (33%)]\tLoss: 1.986447\n",
      "[1,1]<stdout>:Train Epoch: 2 [20000/30000 (67%)]\tLoss: 1.817574\n",
      "[1,0]<stdout>:Train Epoch: 2 [20000/30000 (67%)]\tLoss: 1.905094\n",
      "[1,1]<stdout>:Train Epoch: 3 [0/30000 (0%)]\tLoss: 1.580329\n",
      "[1,0]<stdout>:Train Epoch: 3 [0/30000 (0%)]\tLoss: 1.555757\n",
      "[1,1]<stdout>:Train Epoch: 3 [10000/30000 (33%)]\tLoss: 1.413323\n",
      "[1,0]<stdout>:Train Epoch: 3 [10000/30000 (33%)]\tLoss: 1.217132\n",
      "[1,0]<stdout>:Train Epoch: 3 [20000/30000 (67%)]\tLoss: 1.406823\n",
      "[1,1]<stdout>:Train Epoch: 3 [20000/30000 (67%)]\tLoss: 1.250931\n",
      "[1,0]<stdout>:Train Epoch: 4 [0/30000 (0%)]\tLoss: 1.079041\n",
      "[1,1]<stdout>:Train Epoch: 4 [0/30000 (0%)]\tLoss: 1.377007\n",
      "[1,0]<stdout>:Train Epoch: 4 [10000/30000 (33%)]\tLoss: 1.086087\n",
      "[1,1]<stdout>:Train Epoch: 4 [10000/30000 (33%)]\tLoss: 0.977251\n",
      "[1,0]<stdout>:Train Epoch: 4 [20000/30000 (67%)]\tLoss: 0.934520\n",
      "[1,1]<stdout>:Train Epoch: 4 [20000/30000 (67%)]\tLoss: 1.011824\n",
      "[1,1]<stdout>:Train Epoch: 5 [0/30000 (0%)]\tLoss: 0.938788\n",
      "[1,0]<stdout>:Train Epoch: 5 [0/30000 (0%)]\tLoss: 0.761381\n",
      "[1,0]<stdout>:Train Epoch: 5 [10000/30000 (33%)]\tLoss: 0.704237\n",
      "[1,1]<stdout>:Train Epoch: 5 [10000/30000 (33%)]\tLoss: 0.844767\n",
      "[1,0]<stdout>:Train Epoch: 5 [20000/30000 (67%)]\tLoss: 0.921442\n",
      "[1,1]<stdout>:Train Epoch: 5 [20000/30000 (67%)]\tLoss: 0.873927\n",
      "[1,1]<stderr>:\n",
      "[1,1]<stderr>:learn_hvd.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "[1,1]<stderr>:  return F.log_softmax(x)\n",
      "[1,0]<stderr>:learn_hvd.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "[1,0]<stderr>:  return F.log_softmax(x)\n"
     ]
    }
   ],
   "source": [
    "! horovodrun -np 2 -H localhost:2 python3 learn_hvd.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lLG8A8QBTruv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "final09.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
