{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Оптимизация сетей\n",
    "\n",
    "Нейронные сети - это модели, которые могут быть потенциально произвольно сложными. Помимо того, что теперь для обучения таких моделей нам требуются гигантские датасеты, сами модели стали слишком большими. \n",
    "\n",
    "Это означает, что они занимают очень много места и например могут физически не помещаться на устройство пользователя. А также они очень медленно делают предсказания.\n",
    "\n",
    "В этот раз поговорим про то, какие приемы могут помочь нам уменьшить размер модели и ускорить ее выполнение, стараясь при этом сохранить качество."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-03-13 21:49:41--  https://raw.githubusercontent.com/ADKosm/lsml-2021-public/main/data/mnist/mnist_784.csv.gz\n",
      "Распознаётся raw.githubusercontent.com (raw.githubusercontent.com)… 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Подключение к raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
      "Длина: 14824659 (14M) [application/octet-stream]\n",
      "Сохранение в: «STDOUT»\n",
      "\n",
      "-                   100%[===================>]  14,14M   704KB/s    за 34s     \n",
      "\n",
      "/2022-03-13 21:50:16 (432 KB/s) - записан в stdout [14824659/14824659]\n",
      "\n",
      "--2022-03-13 21:50:17--  https://raw.githubusercontent.com/ADKosm/lsml-2021-public/main/data/mnist/fashion-mnist_train.csv.gz\n",
      "Распознаётся raw.githubusercontent.com (raw.githubusercontent.com)… 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Подключение к raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
      "Длина: 33060030 (32M) [application/octet-stream]\n",
      "Сохранение в: «STDOUT»\n",
      "\n",
      "-                   100%[===================>]  31,53M   463KB/s    за 46s     \n",
      "\n",
      "/2022-03-13 21:51:04 (706 KB/s) - записан в stdout [33060030/33060030]\n",
      "\n",
      "--2022-03-13 21:51:04--  https://raw.githubusercontent.com/ADKosm/lsml-2021-public/main/data/mnist/fashion-mnist_test.csv.gz\n",
      "Распознаётся raw.githubusercontent.com (raw.githubusercontent.com)… 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Подключение к raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
      "Длина: 5503342 (5,2M) [application/octet-stream]\n",
      "Сохранение в: «STDOUT»\n",
      "\n",
      "-                   100%[===================>]   5,25M   796KB/s    за 6,8s    \n",
      "\n",
      "/2022-03-13 21:51:12 (790 KB/s) - записан в stdout [5503342/5503342]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget -O- https://raw.githubusercontent.com/ADKosm/lsml-2021-public/main/data/mnist/mnist_784.csv.gz | gunzip > mnist_784.csv\n",
    "! wget -O- https://raw.githubusercontent.com/ADKosm/lsml-2021-public/main/data/mnist/fashion-mnist_train.csv.gz | gunzip > fashion-mnist_train.csv\n",
    "! wget -O- https://raw.githubusercontent.com/ADKosm/lsml-2021-public/main/data/mnist/fashion-mnist_test.csv.gz | gunzip > fashion-mnist_test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Прореживание нейронных сетей\n",
    "\n",
    "Попробуем уменьшить размер нейронной сети за счет удаления из нее части весов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==1.7.1\n",
      "  Downloading torch-1.7.1-cp39-none-macosx_10_9_x86_64.whl (110.0 MB)\n",
      "     |████████████████████████████████| 110.0 MB 144 kB/s            \n",
      "\u001b[?25hCollecting typing-extensions\n",
      "  Using cached typing_extensions-4.1.1-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: numpy in /Users/AlexHome/vms/lsml-internal/venv/lib/python3.9/site-packages (from torch==1.7.1) (1.22.1)\n",
      "Installing collected packages: typing-extensions, torch\n",
      "Successfully installed torch-1.7.1 typing-extensions-4.1.1\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/Users/AlexHome/vms/lsml-internal/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install torch==1.7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision==0.8.2\n",
      "  Downloading torchvision-0.8.2-cp39-cp39-macosx_10_9_x86_64.whl (1.0 MB)\n",
      "     |████████████████████████████████| 1.0 MB 665 kB/s            \n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/AlexHome/vms/lsml-internal/venv/lib/python3.9/site-packages (from torchvision==0.8.2) (1.22.1)\n",
      "Requirement already satisfied: torch==1.7.1 in /Users/AlexHome/vms/lsml-internal/venv/lib/python3.9/site-packages (from torchvision==0.8.2) (1.7.1)\n",
      "Collecting pillow>=4.1.1\n",
      "  Using cached Pillow-9.0.1-cp39-cp39-macosx_10_10_x86_64.whl (3.0 MB)\n",
      "Requirement already satisfied: typing-extensions in /Users/AlexHome/vms/lsml-internal/venv/lib/python3.9/site-packages (from torch==1.7.1->torchvision==0.8.2) (4.1.1)\n",
      "Installing collected packages: pillow, torchvision\n",
      "Successfully installed pillow-9.0.1 torchvision-0.8.2\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/Users/AlexHome/vms/lsml-internal/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install torchvision==0.8.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.5.1-cp39-cp39-macosx_10_9_x86_64.whl (7.3 MB)\n",
      "     |████████████████████████████████| 7.3 MB 608 kB/s            \n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
      "  Using cached kiwisolver-1.3.2-cp39-cp39-macosx_10_9_x86_64.whl (61 kB)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/AlexHome/vms/lsml-internal/venv/lib/python3.9/site-packages (from matplotlib) (3.0.6)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.30.0-py3-none-any.whl (898 kB)\n",
      "     |████████████████████████████████| 898 kB 164 kB/s            \n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/AlexHome/vms/lsml-internal/venv/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/AlexHome/vms/lsml-internal/venv/lib/python3.9/site-packages (from matplotlib) (1.22.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/AlexHome/vms/lsml-internal/venv/lib/python3.9/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/AlexHome/vms/lsml-internal/venv/lib/python3.9/site-packages (from matplotlib) (9.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/AlexHome/vms/lsml-internal/venv/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Installing collected packages: kiwisolver, fonttools, cycler, matplotlib\n",
      "Successfully installed cycler-0.11.0 fonttools-4.30.0 kiwisolver-1.3.2 matplotlib-3.5.1\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/Users/AlexHome/vms/lsml-internal/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0-py2.py3-none-any.whl\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.0.2-cp39-cp39-macosx_10_13_x86_64.whl (8.0 MB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /Users/AlexHome/vms/lsml-internal/venv/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.22.1)\n",
      "Collecting joblib>=0.11\n",
      "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "Collecting scipy>=1.1.0\n",
      "  Downloading scipy-1.8.0-cp39-cp39-macosx_12_0_universal2.macosx_10_9_x86_64.whl (55.6 MB)\n",
      "     |████████████████████████████████| 55.6 MB 760 kB/s            \n",
      "\u001b[?25hInstalling collected packages: threadpoolctl, scipy, joblib, scikit-learn, sklearn\n",
      "Successfully installed joblib-1.1.0 scikit-learn-1.0.2 scipy-1.8.0 sklearn-0.0 threadpoolctl-3.1.0\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/Users/AlexHome/vms/lsml-internal/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x133dbccd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED=9876\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве данных будем использовать стандартный mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>pixel10</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  \\\n",
       "0       0       0       0       0       0       0       0       0       0   \n",
       "1       0       0       0       0       0       0       0       0       0   \n",
       "2       0       0       0       0       0       0       0       0       0   \n",
       "3       0       0       0       0       0       0       0       0       0   \n",
       "4       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel10  ...  pixel776  pixel777  pixel778  pixel779  pixel780  pixel781  \\\n",
       "0        0  ...         0         0         0         0         0         0   \n",
       "1        0  ...         0         0         0         0         0         0   \n",
       "2        0  ...         0         0         0         0         0         0   \n",
       "3        0  ...         0         0         0         0         0         0   \n",
       "4        0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel782  pixel783  pixel784  class  \n",
       "0         0         0         0      5  \n",
       "1         0         0         0      0  \n",
       "2         0         0         0      4  \n",
       "3         0         0         0      1  \n",
       "4         0         0         0      9  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('mnist_784.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y = df['class'].values\n",
    "X = df.drop(['class'],axis=1).values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14a584d60>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0].reshape(28, 28))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первое, что мы попробуем сделать - это собрать какую-то несложную архитектуру нейронной сети и просто обучить ее на данных. \n",
    "\n",
    "После этого мы замерим ее размер, а также качество, которое она выдает. Все дальнейшие полученные модели будем сравнивать с этими результатами, как с базовыми и понимать - получилось лучше или хуже.\n",
    "\n",
    "Вначале просто подготовим данные для обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "torch_X_train = torch.from_numpy(X_train).type(torch.LongTensor)\n",
    "torch_y_train = torch.from_numpy(y_train).type(torch.LongTensor)\n",
    "torch_X_test = torch.from_numpy(X_test).type(torch.LongTensor)\n",
    "torch_y_test = torch.from_numpy(y_test).type(torch.LongTensor)\n",
    "\n",
    "train = torch.utils.data.TensorDataset(torch_X_train,torch_y_train)\n",
    "test = torch.utils.data.TensorDataset(torch_X_test,torch_y_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size = BATCH_SIZE, shuffle = False)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size = BATCH_SIZE, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В реальной жизни для задачи распознавания числа на картинке мы бы скорее всего использовали более продвинутую архитектуру сети, однако для наглядности мы возьмем простую сеть, которая при этом имеет много параметров. В ней будут просто три полносвязных слоя: 784 - 250 - 100 - 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (linear1): Linear(in_features=784, out_features=250, bias=True)\n",
      "  (linear2): Linear(in_features=250, out_features=100, bias=True)\n",
      "  (linear3): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(784,250)\n",
    "        self.linear2 = nn.Linear(250,100)\n",
    "        self.linear3 = nn.Linear(100,10)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        X = F.relu(self.linear1(X))\n",
    "        X = F.relu(self.linear2(X))\n",
    "        X = self.linear3(X)\n",
    "        return F.log_softmax(X, dim=1)\n",
    "\n",
    "mlp = MLP()\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем самым обычным способом, используя кросс-энтропию в качестве меры ошибки и используя 5 эпох"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def fit(model, train_loader, epoch_number=5):\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    error = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    for epoch in range(epoch_number):\n",
    "        correct = 0\n",
    "        for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "            var_X_batch = Variable(X_batch).float()\n",
    "            var_y_batch = Variable(y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(var_X_batch)\n",
    "            loss = error(output, var_y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            predicted = torch.max(output.data, 1)[1] \n",
    "            correct += (predicted == var_y_batch).sum()\n",
    "            if batch_idx % 50 == 0:\n",
    "                print('Epoch : {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Accuracy:{:.3f}%'.format(\n",
    "                    epoch, batch_idx*len(X_batch), len(train_loader.dataset), 100.*batch_idx / len(train_loader), loss.data, float(correct*100) / float(BATCH_SIZE*(batch_idx+1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 [0/49000 (0%)]\tLoss: 8.752557\t Accuracy:9.375%\n",
      "Epoch : 0 [1600/49000 (3%)]\tLoss: 0.328137\t Accuracy:69.179%\n",
      "Epoch : 0 [3200/49000 (7%)]\tLoss: 0.348055\t Accuracy:76.825%\n",
      "Epoch : 0 [4800/49000 (10%)]\tLoss: 0.260353\t Accuracy:79.988%\n",
      "Epoch : 0 [6400/49000 (13%)]\tLoss: 0.274885\t Accuracy:82.478%\n",
      "Epoch : 0 [8000/49000 (16%)]\tLoss: 0.284318\t Accuracy:83.877%\n",
      "Epoch : 0 [9600/49000 (20%)]\tLoss: 0.142767\t Accuracy:84.759%\n",
      "Epoch : 0 [11200/49000 (23%)]\tLoss: 0.533794\t Accuracy:85.684%\n",
      "Epoch : 0 [12800/49000 (26%)]\tLoss: 0.383269\t Accuracy:86.386%\n",
      "Epoch : 0 [14400/49000 (29%)]\tLoss: 0.217726\t Accuracy:86.973%\n",
      "Epoch : 0 [16000/49000 (33%)]\tLoss: 0.247426\t Accuracy:87.506%\n",
      "Epoch : 0 [17600/49000 (36%)]\tLoss: 0.295171\t Accuracy:87.903%\n",
      "Epoch : 0 [19200/49000 (39%)]\tLoss: 0.768353\t Accuracy:88.228%\n",
      "Epoch : 0 [20800/49000 (42%)]\tLoss: 0.029675\t Accuracy:88.609%\n",
      "Epoch : 0 [22400/49000 (46%)]\tLoss: 0.423048\t Accuracy:88.886%\n",
      "Epoch : 0 [24000/49000 (49%)]\tLoss: 0.439481\t Accuracy:89.160%\n",
      "Epoch : 0 [25600/49000 (52%)]\tLoss: 0.070248\t Accuracy:89.310%\n",
      "Epoch : 0 [27200/49000 (55%)]\tLoss: 0.486616\t Accuracy:89.472%\n",
      "Epoch : 0 [28800/49000 (59%)]\tLoss: 0.065676\t Accuracy:89.720%\n",
      "Epoch : 0 [30400/49000 (62%)]\tLoss: 0.271769\t Accuracy:89.905%\n",
      "Epoch : 0 [32000/49000 (65%)]\tLoss: 0.097908\t Accuracy:90.122%\n",
      "Epoch : 0 [33600/49000 (69%)]\tLoss: 0.354305\t Accuracy:90.316%\n",
      "Epoch : 0 [35200/49000 (72%)]\tLoss: 0.064940\t Accuracy:90.458%\n",
      "Epoch : 0 [36800/49000 (75%)]\tLoss: 0.327679\t Accuracy:90.620%\n",
      "Epoch : 0 [38400/49000 (78%)]\tLoss: 0.144602\t Accuracy:90.758%\n",
      "Epoch : 0 [40000/49000 (82%)]\tLoss: 0.118381\t Accuracy:90.912%\n",
      "Epoch : 0 [41600/49000 (85%)]\tLoss: 0.199216\t Accuracy:91.057%\n",
      "Epoch : 0 [43200/49000 (88%)]\tLoss: 0.189860\t Accuracy:91.157%\n",
      "Epoch : 0 [44800/49000 (91%)]\tLoss: 0.413594\t Accuracy:91.263%\n",
      "Epoch : 0 [46400/49000 (95%)]\tLoss: 0.041479\t Accuracy:91.387%\n",
      "Epoch : 0 [48000/49000 (98%)]\tLoss: 0.092959\t Accuracy:91.470%\n",
      "Epoch : 1 [0/49000 (0%)]\tLoss: 0.266363\t Accuracy:93.750%\n",
      "Epoch : 1 [1600/49000 (3%)]\tLoss: 0.055156\t Accuracy:95.098%\n",
      "Epoch : 1 [3200/49000 (7%)]\tLoss: 0.137875\t Accuracy:94.833%\n",
      "Epoch : 1 [4800/49000 (10%)]\tLoss: 0.111777\t Accuracy:95.137%\n",
      "Epoch : 1 [6400/49000 (13%)]\tLoss: 0.073176\t Accuracy:95.165%\n",
      "Epoch : 1 [8000/49000 (16%)]\tLoss: 0.270996\t Accuracy:95.107%\n",
      "Epoch : 1 [9600/49000 (20%)]\tLoss: 0.057180\t Accuracy:95.017%\n",
      "Epoch : 1 [11200/49000 (23%)]\tLoss: 0.232776\t Accuracy:94.961%\n",
      "Epoch : 1 [12800/49000 (26%)]\tLoss: 0.398035\t Accuracy:95.036%\n",
      "Epoch : 1 [14400/49000 (29%)]\tLoss: 0.192055\t Accuracy:94.956%\n",
      "Epoch : 1 [16000/49000 (33%)]\tLoss: 0.071758\t Accuracy:94.948%\n",
      "Epoch : 1 [17600/49000 (36%)]\tLoss: 0.023633\t Accuracy:94.998%\n",
      "Epoch : 1 [19200/49000 (39%)]\tLoss: 0.323150\t Accuracy:94.998%\n",
      "Epoch : 1 [20800/49000 (42%)]\tLoss: 0.050068\t Accuracy:95.008%\n",
      "Epoch : 1 [22400/49000 (46%)]\tLoss: 0.120434\t Accuracy:94.985%\n",
      "Epoch : 1 [24000/49000 (49%)]\tLoss: 0.077826\t Accuracy:95.002%\n",
      "Epoch : 1 [25600/49000 (52%)]\tLoss: 0.059897\t Accuracy:94.979%\n",
      "Epoch : 1 [27200/49000 (55%)]\tLoss: 0.461971\t Accuracy:94.954%\n",
      "Epoch : 1 [28800/49000 (59%)]\tLoss: 0.231923\t Accuracy:94.981%\n",
      "Epoch : 1 [30400/49000 (62%)]\tLoss: 0.124759\t Accuracy:94.999%\n",
      "Epoch : 1 [32000/49000 (65%)]\tLoss: 0.175783\t Accuracy:95.052%\n",
      "Epoch : 1 [33600/49000 (69%)]\tLoss: 0.133314\t Accuracy:95.091%\n",
      "Epoch : 1 [35200/49000 (72%)]\tLoss: 0.115610\t Accuracy:95.093%\n",
      "Epoch : 1 [36800/49000 (75%)]\tLoss: 0.016148\t Accuracy:95.140%\n",
      "Epoch : 1 [38400/49000 (78%)]\tLoss: 0.090979\t Accuracy:95.163%\n",
      "Epoch : 1 [40000/49000 (82%)]\tLoss: 0.209329\t Accuracy:95.216%\n",
      "Epoch : 1 [41600/49000 (85%)]\tLoss: 0.198735\t Accuracy:95.294%\n",
      "Epoch : 1 [43200/49000 (88%)]\tLoss: 0.124494\t Accuracy:95.323%\n",
      "Epoch : 1 [44800/49000 (91%)]\tLoss: 0.288605\t Accuracy:95.336%\n",
      "Epoch : 1 [46400/49000 (95%)]\tLoss: 0.030877\t Accuracy:95.387%\n",
      "Epoch : 1 [48000/49000 (98%)]\tLoss: 0.017310\t Accuracy:95.397%\n",
      "Epoch : 2 [0/49000 (0%)]\tLoss: 0.137043\t Accuracy:96.875%\n",
      "Epoch : 2 [1600/49000 (3%)]\tLoss: 0.110494\t Accuracy:96.507%\n",
      "Epoch : 2 [3200/49000 (7%)]\tLoss: 0.049812\t Accuracy:96.689%\n",
      "Epoch : 2 [4800/49000 (10%)]\tLoss: 0.209668\t Accuracy:96.233%\n",
      "Epoch : 2 [6400/49000 (13%)]\tLoss: 0.052262\t Accuracy:96.362%\n",
      "Epoch : 2 [8000/49000 (16%)]\tLoss: 0.042578\t Accuracy:96.240%\n",
      "Epoch : 2 [9600/49000 (20%)]\tLoss: 0.035879\t Accuracy:96.179%\n",
      "Epoch : 2 [11200/49000 (23%)]\tLoss: 0.273037\t Accuracy:96.065%\n",
      "Epoch : 2 [12800/49000 (26%)]\tLoss: 0.306209\t Accuracy:96.057%\n",
      "Epoch : 2 [14400/49000 (29%)]\tLoss: 0.176272\t Accuracy:96.113%\n",
      "Epoch : 2 [16000/49000 (33%)]\tLoss: 0.086812\t Accuracy:96.145%\n",
      "Epoch : 2 [17600/49000 (36%)]\tLoss: 0.121697\t Accuracy:96.143%\n",
      "Epoch : 2 [19200/49000 (39%)]\tLoss: 0.439350\t Accuracy:96.111%\n",
      "Epoch : 2 [20800/49000 (42%)]\tLoss: 0.010299\t Accuracy:96.030%\n",
      "Epoch : 2 [22400/49000 (46%)]\tLoss: 0.177998\t Accuracy:96.010%\n",
      "Epoch : 2 [24000/49000 (49%)]\tLoss: 0.094246\t Accuracy:95.947%\n",
      "Epoch : 2 [25600/49000 (52%)]\tLoss: 0.092448\t Accuracy:95.970%\n",
      "Epoch : 2 [27200/49000 (55%)]\tLoss: 0.214489\t Accuracy:95.968%\n",
      "Epoch : 2 [28800/49000 (59%)]\tLoss: 0.104497\t Accuracy:96.011%\n",
      "Epoch : 2 [30400/49000 (62%)]\tLoss: 0.237579\t Accuracy:96.017%\n",
      "Epoch : 2 [32000/49000 (65%)]\tLoss: 0.011109\t Accuracy:96.088%\n",
      "Epoch : 2 [33600/49000 (69%)]\tLoss: 0.127554\t Accuracy:96.111%\n",
      "Epoch : 2 [35200/49000 (72%)]\tLoss: 0.046794\t Accuracy:96.094%\n",
      "Epoch : 2 [36800/49000 (75%)]\tLoss: 0.107856\t Accuracy:96.112%\n",
      "Epoch : 2 [38400/49000 (78%)]\tLoss: 0.158532\t Accuracy:96.123%\n",
      "Epoch : 2 [40000/49000 (82%)]\tLoss: 0.256969\t Accuracy:96.123%\n",
      "Epoch : 2 [41600/49000 (85%)]\tLoss: 0.087037\t Accuracy:96.154%\n",
      "Epoch : 2 [43200/49000 (88%)]\tLoss: 0.098182\t Accuracy:96.179%\n",
      "Epoch : 2 [44800/49000 (91%)]\tLoss: 0.129102\t Accuracy:96.179%\n",
      "Epoch : 2 [46400/49000 (95%)]\tLoss: 0.010749\t Accuracy:96.197%\n",
      "Epoch : 2 [48000/49000 (98%)]\tLoss: 0.081919\t Accuracy:96.190%\n",
      "Epoch : 3 [0/49000 (0%)]\tLoss: 0.065258\t Accuracy:96.875%\n",
      "Epoch : 3 [1600/49000 (3%)]\tLoss: 0.067349\t Accuracy:96.814%\n",
      "Epoch : 3 [3200/49000 (7%)]\tLoss: 0.053883\t Accuracy:96.627%\n",
      "Epoch : 3 [4800/49000 (10%)]\tLoss: 0.069199\t Accuracy:96.523%\n",
      "Epoch : 3 [6400/49000 (13%)]\tLoss: 0.002654\t Accuracy:96.797%\n",
      "Epoch : 3 [8000/49000 (16%)]\tLoss: 0.165492\t Accuracy:96.489%\n",
      "Epoch : 3 [9600/49000 (20%)]\tLoss: 0.053892\t Accuracy:96.470%\n",
      "Epoch : 3 [11200/49000 (23%)]\tLoss: 0.182808\t Accuracy:96.554%\n",
      "Epoch : 3 [12800/49000 (26%)]\tLoss: 0.163896\t Accuracy:96.594%\n",
      "Epoch : 3 [14400/49000 (29%)]\tLoss: 0.066412\t Accuracy:96.653%\n",
      "Epoch : 3 [16000/49000 (33%)]\tLoss: 0.200641\t Accuracy:96.675%\n",
      "Epoch : 3 [17600/49000 (36%)]\tLoss: 0.038016\t Accuracy:96.716%\n",
      "Epoch : 3 [19200/49000 (39%)]\tLoss: 0.226017\t Accuracy:96.703%\n",
      "Epoch : 3 [20800/49000 (42%)]\tLoss: 0.010349\t Accuracy:96.712%\n",
      "Epoch : 3 [22400/49000 (46%)]\tLoss: 0.367582\t Accuracy:96.719%\n",
      "Epoch : 3 [24000/49000 (49%)]\tLoss: 0.111751\t Accuracy:96.684%\n",
      "Epoch : 3 [25600/49000 (52%)]\tLoss: 0.170642\t Accuracy:96.637%\n",
      "Epoch : 3 [27200/49000 (55%)]\tLoss: 1.033486\t Accuracy:96.614%\n",
      "Epoch : 3 [28800/49000 (59%)]\tLoss: 0.161623\t Accuracy:96.639%\n",
      "Epoch : 3 [30400/49000 (62%)]\tLoss: 0.058980\t Accuracy:96.619%\n",
      "Epoch : 3 [32000/49000 (65%)]\tLoss: 0.058076\t Accuracy:96.641%\n",
      "Epoch : 3 [33600/49000 (69%)]\tLoss: 0.204643\t Accuracy:96.670%\n",
      "Epoch : 3 [35200/49000 (72%)]\tLoss: 0.064739\t Accuracy:96.671%\n",
      "Epoch : 3 [36800/49000 (75%)]\tLoss: 0.022647\t Accuracy:96.669%\n",
      "Epoch : 3 [38400/49000 (78%)]\tLoss: 0.071490\t Accuracy:96.675%\n",
      "Epoch : 3 [40000/49000 (82%)]\tLoss: 0.087144\t Accuracy:96.635%\n",
      "Epoch : 3 [41600/49000 (85%)]\tLoss: 0.181756\t Accuracy:96.664%\n",
      "Epoch : 3 [43200/49000 (88%)]\tLoss: 0.005839\t Accuracy:96.706%\n",
      "Epoch : 3 [44800/49000 (91%)]\tLoss: 0.141244\t Accuracy:96.737%\n",
      "Epoch : 3 [46400/49000 (95%)]\tLoss: 0.026308\t Accuracy:96.776%\n",
      "Epoch : 3 [48000/49000 (98%)]\tLoss: 0.012498\t Accuracy:96.777%\n",
      "Epoch : 4 [0/49000 (0%)]\tLoss: 0.053175\t Accuracy:100.000%\n",
      "Epoch : 4 [1600/49000 (3%)]\tLoss: 0.079510\t Accuracy:96.998%\n",
      "Epoch : 4 [3200/49000 (7%)]\tLoss: 0.179215\t Accuracy:96.875%\n",
      "Epoch : 4 [4800/49000 (10%)]\tLoss: 0.160457\t Accuracy:96.978%\n",
      "Epoch : 4 [6400/49000 (13%)]\tLoss: 0.087734\t Accuracy:97.093%\n",
      "Epoch : 4 [8000/49000 (16%)]\tLoss: 0.067092\t Accuracy:97.099%\n",
      "Epoch : 4 [9600/49000 (20%)]\tLoss: 0.011775\t Accuracy:96.989%\n",
      "Epoch : 4 [11200/49000 (23%)]\tLoss: 0.096461\t Accuracy:96.902%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 4 [12800/49000 (26%)]\tLoss: 0.186442\t Accuracy:96.930%\n",
      "Epoch : 4 [14400/49000 (29%)]\tLoss: 0.031756\t Accuracy:96.889%\n",
      "Epoch : 4 [16000/49000 (33%)]\tLoss: 0.049820\t Accuracy:96.956%\n",
      "Epoch : 4 [17600/49000 (36%)]\tLoss: 0.069461\t Accuracy:96.943%\n",
      "Epoch : 4 [19200/49000 (39%)]\tLoss: 0.364155\t Accuracy:96.948%\n",
      "Epoch : 4 [20800/49000 (42%)]\tLoss: 0.004673\t Accuracy:96.918%\n",
      "Epoch : 4 [22400/49000 (46%)]\tLoss: 0.272254\t Accuracy:96.897%\n",
      "Epoch : 4 [24000/49000 (49%)]\tLoss: 0.197898\t Accuracy:96.896%\n",
      "Epoch : 4 [25600/49000 (52%)]\tLoss: 0.007085\t Accuracy:96.871%\n",
      "Epoch : 4 [27200/49000 (55%)]\tLoss: 0.201744\t Accuracy:96.915%\n",
      "Epoch : 4 [28800/49000 (59%)]\tLoss: 0.098582\t Accuracy:96.927%\n",
      "Epoch : 4 [30400/49000 (62%)]\tLoss: 0.009788\t Accuracy:96.918%\n",
      "Epoch : 4 [32000/49000 (65%)]\tLoss: 0.016327\t Accuracy:96.937%\n",
      "Epoch : 4 [33600/49000 (69%)]\tLoss: 0.130120\t Accuracy:96.967%\n",
      "Epoch : 4 [35200/49000 (72%)]\tLoss: 0.122135\t Accuracy:96.969%\n",
      "Epoch : 4 [36800/49000 (75%)]\tLoss: 0.119698\t Accuracy:96.997%\n",
      "Epoch : 4 [38400/49000 (78%)]\tLoss: 0.125789\t Accuracy:97.018%\n",
      "Epoch : 4 [40000/49000 (82%)]\tLoss: 0.087052\t Accuracy:97.027%\n",
      "Epoch : 4 [41600/49000 (85%)]\tLoss: 0.008042\t Accuracy:97.036%\n",
      "Epoch : 4 [43200/49000 (88%)]\tLoss: 0.101792\t Accuracy:97.048%\n",
      "Epoch : 4 [44800/49000 (91%)]\tLoss: 0.027414\t Accuracy:97.051%\n",
      "Epoch : 4 [46400/49000 (95%)]\tLoss: 0.010077\t Accuracy:97.077%\n",
      "Epoch : 4 [48000/49000 (98%)]\tLoss: 0.005471\t Accuracy:97.098%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "fit(mlp, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метрики качества возьмем обычный accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.960% \n"
     ]
    }
   ],
   "source": [
    "def evaluate(model):\n",
    "    correct = 0 \n",
    "    for test_imgs, test_labels in test_loader:\n",
    "        test_imgs = Variable(test_imgs).float()\n",
    "        output = model(test_imgs)\n",
    "        predicted = torch.max(output,1)[1]\n",
    "        correct += (predicted == test_labels).sum()\n",
    "    print(\"Test accuracy:{:.3f}% \".format( float(correct) / (len(test_loader)*BATCH_SIZE)))\n",
    "\n",
    "evaluate(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Весьма неплохое качество, учитывая, что мы почти ничего не придумывали с сетью.\n",
    "\n",
    "Посмотрим, сколько параметров нам потребовалось, чтобы получить это качество."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calc_weights(model):\n",
    "    result = 0\n",
    "    for layer in model.children():\n",
    "        result += len(layer.weight.reshape(-1))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_weights(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что полносвязные слои достаточно тяжелые и всего три слоя дали нам больше чем 200 000 параметров. Попробуем ужать это число, не сильно уменьшим при этом качество."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Удаляем связи внутри сети\n",
    "\n",
    "Для того, чтобы начать оптимизировать размер сети, нам нужен инструментарий для удаления связей внутри нашей модели.\n",
    "\n",
    "Нам потребуется особый полносвязный слой, в котором мы можем отключать конкретные веса. Используя такие слои, соберем такую же архитектруру с тремя полносвязными.\n",
    "\n",
    "Отключать сами веса мы будем исходя из их абсолютного значения - задавая пороговое значение, мы будем занулять только те веса, которые меньше этого значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MaskedLinear(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(MaskedLinear, self).__init__()\n",
    "        # Обычный полносвязный слой\n",
    "        self._linear = nn.Linear(in_size, out_size) \n",
    "        # Маска для слоя. Для связи из оригинального слоя, здесь будут хранится 0 и 1. \n",
    "        # 1 - связь действует, 0 - связь не действует.\n",
    "        self._mask = nn.Linear(in_size, out_size)\n",
    "        # Изначально все числа в маске - 1. То есть изначально мы не выключаем вообще никакие веса\n",
    "        self._mask.weight.data = torch.ones(self._mask.weight.size())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Чтобы применить этот слой, нужно вначале умножить веса на маску. \n",
    "        # Тогда те веса, которые мы выключили, занулятся, что и будет означать, что мы их просто выкинули\n",
    "        self._linear.weight.data = torch.mul(self._linear.weight, self._mask.weight)\n",
    "        return self._linear(x)\n",
    "    \n",
    "    def prune(self, threshold):\n",
    "        # Для того, чтобы выключить часть связей задается threshold\n",
    "        # Если значение веса по модулю в сети меньше, чем threshold, то мы его выключаем, а значит выставляем 0 в маске.\n",
    "        self._mask.weight.data = torch.mul(torch.gt(torch.abs(self._linear.weight), threshold).float(), self._mask.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Составляем точно такую же архитектуру, но используя наши особенные полносвязные слои, в которых мы можем отключать веса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class AutoCompressMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoCompressMLP, self).__init__()\n",
    "        self.linear1 = MaskedLinear(784,250)\n",
    "        self.linear2 = MaskedLinear(250,100)\n",
    "        self.linear3 = MaskedLinear(100,10)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        X = F.relu(self.linear1(X))\n",
    "        X = F.relu(self.linear2(X))\n",
    "        X = self.linear3(X)\n",
    "        return F.log_softmax(X, dim=1)\n",
    "    \n",
    "    def prune(self, threshold):\n",
    "        self.linear1.prune(threshold)\n",
    "        self.linear2.prune(threshold)\n",
    "        self.linear3.prune(threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы удалить какую-то долю связей из сети, необходимо вначале подсчитать необходимое пороговое значение. \n",
    "\n",
    "Так, чтобы удалить N% связей по этой схеме, необходимо найти такое число, чтобы ровно N% связей имело вес меньше этого числа по модулю. Другими словами найти N-перцентиль.\n",
    "\n",
    "Напишем функцию, которая будет искать такое пороговое значение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calc_threshhold(model, rate):\n",
    "    all_weights = torch.Tensor()\n",
    "    for layer in model.children():\n",
    "        all_weights = torch.cat( (layer._linear.weight.view(-1), all_weights.view(-1)) )\n",
    "    abs_weight = torch.abs(all_weights)\n",
    "    \n",
    "    return np.percentile(abs_weight.detach().cpu().numpy(), rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.018787425011396408"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acmlp = AutoCompressMLP()\n",
    "t = calc_threshhold(acmlp, 50.0)\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы следить за тем, сколько параметров осталось внури нашей сети, нам потребуется немного другая функция подсчета активных весов, учитываящая маску."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calc_pruned_weights(model):\n",
    "    result = 0\n",
    "    for layer in model.children():\n",
    "        result += torch.sum(layer._mask.weight.reshape(-1))\n",
    "    return int(result.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acmlp.prune(t)\n",
    "calc_pruned_weights(acmlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Итеративное прореживание\n",
    "\n",
    "Идин из способов сжатия нейронных сетей - итеративное прореживание (Incremental Magnitude Pruning). Он достаточно ресурсоемкий, однако позволяет достаточно несложными методами добиться неплохого результата."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "acmlp = AutoCompressMLP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вначале просто обучим нашу модель, никаким образом ее не модифицируя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 [0/49000 (0%)]\tLoss: 12.988437\t Accuracy:34.375%\n",
      "Epoch : 0 [1600/49000 (3%)]\tLoss: 0.285285\t Accuracy:67.770%\n",
      "Epoch : 0 [3200/49000 (7%)]\tLoss: 0.399245\t Accuracy:75.619%\n",
      "Epoch : 0 [4800/49000 (10%)]\tLoss: 0.295355\t Accuracy:79.863%\n",
      "Epoch : 0 [6400/49000 (13%)]\tLoss: 0.627711\t Accuracy:82.090%\n",
      "Epoch : 0 [8000/49000 (16%)]\tLoss: 0.715258\t Accuracy:83.528%\n",
      "Epoch : 0 [9600/49000 (20%)]\tLoss: 0.129665\t Accuracy:84.437%\n",
      "Epoch : 0 [11200/49000 (23%)]\tLoss: 0.482205\t Accuracy:85.239%\n",
      "Epoch : 0 [12800/49000 (26%)]\tLoss: 0.412768\t Accuracy:85.817%\n",
      "Epoch : 0 [14400/49000 (29%)]\tLoss: 0.358222\t Accuracy:86.481%\n",
      "Epoch : 0 [16000/49000 (33%)]\tLoss: 0.488665\t Accuracy:87.070%\n",
      "Epoch : 0 [17600/49000 (36%)]\tLoss: 0.053358\t Accuracy:87.523%\n",
      "Epoch : 0 [19200/49000 (39%)]\tLoss: 0.866225\t Accuracy:87.895%\n",
      "Epoch : 0 [20800/49000 (42%)]\tLoss: 0.036892\t Accuracy:88.268%\n",
      "Epoch : 0 [22400/49000 (46%)]\tLoss: 0.306077\t Accuracy:88.534%\n",
      "Epoch : 0 [24000/49000 (49%)]\tLoss: 0.166066\t Accuracy:88.777%\n",
      "Epoch : 0 [25600/49000 (52%)]\tLoss: 0.192589\t Accuracy:88.986%\n",
      "Epoch : 0 [27200/49000 (55%)]\tLoss: 0.523932\t Accuracy:89.237%\n",
      "Epoch : 0 [28800/49000 (59%)]\tLoss: 0.199876\t Accuracy:89.494%\n",
      "Epoch : 0 [30400/49000 (62%)]\tLoss: 0.316512\t Accuracy:89.682%\n",
      "Epoch : 0 [32000/49000 (65%)]\tLoss: 0.055630\t Accuracy:89.898%\n",
      "Epoch : 0 [33600/49000 (69%)]\tLoss: 0.147096\t Accuracy:90.108%\n",
      "Epoch : 0 [35200/49000 (72%)]\tLoss: 0.182908\t Accuracy:90.279%\n",
      "Epoch : 0 [36800/49000 (75%)]\tLoss: 0.131295\t Accuracy:90.443%\n",
      "Epoch : 0 [38400/49000 (78%)]\tLoss: 0.150510\t Accuracy:90.604%\n",
      "Epoch : 0 [40000/49000 (82%)]\tLoss: 0.038646\t Accuracy:90.767%\n",
      "Epoch : 0 [41600/49000 (85%)]\tLoss: 0.193516\t Accuracy:90.947%\n",
      "Epoch : 0 [43200/49000 (88%)]\tLoss: 0.224253\t Accuracy:91.083%\n",
      "Epoch : 0 [44800/49000 (91%)]\tLoss: 0.654513\t Accuracy:91.165%\n",
      "Epoch : 0 [46400/49000 (95%)]\tLoss: 0.030504\t Accuracy:91.297%\n",
      "Epoch : 0 [48000/49000 (98%)]\tLoss: 0.194598\t Accuracy:91.410%\n",
      "Epoch : 1 [0/49000 (0%)]\tLoss: 0.557197\t Accuracy:93.750%\n",
      "Epoch : 1 [1600/49000 (3%)]\tLoss: 0.045402\t Accuracy:95.037%\n",
      "Epoch : 1 [3200/49000 (7%)]\tLoss: 0.117303\t Accuracy:95.080%\n",
      "Epoch : 1 [4800/49000 (10%)]\tLoss: 0.132050\t Accuracy:95.199%\n",
      "Epoch : 1 [6400/49000 (13%)]\tLoss: 0.070592\t Accuracy:95.429%\n",
      "Epoch : 1 [8000/49000 (16%)]\tLoss: 0.374703\t Accuracy:95.369%\n",
      "Epoch : 1 [9600/49000 (20%)]\tLoss: 0.077805\t Accuracy:95.255%\n",
      "Epoch : 1 [11200/49000 (23%)]\tLoss: 0.273463\t Accuracy:95.379%\n",
      "Epoch : 1 [12800/49000 (26%)]\tLoss: 0.377881\t Accuracy:95.371%\n",
      "Epoch : 1 [14400/49000 (29%)]\tLoss: 0.158183\t Accuracy:95.316%\n",
      "Epoch : 1 [16000/49000 (33%)]\tLoss: 0.062644\t Accuracy:95.272%\n",
      "Epoch : 1 [17600/49000 (36%)]\tLoss: 0.034685\t Accuracy:95.281%\n",
      "Epoch : 1 [19200/49000 (39%)]\tLoss: 0.460806\t Accuracy:95.268%\n",
      "Epoch : 1 [20800/49000 (42%)]\tLoss: 0.035914\t Accuracy:95.243%\n",
      "Epoch : 1 [22400/49000 (46%)]\tLoss: 0.390362\t Accuracy:95.159%\n",
      "Epoch : 1 [24000/49000 (49%)]\tLoss: 0.109117\t Accuracy:95.131%\n",
      "Epoch : 1 [25600/49000 (52%)]\tLoss: 0.069844\t Accuracy:95.069%\n",
      "Epoch : 1 [27200/49000 (55%)]\tLoss: 0.455523\t Accuracy:95.043%\n",
      "Epoch : 1 [28800/49000 (59%)]\tLoss: 0.101143\t Accuracy:95.058%\n",
      "Epoch : 1 [30400/49000 (62%)]\tLoss: 0.198080\t Accuracy:95.051%\n",
      "Epoch : 1 [32000/49000 (65%)]\tLoss: 0.056897\t Accuracy:95.121%\n",
      "Epoch : 1 [33600/49000 (69%)]\tLoss: 0.050777\t Accuracy:95.145%\n",
      "Epoch : 1 [35200/49000 (72%)]\tLoss: 0.027494\t Accuracy:95.175%\n",
      "Epoch : 1 [36800/49000 (75%)]\tLoss: 0.032311\t Accuracy:95.235%\n",
      "Epoch : 1 [38400/49000 (78%)]\tLoss: 0.107613\t Accuracy:95.251%\n",
      "Epoch : 1 [40000/49000 (82%)]\tLoss: 0.006813\t Accuracy:95.261%\n",
      "Epoch : 1 [41600/49000 (85%)]\tLoss: 0.037754\t Accuracy:95.290%\n",
      "Epoch : 1 [43200/49000 (88%)]\tLoss: 0.049220\t Accuracy:95.316%\n",
      "Epoch : 1 [44800/49000 (91%)]\tLoss: 0.452983\t Accuracy:95.320%\n",
      "Epoch : 1 [46400/49000 (95%)]\tLoss: 0.079722\t Accuracy:95.337%\n",
      "Epoch : 1 [48000/49000 (98%)]\tLoss: 0.021193\t Accuracy:95.361%\n",
      "Epoch : 2 [0/49000 (0%)]\tLoss: 0.108483\t Accuracy:93.750%\n",
      "Epoch : 2 [1600/49000 (3%)]\tLoss: 0.043741\t Accuracy:95.588%\n",
      "Epoch : 2 [3200/49000 (7%)]\tLoss: 0.110185\t Accuracy:95.390%\n",
      "Epoch : 2 [4800/49000 (10%)]\tLoss: 0.090026\t Accuracy:95.385%\n",
      "Epoch : 2 [6400/49000 (13%)]\tLoss: 0.185464\t Accuracy:95.678%\n",
      "Epoch : 2 [8000/49000 (16%)]\tLoss: 0.130435\t Accuracy:95.792%\n",
      "Epoch : 2 [9600/49000 (20%)]\tLoss: 0.131295\t Accuracy:95.608%\n",
      "Epoch : 2 [11200/49000 (23%)]\tLoss: 0.313889\t Accuracy:95.637%\n",
      "Epoch : 2 [12800/49000 (26%)]\tLoss: 0.242080\t Accuracy:95.659%\n",
      "Epoch : 2 [14400/49000 (29%)]\tLoss: 0.117261\t Accuracy:95.759%\n",
      "Epoch : 2 [16000/49000 (33%)]\tLoss: 0.097905\t Accuracy:95.771%\n",
      "Epoch : 2 [17600/49000 (36%)]\tLoss: 0.067355\t Accuracy:95.814%\n",
      "Epoch : 2 [19200/49000 (39%)]\tLoss: 0.599994\t Accuracy:95.851%\n",
      "Epoch : 2 [20800/49000 (42%)]\tLoss: 0.005447\t Accuracy:95.838%\n",
      "Epoch : 2 [22400/49000 (46%)]\tLoss: 0.162546\t Accuracy:95.845%\n",
      "Epoch : 2 [24000/49000 (49%)]\tLoss: 0.051166\t Accuracy:95.880%\n",
      "Epoch : 2 [25600/49000 (52%)]\tLoss: 0.124422\t Accuracy:95.849%\n",
      "Epoch : 2 [27200/49000 (55%)]\tLoss: 0.525652\t Accuracy:95.865%\n",
      "Epoch : 2 [28800/49000 (59%)]\tLoss: 0.091089\t Accuracy:95.914%\n",
      "Epoch : 2 [30400/49000 (62%)]\tLoss: 0.189846\t Accuracy:95.899%\n",
      "Epoch : 2 [32000/49000 (65%)]\tLoss: 0.014852\t Accuracy:95.942%\n",
      "Epoch : 2 [33600/49000 (69%)]\tLoss: 0.152161\t Accuracy:95.953%\n",
      "Epoch : 2 [35200/49000 (72%)]\tLoss: 0.039732\t Accuracy:95.938%\n",
      "Epoch : 2 [36800/49000 (75%)]\tLoss: 0.060842\t Accuracy:95.974%\n",
      "Epoch : 2 [38400/49000 (78%)]\tLoss: 0.069851\t Accuracy:95.990%\n",
      "Epoch : 2 [40000/49000 (82%)]\tLoss: 0.041200\t Accuracy:96.036%\n",
      "Epoch : 2 [41600/49000 (85%)]\tLoss: 0.022217\t Accuracy:96.061%\n",
      "Epoch : 2 [43200/49000 (88%)]\tLoss: 0.219435\t Accuracy:96.070%\n",
      "Epoch : 2 [44800/49000 (91%)]\tLoss: 0.340969\t Accuracy:96.052%\n",
      "Epoch : 2 [46400/49000 (95%)]\tLoss: 0.094547\t Accuracy:96.070%\n",
      "Epoch : 2 [48000/49000 (98%)]\tLoss: 0.051869\t Accuracy:96.073%\n",
      "Epoch : 3 [0/49000 (0%)]\tLoss: 0.049121\t Accuracy:96.875%\n",
      "Epoch : 3 [1600/49000 (3%)]\tLoss: 0.036536\t Accuracy:96.814%\n",
      "Epoch : 3 [3200/49000 (7%)]\tLoss: 0.016172\t Accuracy:96.751%\n",
      "Epoch : 3 [4800/49000 (10%)]\tLoss: 0.262573\t Accuracy:96.502%\n",
      "Epoch : 3 [6400/49000 (13%)]\tLoss: 0.020144\t Accuracy:96.859%\n",
      "Epoch : 3 [8000/49000 (16%)]\tLoss: 0.140031\t Accuracy:96.788%\n",
      "Epoch : 3 [9600/49000 (20%)]\tLoss: 0.033247\t Accuracy:96.657%\n",
      "Epoch : 3 [11200/49000 (23%)]\tLoss: 0.088068\t Accuracy:96.572%\n",
      "Epoch : 3 [12800/49000 (26%)]\tLoss: 0.127622\t Accuracy:96.610%\n",
      "Epoch : 3 [14400/49000 (29%)]\tLoss: 0.066233\t Accuracy:96.549%\n",
      "Epoch : 3 [16000/49000 (33%)]\tLoss: 0.015820\t Accuracy:96.482%\n",
      "Epoch : 3 [17600/49000 (36%)]\tLoss: 0.021444\t Accuracy:96.438%\n",
      "Epoch : 3 [19200/49000 (39%)]\tLoss: 0.352029\t Accuracy:96.360%\n",
      "Epoch : 3 [20800/49000 (42%)]\tLoss: 0.001287\t Accuracy:96.342%\n",
      "Epoch : 3 [22400/49000 (46%)]\tLoss: 0.235799\t Accuracy:96.295%\n",
      "Epoch : 3 [24000/49000 (49%)]\tLoss: 0.027446\t Accuracy:96.301%\n",
      "Epoch : 3 [25600/49000 (52%)]\tLoss: 0.156459\t Accuracy:96.278%\n",
      "Epoch : 3 [27200/49000 (55%)]\tLoss: 0.413482\t Accuracy:96.291%\n",
      "Epoch : 3 [28800/49000 (59%)]\tLoss: 0.221374\t Accuracy:96.320%\n",
      "Epoch : 3 [30400/49000 (62%)]\tLoss: 0.289342\t Accuracy:96.274%\n",
      "Epoch : 3 [32000/49000 (65%)]\tLoss: 0.007334\t Accuracy:96.310%\n",
      "Epoch : 3 [33600/49000 (69%)]\tLoss: 0.130137\t Accuracy:96.358%\n",
      "Epoch : 3 [35200/49000 (72%)]\tLoss: 0.080442\t Accuracy:96.361%\n",
      "Epoch : 3 [36800/49000 (75%)]\tLoss: 0.008631\t Accuracy:96.392%\n",
      "Epoch : 3 [38400/49000 (78%)]\tLoss: 0.089992\t Accuracy:96.414%\n",
      "Epoch : 3 [40000/49000 (82%)]\tLoss: 0.031601\t Accuracy:96.423%\n",
      "Epoch : 3 [41600/49000 (85%)]\tLoss: 0.215992\t Accuracy:96.474%\n",
      "Epoch : 3 [43200/49000 (88%)]\tLoss: 0.176845\t Accuracy:96.507%\n",
      "Epoch : 3 [44800/49000 (91%)]\tLoss: 0.194238\t Accuracy:96.511%\n",
      "Epoch : 3 [46400/49000 (95%)]\tLoss: 0.267442\t Accuracy:96.541%\n",
      "Epoch : 3 [48000/49000 (98%)]\tLoss: 0.119281\t Accuracy:96.561%\n",
      "Epoch : 4 [0/49000 (0%)]\tLoss: 0.215721\t Accuracy:93.750%\n",
      "Epoch : 4 [1600/49000 (3%)]\tLoss: 0.166211\t Accuracy:96.385%\n",
      "Epoch : 4 [3200/49000 (7%)]\tLoss: 0.237053\t Accuracy:96.844%\n",
      "Epoch : 4 [4800/49000 (10%)]\tLoss: 0.161379\t Accuracy:96.647%\n",
      "Epoch : 4 [6400/49000 (13%)]\tLoss: 0.012915\t Accuracy:96.673%\n",
      "Epoch : 4 [8000/49000 (16%)]\tLoss: 0.026139\t Accuracy:96.701%\n",
      "Epoch : 4 [9600/49000 (20%)]\tLoss: 0.083053\t Accuracy:96.512%\n",
      "Epoch : 4 [11200/49000 (23%)]\tLoss: 0.101762\t Accuracy:96.581%\n",
      "Epoch : 4 [12800/49000 (26%)]\tLoss: 0.191374\t Accuracy:96.618%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 4 [14400/49000 (29%)]\tLoss: 0.133909\t Accuracy:96.570%\n",
      "Epoch : 4 [16000/49000 (33%)]\tLoss: 0.195847\t Accuracy:96.632%\n",
      "Epoch : 4 [17600/49000 (36%)]\tLoss: 0.082457\t Accuracy:96.659%\n",
      "Epoch : 4 [19200/49000 (39%)]\tLoss: 0.340325\t Accuracy:96.641%\n",
      "Epoch : 4 [20800/49000 (42%)]\tLoss: 0.009543\t Accuracy:96.659%\n",
      "Epoch : 4 [22400/49000 (46%)]\tLoss: 0.129090\t Accuracy:96.608%\n",
      "Epoch : 4 [24000/49000 (49%)]\tLoss: 0.112892\t Accuracy:96.563%\n",
      "Epoch : 4 [25600/49000 (52%)]\tLoss: 0.026490\t Accuracy:96.578%\n",
      "Epoch : 4 [27200/49000 (55%)]\tLoss: 0.279369\t Accuracy:96.600%\n",
      "Epoch : 4 [28800/49000 (59%)]\tLoss: 0.059310\t Accuracy:96.622%\n",
      "Epoch : 4 [30400/49000 (62%)]\tLoss: 0.095430\t Accuracy:96.586%\n",
      "Epoch : 4 [32000/49000 (65%)]\tLoss: 0.007253\t Accuracy:96.631%\n",
      "Epoch : 4 [33600/49000 (69%)]\tLoss: 0.044925\t Accuracy:96.667%\n",
      "Epoch : 4 [35200/49000 (72%)]\tLoss: 0.052482\t Accuracy:96.651%\n",
      "Epoch : 4 [36800/49000 (75%)]\tLoss: 0.008377\t Accuracy:96.690%\n",
      "Epoch : 4 [38400/49000 (78%)]\tLoss: 0.057706\t Accuracy:96.711%\n",
      "Epoch : 4 [40000/49000 (82%)]\tLoss: 0.013375\t Accuracy:96.735%\n",
      "Epoch : 4 [41600/49000 (85%)]\tLoss: 0.005897\t Accuracy:96.757%\n",
      "Epoch : 4 [43200/49000 (88%)]\tLoss: 0.017433\t Accuracy:96.750%\n",
      "Epoch : 4 [44800/49000 (91%)]\tLoss: 0.076696\t Accuracy:96.761%\n",
      "Epoch : 4 [46400/49000 (95%)]\tLoss: 0.073244\t Accuracy:96.800%\n",
      "Epoch : 4 [48000/49000 (98%)]\tLoss: 0.007467\t Accuracy:96.813%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "fit(acmlp, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.955% \n"
     ]
    }
   ],
   "source": [
    "evaluate(acmlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично, получили примерно такую же модель, как и в самом начале. \n",
    "\n",
    "Сейчас модель уже имеет хорошие веса для предсказаний. Теперь попробуем убрать из нее 50% связей и посмотрим, насколько ей удастся сохранить качество.\n",
    "\n",
    "Как уже отмечалось, отключим 50% наиболее слабых связей в сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "acmlp_test1 = copy.deepcopy(acmlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "t_50 = calc_threshhold(acmlp_test1, 50.0)\n",
    "acmlp_test1.prune(t_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.943% \n"
     ]
    }
   ],
   "source": [
    "evaluate(acmlp_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно заметить, что таким образом выкинутые веса почти не повлияли на качество сети. При этом мы выкинули половину всех коэффициентов! Весьма неплохой результат.\n",
    "\n",
    "Давайте посмотрим, можем ли мы с таким же успехом выкинуть 90% сети?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "acmlp_test2 = copy.deepcopy(acmlp)\n",
    "\n",
    "t_90 = calc_threshhold(acmlp_test2, 90.0)\n",
    "acmlp_test2.prune(t_90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.296% \n"
     ]
    }
   ],
   "source": [
    "evaluate(acmlp_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Увы, так просто выкинуть 90% и оставить качество не получается. Будем использовать более хитрый подход.\n",
    "\n",
    "Будет идти с шагом в 10%. Каждый раз будет отключать внутри сети 10% связей. После отключения, оставшиеся веса дообучим на всех данных используя всего одну эпоху. Ожидается, что так как мы выкинули за один раз не очень много, то оставшиеся связи \"перехватят\" ответственность тех слабых, которые мы только что отключили.\n",
    "\n",
    "Таким образом за P таких итераций мы выкинем 10P% всей сети и не должны при этом потерять сильно в качестве."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def smart_prune(model, train_loader, compress_rate):\n",
    "    # Создаем именно новую модель, старую не трогаем\n",
    "    model = copy.deepcopy(model)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    error = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    \n",
    "    for rate in range(0, compress_rate+1, 10):  # Идем с шагом в 10%\n",
    "        t = calc_threshhold(model, float(rate))  # Считаем очередное пороговое значение\n",
    "        model.prune(t)  # Отключаем слабые связи\n",
    "        correct = 0\n",
    "        for batch_idx, (X_batch, y_batch) in enumerate(train_loader):  # Далее дообучаем модель как обычно в течение одной эпохи\n",
    "            var_X_batch = Variable(X_batch).float()\n",
    "            var_y_batch = Variable(y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(var_X_batch)\n",
    "            loss = error(output, var_y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            predicted = torch.max(output.data, 1)[1] \n",
    "            correct += (predicted == var_y_batch).sum()\n",
    "            if batch_idx % 20 == 0:\n",
    "                print('Rate : {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Accuracy:{:.3f}%'.format(\n",
    "                    rate, batch_idx*len(X_batch), len(train_loader.dataset), 100.*batch_idx / len(train_loader), loss.data, float(correct*100) / float(BATCH_SIZE*(batch_idx+1))))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем для начала выкинуть 70% таким образом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate : 0 [0/49000 (0%)]\tLoss: 0.031662\t Accuracy:100.000%\n",
      "Rate : 0 [640/49000 (1%)]\tLoss: 0.131079\t Accuracy:96.726%\n",
      "Rate : 0 [1280/49000 (3%)]\tLoss: 0.260266\t Accuracy:96.494%\n",
      "Rate : 0 [1920/49000 (4%)]\tLoss: 0.021289\t Accuracy:96.516%\n",
      "Rate : 0 [2560/49000 (5%)]\tLoss: 0.220054\t Accuracy:96.489%\n",
      "Rate : 0 [3200/49000 (7%)]\tLoss: 0.141963\t Accuracy:96.844%\n",
      "Rate : 0 [3840/49000 (8%)]\tLoss: 0.002043\t Accuracy:96.823%\n",
      "Rate : 0 [4480/49000 (9%)]\tLoss: 0.401672\t Accuracy:96.764%\n",
      "Rate : 0 [5120/49000 (10%)]\tLoss: 0.790331\t Accuracy:96.894%\n",
      "Rate : 0 [5760/49000 (12%)]\tLoss: 0.257273\t Accuracy:97.065%\n",
      "Rate : 0 [6400/49000 (13%)]\tLoss: 0.094841\t Accuracy:96.999%\n",
      "Rate : 0 [7040/49000 (14%)]\tLoss: 0.193954\t Accuracy:97.002%\n",
      "Rate : 0 [7680/49000 (16%)]\tLoss: 0.311790\t Accuracy:96.940%\n",
      "Rate : 0 [8320/49000 (17%)]\tLoss: 0.260763\t Accuracy:96.887%\n",
      "Rate : 0 [8960/49000 (18%)]\tLoss: 0.022661\t Accuracy:96.875%\n",
      "Rate : 0 [9600/49000 (20%)]\tLoss: 0.021327\t Accuracy:96.865%\n",
      "Rate : 0 [10240/49000 (21%)]\tLoss: 0.099176\t Accuracy:96.885%\n",
      "Rate : 0 [10880/49000 (22%)]\tLoss: 0.380611\t Accuracy:96.912%\n",
      "Rate : 0 [11520/49000 (23%)]\tLoss: 0.088750\t Accuracy:96.910%\n",
      "Rate : 0 [12160/49000 (25%)]\tLoss: 0.140996\t Accuracy:96.932%\n",
      "Rate : 0 [12800/49000 (26%)]\tLoss: 0.125966\t Accuracy:96.914%\n",
      "Rate : 0 [13440/49000 (27%)]\tLoss: 0.201027\t Accuracy:96.920%\n",
      "Rate : 0 [14080/49000 (29%)]\tLoss: 0.057506\t Accuracy:96.896%\n",
      "Rate : 0 [14720/49000 (30%)]\tLoss: 0.145320\t Accuracy:96.909%\n",
      "Rate : 0 [15360/49000 (31%)]\tLoss: 0.045877\t Accuracy:96.946%\n",
      "Rate : 0 [16000/49000 (33%)]\tLoss: 0.081683\t Accuracy:96.900%\n",
      "Rate : 0 [16640/49000 (34%)]\tLoss: 0.108142\t Accuracy:96.923%\n",
      "Rate : 0 [17280/49000 (35%)]\tLoss: 0.173778\t Accuracy:96.962%\n",
      "Rate : 0 [17920/49000 (37%)]\tLoss: 0.008849\t Accuracy:96.959%\n",
      "Rate : 0 [18560/49000 (38%)]\tLoss: 0.102910\t Accuracy:96.977%\n",
      "Rate : 0 [19200/49000 (39%)]\tLoss: 0.630267\t Accuracy:96.969%\n",
      "Rate : 0 [19840/49000 (40%)]\tLoss: 0.120984\t Accuracy:96.910%\n",
      "Rate : 0 [20480/49000 (42%)]\tLoss: 0.020432\t Accuracy:96.885%\n",
      "Rate : 0 [21120/49000 (43%)]\tLoss: 0.024543\t Accuracy:96.899%\n",
      "Rate : 0 [21760/49000 (44%)]\tLoss: 0.092338\t Accuracy:96.884%\n",
      "Rate : 0 [22400/49000 (46%)]\tLoss: 0.281558\t Accuracy:96.857%\n",
      "Rate : 0 [23040/49000 (47%)]\tLoss: 0.497485\t Accuracy:96.832%\n",
      "Rate : 0 [23680/49000 (48%)]\tLoss: 0.025263\t Accuracy:96.808%\n",
      "Rate : 0 [24320/49000 (50%)]\tLoss: 0.055038\t Accuracy:96.809%\n",
      "Rate : 0 [24960/49000 (51%)]\tLoss: 0.033286\t Accuracy:96.807%\n",
      "Rate : 0 [25600/49000 (52%)]\tLoss: 0.027881\t Accuracy:96.762%\n",
      "Rate : 0 [26240/49000 (54%)]\tLoss: 0.114512\t Accuracy:96.749%\n",
      "Rate : 0 [26880/49000 (55%)]\tLoss: 0.011880\t Accuracy:96.756%\n",
      "Rate : 0 [27520/49000 (56%)]\tLoss: 0.002571\t Accuracy:96.755%\n",
      "Rate : 0 [28160/49000 (57%)]\tLoss: 0.099591\t Accuracy:96.790%\n",
      "Rate : 0 [28800/49000 (59%)]\tLoss: 0.171728\t Accuracy:96.799%\n",
      "Rate : 0 [29440/49000 (60%)]\tLoss: 0.106021\t Accuracy:96.797%\n",
      "Rate : 0 [30080/49000 (61%)]\tLoss: 0.066594\t Accuracy:96.795%\n",
      "Rate : 0 [30720/49000 (63%)]\tLoss: 0.000095\t Accuracy:96.794%\n",
      "Rate : 0 [31360/49000 (64%)]\tLoss: 0.075751\t Accuracy:96.805%\n",
      "Rate : 0 [32000/49000 (65%)]\tLoss: 0.125015\t Accuracy:96.809%\n",
      "Rate : 0 [32640/49000 (67%)]\tLoss: 0.114442\t Accuracy:96.814%\n",
      "Rate : 0 [33280/49000 (68%)]\tLoss: 0.000505\t Accuracy:96.812%\n",
      "Rate : 0 [33920/49000 (69%)]\tLoss: 0.026177\t Accuracy:96.813%\n",
      "Rate : 0 [34560/49000 (70%)]\tLoss: 0.073853\t Accuracy:96.811%\n",
      "Rate : 0 [35200/49000 (72%)]\tLoss: 0.063447\t Accuracy:96.813%\n",
      "Rate : 0 [35840/49000 (73%)]\tLoss: 0.001290\t Accuracy:96.814%\n",
      "Rate : 0 [36480/49000 (74%)]\tLoss: 0.062354\t Accuracy:96.823%\n",
      "Rate : 0 [37120/49000 (76%)]\tLoss: 0.079900\t Accuracy:96.827%\n",
      "Rate : 0 [37760/49000 (77%)]\tLoss: 0.074925\t Accuracy:96.830%\n",
      "Rate : 0 [38400/49000 (78%)]\tLoss: 0.004848\t Accuracy:96.849%\n",
      "Rate : 0 [39040/49000 (80%)]\tLoss: 0.029781\t Accuracy:96.862%\n",
      "Rate : 0 [39680/49000 (81%)]\tLoss: 0.071446\t Accuracy:96.855%\n",
      "Rate : 0 [40320/49000 (82%)]\tLoss: 0.039463\t Accuracy:96.868%\n",
      "Rate : 0 [40960/49000 (84%)]\tLoss: 0.118597\t Accuracy:96.868%\n",
      "Rate : 0 [41600/49000 (85%)]\tLoss: 0.178873\t Accuracy:96.880%\n",
      "Rate : 0 [42240/49000 (86%)]\tLoss: 0.000623\t Accuracy:96.894%\n",
      "Rate : 0 [42880/49000 (87%)]\tLoss: 0.086130\t Accuracy:96.894%\n",
      "Rate : 0 [43520/49000 (89%)]\tLoss: 0.080527\t Accuracy:96.898%\n",
      "Rate : 0 [44160/49000 (90%)]\tLoss: 0.239529\t Accuracy:96.893%\n",
      "Rate : 0 [44800/49000 (91%)]\tLoss: 0.193887\t Accuracy:96.879%\n",
      "Rate : 0 [45440/49000 (93%)]\tLoss: 0.055580\t Accuracy:96.884%\n",
      "Rate : 0 [46080/49000 (94%)]\tLoss: 0.020557\t Accuracy:96.886%\n",
      "Rate : 0 [46720/49000 (95%)]\tLoss: 0.011671\t Accuracy:96.905%\n",
      "Rate : 0 [47360/49000 (97%)]\tLoss: 0.007573\t Accuracy:96.913%\n",
      "Rate : 0 [48000/49000 (98%)]\tLoss: 0.012418\t Accuracy:96.910%\n",
      "Rate : 0 [48640/49000 (99%)]\tLoss: 0.186314\t Accuracy:96.908%\n",
      "Rate : 10 [0/49000 (0%)]\tLoss: 0.130172\t Accuracy:96.875%\n",
      "Rate : 10 [640/49000 (1%)]\tLoss: 0.070500\t Accuracy:97.173%\n",
      "Rate : 10 [1280/49000 (3%)]\tLoss: 0.045693\t Accuracy:97.485%\n",
      "Rate : 10 [1920/49000 (4%)]\tLoss: 0.046309\t Accuracy:97.541%\n",
      "Rate : 10 [2560/49000 (5%)]\tLoss: 0.016294\t Accuracy:97.840%\n",
      "Rate : 10 [3200/49000 (7%)]\tLoss: 0.018504\t Accuracy:98.020%\n",
      "Rate : 10 [3840/49000 (8%)]\tLoss: 0.081994\t Accuracy:97.856%\n",
      "Rate : 10 [4480/49000 (9%)]\tLoss: 0.050444\t Accuracy:97.739%\n",
      "Rate : 10 [5120/49000 (10%)]\tLoss: 0.013676\t Accuracy:97.729%\n",
      "Rate : 10 [5760/49000 (12%)]\tLoss: 0.057164\t Accuracy:97.756%\n",
      "Rate : 10 [6400/49000 (13%)]\tLoss: 0.150604\t Accuracy:97.761%\n",
      "Rate : 10 [7040/49000 (14%)]\tLoss: 0.054256\t Accuracy:97.837%\n",
      "Rate : 10 [7680/49000 (16%)]\tLoss: 0.104461\t Accuracy:97.796%\n",
      "Rate : 10 [8320/49000 (17%)]\tLoss: 0.190020\t Accuracy:97.797%\n",
      "Rate : 10 [8960/49000 (18%)]\tLoss: 0.048253\t Accuracy:97.742%\n",
      "Rate : 10 [9600/49000 (20%)]\tLoss: 0.000412\t Accuracy:97.737%\n",
      "Rate : 10 [10240/49000 (21%)]\tLoss: 0.026874\t Accuracy:97.771%\n",
      "Rate : 10 [10880/49000 (22%)]\tLoss: 0.155573\t Accuracy:97.764%\n",
      "Rate : 10 [11520/49000 (23%)]\tLoss: 0.034213\t Accuracy:97.741%\n",
      "Rate : 10 [12160/49000 (25%)]\tLoss: 0.152737\t Accuracy:97.720%\n",
      "Rate : 10 [12800/49000 (26%)]\tLoss: 0.171847\t Accuracy:97.693%\n",
      "Rate : 10 [13440/49000 (27%)]\tLoss: 0.195356\t Accuracy:97.714%\n",
      "Rate : 10 [14080/49000 (29%)]\tLoss: 0.201085\t Accuracy:97.718%\n",
      "Rate : 10 [14720/49000 (30%)]\tLoss: 0.116481\t Accuracy:97.729%\n",
      "Rate : 10 [15360/49000 (31%)]\tLoss: 0.003904\t Accuracy:97.694%\n",
      "Rate : 10 [16000/49000 (33%)]\tLoss: 0.078976\t Accuracy:97.680%\n",
      "Rate : 10 [16640/49000 (34%)]\tLoss: 0.099231\t Accuracy:97.685%\n",
      "Rate : 10 [17280/49000 (35%)]\tLoss: 0.013693\t Accuracy:97.684%\n",
      "Rate : 10 [17920/49000 (37%)]\tLoss: 0.009308\t Accuracy:97.683%\n",
      "Rate : 10 [18560/49000 (38%)]\tLoss: 0.015774\t Accuracy:97.660%\n",
      "Rate : 10 [19200/49000 (39%)]\tLoss: 0.228800\t Accuracy:97.639%\n",
      "Rate : 10 [19840/49000 (40%)]\tLoss: 0.126839\t Accuracy:97.620%\n",
      "Rate : 10 [20480/49000 (42%)]\tLoss: 0.191691\t Accuracy:97.621%\n",
      "Rate : 10 [21120/49000 (43%)]\tLoss: 0.218321\t Accuracy:97.631%\n",
      "Rate : 10 [21760/49000 (44%)]\tLoss: 0.083452\t Accuracy:97.614%\n",
      "Rate : 10 [22400/49000 (46%)]\tLoss: 0.197259\t Accuracy:97.602%\n",
      "Rate : 10 [23040/49000 (47%)]\tLoss: 0.365309\t Accuracy:97.581%\n",
      "Rate : 10 [23680/49000 (48%)]\tLoss: 0.000575\t Accuracy:97.575%\n",
      "Rate : 10 [24320/49000 (50%)]\tLoss: 0.007291\t Accuracy:97.573%\n",
      "Rate : 10 [24960/49000 (51%)]\tLoss: 0.045943\t Accuracy:97.587%\n",
      "Rate : 10 [25600/49000 (52%)]\tLoss: 0.002280\t Accuracy:97.585%\n",
      "Rate : 10 [26240/49000 (54%)]\tLoss: 0.010286\t Accuracy:97.613%\n",
      "Rate : 10 [26880/49000 (55%)]\tLoss: 0.000270\t Accuracy:97.614%\n",
      "Rate : 10 [27520/49000 (56%)]\tLoss: 0.005613\t Accuracy:97.623%\n",
      "Rate : 10 [28160/49000 (57%)]\tLoss: 0.316800\t Accuracy:97.623%\n",
      "Rate : 10 [28800/49000 (59%)]\tLoss: 0.039651\t Accuracy:97.638%\n",
      "Rate : 10 [29440/49000 (60%)]\tLoss: 0.013408\t Accuracy:97.625%\n",
      "Rate : 10 [30080/49000 (61%)]\tLoss: 0.019781\t Accuracy:97.619%\n",
      "Rate : 10 [30720/49000 (63%)]\tLoss: 0.001216\t Accuracy:97.629%\n",
      "Rate : 10 [31360/49000 (64%)]\tLoss: 0.286940\t Accuracy:97.640%\n",
      "Rate : 10 [32000/49000 (65%)]\tLoss: 0.012899\t Accuracy:97.655%\n",
      "Rate : 10 [32640/49000 (67%)]\tLoss: 0.017369\t Accuracy:97.652%\n",
      "Rate : 10 [33280/49000 (68%)]\tLoss: 0.011355\t Accuracy:97.659%\n",
      "Rate : 10 [33920/49000 (69%)]\tLoss: 0.010502\t Accuracy:97.647%\n",
      "Rate : 10 [34560/49000 (70%)]\tLoss: 0.614559\t Accuracy:97.647%\n",
      "Rate : 10 [35200/49000 (72%)]\tLoss: 0.172922\t Accuracy:97.630%\n",
      "Rate : 10 [35840/49000 (73%)]\tLoss: 0.028295\t Accuracy:97.642%\n",
      "Rate : 10 [36480/49000 (74%)]\tLoss: 0.004890\t Accuracy:97.636%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate : 10 [37120/49000 (76%)]\tLoss: 0.004724\t Accuracy:97.648%\n",
      "Rate : 10 [37760/49000 (77%)]\tLoss: 0.010148\t Accuracy:97.664%\n",
      "Rate : 10 [38400/49000 (78%)]\tLoss: 0.180426\t Accuracy:97.674%\n",
      "Rate : 10 [39040/49000 (80%)]\tLoss: 0.026186\t Accuracy:97.676%\n",
      "Rate : 10 [39680/49000 (81%)]\tLoss: 0.006909\t Accuracy:97.681%\n",
      "Rate : 10 [40320/49000 (82%)]\tLoss: 0.015055\t Accuracy:97.703%\n",
      "Rate : 10 [40960/49000 (84%)]\tLoss: 0.019710\t Accuracy:97.704%\n",
      "Rate : 10 [41600/49000 (85%)]\tLoss: 0.130606\t Accuracy:97.713%\n",
      "Rate : 10 [42240/49000 (86%)]\tLoss: 0.029194\t Accuracy:97.708%\n",
      "Rate : 10 [42880/49000 (87%)]\tLoss: 0.044540\t Accuracy:97.695%\n",
      "Rate : 10 [43520/49000 (89%)]\tLoss: 0.014154\t Accuracy:97.699%\n",
      "Rate : 10 [44160/49000 (90%)]\tLoss: 0.025861\t Accuracy:97.694%\n",
      "Rate : 10 [44800/49000 (91%)]\tLoss: 0.049685\t Accuracy:97.689%\n",
      "Rate : 10 [45440/49000 (93%)]\tLoss: 0.088772\t Accuracy:97.689%\n",
      "Rate : 10 [46080/49000 (94%)]\tLoss: 0.094827\t Accuracy:97.688%\n",
      "Rate : 10 [46720/49000 (95%)]\tLoss: 0.048499\t Accuracy:97.690%\n",
      "Rate : 10 [47360/49000 (97%)]\tLoss: 0.005357\t Accuracy:97.698%\n",
      "Rate : 10 [48000/49000 (98%)]\tLoss: 0.239996\t Accuracy:97.689%\n",
      "Rate : 10 [48640/49000 (99%)]\tLoss: 0.266902\t Accuracy:97.682%\n",
      "Rate : 20 [0/49000 (0%)]\tLoss: 0.018176\t Accuracy:100.000%\n",
      "Rate : 20 [640/49000 (1%)]\tLoss: 0.007665\t Accuracy:97.619%\n",
      "Rate : 20 [1280/49000 (3%)]\tLoss: 0.045438\t Accuracy:97.637%\n",
      "Rate : 20 [1920/49000 (4%)]\tLoss: 0.016563\t Accuracy:97.643%\n",
      "Rate : 20 [2560/49000 (5%)]\tLoss: 0.041265\t Accuracy:97.569%\n",
      "Rate : 20 [3200/49000 (7%)]\tLoss: 0.105213\t Accuracy:97.679%\n",
      "Rate : 20 [3840/49000 (8%)]\tLoss: 0.034302\t Accuracy:97.676%\n",
      "Rate : 20 [4480/49000 (9%)]\tLoss: 0.006492\t Accuracy:97.784%\n",
      "Rate : 20 [5120/49000 (10%)]\tLoss: 0.133934\t Accuracy:97.768%\n",
      "Rate : 20 [5760/49000 (12%)]\tLoss: 0.019055\t Accuracy:97.807%\n",
      "Rate : 20 [6400/49000 (13%)]\tLoss: 0.008506\t Accuracy:97.808%\n",
      "Rate : 20 [7040/49000 (14%)]\tLoss: 0.015899\t Accuracy:97.822%\n",
      "Rate : 20 [7680/49000 (16%)]\tLoss: 0.047409\t Accuracy:97.705%\n",
      "Rate : 20 [8320/49000 (17%)]\tLoss: 0.136833\t Accuracy:97.641%\n",
      "Rate : 20 [8960/49000 (18%)]\tLoss: 0.063089\t Accuracy:97.687%\n",
      "Rate : 20 [9600/49000 (20%)]\tLoss: 0.122201\t Accuracy:97.726%\n",
      "Rate : 20 [10240/49000 (21%)]\tLoss: 0.053441\t Accuracy:97.751%\n",
      "Rate : 20 [10880/49000 (22%)]\tLoss: 0.244249\t Accuracy:97.718%\n",
      "Rate : 20 [11520/49000 (23%)]\tLoss: 0.001550\t Accuracy:97.732%\n",
      "Rate : 20 [12160/49000 (25%)]\tLoss: 0.073823\t Accuracy:97.744%\n",
      "Rate : 20 [12800/49000 (26%)]\tLoss: 0.138365\t Accuracy:97.763%\n",
      "Rate : 20 [13440/49000 (27%)]\tLoss: 0.047745\t Accuracy:97.773%\n",
      "Rate : 20 [14080/49000 (29%)]\tLoss: 0.306271\t Accuracy:97.747%\n",
      "Rate : 20 [14720/49000 (30%)]\tLoss: 0.177596\t Accuracy:97.777%\n",
      "Rate : 20 [15360/49000 (31%)]\tLoss: 0.076251\t Accuracy:97.791%\n",
      "Rate : 20 [16000/49000 (33%)]\tLoss: 0.016088\t Accuracy:97.792%\n",
      "Rate : 20 [16640/49000 (34%)]\tLoss: 0.164912\t Accuracy:97.811%\n",
      "Rate : 20 [17280/49000 (35%)]\tLoss: 0.112924\t Accuracy:97.840%\n",
      "Rate : 20 [17920/49000 (37%)]\tLoss: 0.046814\t Accuracy:97.822%\n",
      "Rate : 20 [18560/49000 (38%)]\tLoss: 0.001347\t Accuracy:97.822%\n",
      "Rate : 20 [19200/49000 (39%)]\tLoss: 0.229921\t Accuracy:97.816%\n",
      "Rate : 20 [19840/49000 (40%)]\tLoss: 0.048843\t Accuracy:97.831%\n",
      "Rate : 20 [20480/49000 (42%)]\tLoss: 0.008912\t Accuracy:97.850%\n",
      "Rate : 20 [21120/49000 (43%)]\tLoss: 0.475268\t Accuracy:97.844%\n",
      "Rate : 20 [21760/49000 (44%)]\tLoss: 0.676076\t Accuracy:97.816%\n",
      "Rate : 20 [22400/49000 (46%)]\tLoss: 0.174200\t Accuracy:97.807%\n",
      "Rate : 20 [23040/49000 (47%)]\tLoss: 0.414640\t Accuracy:97.790%\n",
      "Rate : 20 [23680/49000 (48%)]\tLoss: 0.008077\t Accuracy:97.794%\n",
      "Rate : 20 [24320/49000 (50%)]\tLoss: 0.189149\t Accuracy:97.803%\n",
      "Rate : 20 [24960/49000 (51%)]\tLoss: 0.007701\t Accuracy:97.819%\n",
      "Rate : 20 [25600/49000 (52%)]\tLoss: 0.000258\t Accuracy:97.804%\n",
      "Rate : 20 [26240/49000 (54%)]\tLoss: 0.003974\t Accuracy:97.804%\n",
      "Rate : 20 [26880/49000 (55%)]\tLoss: 0.001206\t Accuracy:97.800%\n",
      "Rate : 20 [27520/49000 (56%)]\tLoss: 0.031200\t Accuracy:97.811%\n",
      "Rate : 20 [28160/49000 (57%)]\tLoss: 0.082282\t Accuracy:97.833%\n",
      "Rate : 20 [28800/49000 (59%)]\tLoss: 0.029022\t Accuracy:97.829%\n",
      "Rate : 20 [29440/49000 (60%)]\tLoss: 0.017293\t Accuracy:97.832%\n",
      "Rate : 20 [30080/49000 (61%)]\tLoss: 0.006983\t Accuracy:97.835%\n",
      "Rate : 20 [30720/49000 (63%)]\tLoss: 0.000129\t Accuracy:97.857%\n",
      "Rate : 20 [31360/49000 (64%)]\tLoss: 0.046053\t Accuracy:97.863%\n",
      "Rate : 20 [32000/49000 (65%)]\tLoss: 0.034517\t Accuracy:97.880%\n",
      "Rate : 20 [32640/49000 (67%)]\tLoss: 0.083302\t Accuracy:97.876%\n",
      "Rate : 20 [33280/49000 (68%)]\tLoss: 0.000935\t Accuracy:97.872%\n",
      "Rate : 20 [33920/49000 (69%)]\tLoss: 0.004944\t Accuracy:97.868%\n",
      "Rate : 20 [34560/49000 (70%)]\tLoss: 0.075139\t Accuracy:97.872%\n",
      "Rate : 20 [35200/49000 (72%)]\tLoss: 0.006896\t Accuracy:97.849%\n",
      "Rate : 20 [35840/49000 (73%)]\tLoss: 0.022994\t Accuracy:97.867%\n",
      "Rate : 20 [36480/49000 (74%)]\tLoss: 0.032636\t Accuracy:97.877%\n",
      "Rate : 20 [37120/49000 (76%)]\tLoss: 0.020873\t Accuracy:97.882%\n",
      "Rate : 20 [37760/49000 (77%)]\tLoss: 0.066126\t Accuracy:97.875%\n",
      "Rate : 20 [38400/49000 (78%)]\tLoss: 0.021631\t Accuracy:97.890%\n",
      "Rate : 20 [39040/49000 (80%)]\tLoss: 0.050049\t Accuracy:97.886%\n",
      "Rate : 20 [39680/49000 (81%)]\tLoss: 0.011659\t Accuracy:97.902%\n",
      "Rate : 20 [40320/49000 (82%)]\tLoss: 0.126381\t Accuracy:97.911%\n",
      "Rate : 20 [40960/49000 (84%)]\tLoss: 0.037759\t Accuracy:97.926%\n",
      "Rate : 20 [41600/49000 (85%)]\tLoss: 0.000041\t Accuracy:97.944%\n",
      "Rate : 20 [42240/49000 (86%)]\tLoss: 0.000165\t Accuracy:97.958%\n",
      "Rate : 20 [42880/49000 (87%)]\tLoss: 0.012218\t Accuracy:97.961%\n",
      "Rate : 20 [43520/49000 (89%)]\tLoss: 0.002749\t Accuracy:97.975%\n",
      "Rate : 20 [44160/49000 (90%)]\tLoss: 0.004959\t Accuracy:97.979%\n",
      "Rate : 20 [44800/49000 (91%)]\tLoss: 0.205176\t Accuracy:97.966%\n",
      "Rate : 20 [45440/49000 (93%)]\tLoss: 0.005878\t Accuracy:97.970%\n",
      "Rate : 20 [46080/49000 (94%)]\tLoss: 0.003805\t Accuracy:97.979%\n",
      "Rate : 20 [46720/49000 (95%)]\tLoss: 0.001590\t Accuracy:98.000%\n",
      "Rate : 20 [47360/49000 (97%)]\tLoss: 0.012672\t Accuracy:98.014%\n",
      "Rate : 20 [48000/49000 (98%)]\tLoss: 0.098739\t Accuracy:98.018%\n",
      "Rate : 20 [48640/49000 (99%)]\tLoss: 0.020585\t Accuracy:98.026%\n",
      "Rate : 30 [0/49000 (0%)]\tLoss: 0.035376\t Accuracy:100.000%\n",
      "Rate : 30 [640/49000 (1%)]\tLoss: 0.000935\t Accuracy:97.917%\n",
      "Rate : 30 [1280/49000 (3%)]\tLoss: 0.228531\t Accuracy:97.561%\n",
      "Rate : 30 [1920/49000 (4%)]\tLoss: 0.005565\t Accuracy:97.592%\n",
      "Rate : 30 [2560/49000 (5%)]\tLoss: 0.106431\t Accuracy:97.492%\n",
      "Rate : 30 [3200/49000 (7%)]\tLoss: 0.003466\t Accuracy:97.772%\n",
      "Rate : 30 [3840/49000 (8%)]\tLoss: 0.000575\t Accuracy:97.676%\n",
      "Rate : 30 [4480/49000 (9%)]\tLoss: 0.050802\t Accuracy:97.762%\n",
      "Rate : 30 [5120/49000 (10%)]\tLoss: 0.159722\t Accuracy:97.748%\n",
      "Rate : 30 [5760/49000 (12%)]\tLoss: 0.042979\t Accuracy:97.876%\n",
      "Rate : 30 [6400/49000 (13%)]\tLoss: 0.000538\t Accuracy:97.979%\n",
      "Rate : 30 [7040/49000 (14%)]\tLoss: 0.093829\t Accuracy:98.006%\n",
      "Rate : 30 [7680/49000 (16%)]\tLoss: 0.061130\t Accuracy:97.990%\n",
      "Rate : 30 [8320/49000 (17%)]\tLoss: 0.083032\t Accuracy:98.012%\n",
      "Rate : 30 [8960/49000 (18%)]\tLoss: 0.068017\t Accuracy:98.032%\n",
      "Rate : 30 [9600/49000 (20%)]\tLoss: 0.000363\t Accuracy:98.038%\n",
      "Rate : 30 [10240/49000 (21%)]\tLoss: 0.006010\t Accuracy:98.053%\n",
      "Rate : 30 [10880/49000 (22%)]\tLoss: 0.185848\t Accuracy:98.066%\n",
      "Rate : 30 [11520/49000 (23%)]\tLoss: 0.003007\t Accuracy:98.104%\n",
      "Rate : 30 [12160/49000 (25%)]\tLoss: 0.055719\t Accuracy:98.163%\n",
      "Rate : 30 [12800/49000 (26%)]\tLoss: 0.191642\t Accuracy:98.137%\n",
      "Rate : 30 [13440/49000 (27%)]\tLoss: 0.008630\t Accuracy:98.137%\n",
      "Rate : 30 [14080/49000 (29%)]\tLoss: 0.066988\t Accuracy:98.101%\n",
      "Rate : 30 [14720/49000 (30%)]\tLoss: 0.077559\t Accuracy:98.136%\n",
      "Rate : 30 [15360/49000 (31%)]\tLoss: 0.000277\t Accuracy:98.174%\n",
      "Rate : 30 [16000/49000 (33%)]\tLoss: 0.035449\t Accuracy:98.166%\n",
      "Rate : 30 [16640/49000 (34%)]\tLoss: 0.001090\t Accuracy:98.195%\n",
      "Rate : 30 [17280/49000 (35%)]\tLoss: 0.001895\t Accuracy:98.227%\n",
      "Rate : 30 [17920/49000 (37%)]\tLoss: 0.034852\t Accuracy:98.223%\n",
      "Rate : 30 [18560/49000 (38%)]\tLoss: 0.007322\t Accuracy:98.236%\n",
      "Rate : 30 [19200/49000 (39%)]\tLoss: 0.509448\t Accuracy:98.191%\n",
      "Rate : 30 [19840/49000 (40%)]\tLoss: 0.228519\t Accuracy:98.178%\n",
      "Rate : 30 [20480/49000 (42%)]\tLoss: 0.157045\t Accuracy:98.162%\n",
      "Rate : 30 [21120/49000 (43%)]\tLoss: 0.010604\t Accuracy:98.189%\n",
      "Rate : 30 [21760/49000 (44%)]\tLoss: 0.111506\t Accuracy:98.151%\n",
      "Rate : 30 [22400/49000 (46%)]\tLoss: 0.154484\t Accuracy:98.137%\n",
      "Rate : 30 [23040/49000 (47%)]\tLoss: 0.535011\t Accuracy:98.128%\n",
      "Rate : 30 [23680/49000 (48%)]\tLoss: 0.031808\t Accuracy:98.144%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate : 30 [24320/49000 (50%)]\tLoss: 0.027562\t Accuracy:98.160%\n",
      "Rate : 30 [24960/49000 (51%)]\tLoss: 0.015172\t Accuracy:98.155%\n",
      "Rate : 30 [25600/49000 (52%)]\tLoss: 0.005900\t Accuracy:98.139%\n",
      "Rate : 30 [26240/49000 (54%)]\tLoss: 0.006174\t Accuracy:98.116%\n",
      "Rate : 30 [26880/49000 (55%)]\tLoss: 0.013263\t Accuracy:98.127%\n",
      "Rate : 30 [27520/49000 (56%)]\tLoss: 0.000700\t Accuracy:98.131%\n",
      "Rate : 30 [28160/49000 (57%)]\tLoss: 0.193057\t Accuracy:98.145%\n",
      "Rate : 30 [28800/49000 (59%)]\tLoss: 0.026822\t Accuracy:98.165%\n",
      "Rate : 30 [29440/49000 (60%)]\tLoss: 0.052132\t Accuracy:98.168%\n",
      "Rate : 30 [30080/49000 (61%)]\tLoss: 0.064794\t Accuracy:98.164%\n",
      "Rate : 30 [30720/49000 (63%)]\tLoss: 0.000068\t Accuracy:98.163%\n",
      "Rate : 30 [31360/49000 (64%)]\tLoss: 0.373646\t Accuracy:98.175%\n",
      "Rate : 30 [32000/49000 (65%)]\tLoss: 0.001499\t Accuracy:98.192%\n",
      "Rate : 30 [32640/49000 (67%)]\tLoss: 0.007683\t Accuracy:98.182%\n",
      "Rate : 30 [33280/49000 (68%)]\tLoss: 0.004379\t Accuracy:98.175%\n",
      "Rate : 30 [33920/49000 (69%)]\tLoss: 0.018445\t Accuracy:98.174%\n",
      "Rate : 30 [34560/49000 (70%)]\tLoss: 0.024781\t Accuracy:98.173%\n",
      "Rate : 30 [35200/49000 (72%)]\tLoss: 0.031076\t Accuracy:98.164%\n",
      "Rate : 30 [35840/49000 (73%)]\tLoss: 0.298003\t Accuracy:98.171%\n",
      "Rate : 30 [36480/49000 (74%)]\tLoss: 0.005579\t Accuracy:98.181%\n",
      "Rate : 30 [37120/49000 (76%)]\tLoss: 0.017544\t Accuracy:98.202%\n",
      "Rate : 30 [37760/49000 (77%)]\tLoss: 0.127158\t Accuracy:98.203%\n",
      "Rate : 30 [38400/49000 (78%)]\tLoss: 0.023017\t Accuracy:98.215%\n",
      "Rate : 30 [39040/49000 (80%)]\tLoss: 0.016264\t Accuracy:98.221%\n",
      "Rate : 30 [39680/49000 (81%)]\tLoss: 0.005018\t Accuracy:98.230%\n",
      "Rate : 30 [40320/49000 (82%)]\tLoss: 0.000798\t Accuracy:98.240%\n",
      "Rate : 30 [40960/49000 (84%)]\tLoss: 0.059541\t Accuracy:98.248%\n",
      "Rate : 30 [41600/49000 (85%)]\tLoss: 0.000029\t Accuracy:98.266%\n",
      "Rate : 30 [42240/49000 (86%)]\tLoss: 0.003160\t Accuracy:98.273%\n",
      "Rate : 30 [42880/49000 (87%)]\tLoss: 0.167323\t Accuracy:98.269%\n",
      "Rate : 30 [43520/49000 (89%)]\tLoss: 0.169124\t Accuracy:98.273%\n",
      "Rate : 30 [44160/49000 (90%)]\tLoss: 0.005129\t Accuracy:98.285%\n",
      "Rate : 30 [44800/49000 (91%)]\tLoss: 0.056921\t Accuracy:98.289%\n",
      "Rate : 30 [45440/49000 (93%)]\tLoss: 0.065756\t Accuracy:98.293%\n",
      "Rate : 30 [46080/49000 (94%)]\tLoss: 0.006368\t Accuracy:98.298%\n",
      "Rate : 30 [46720/49000 (95%)]\tLoss: 0.003316\t Accuracy:98.308%\n",
      "Rate : 30 [47360/49000 (97%)]\tLoss: 0.000822\t Accuracy:98.325%\n",
      "Rate : 30 [48000/49000 (98%)]\tLoss: 0.015576\t Accuracy:98.324%\n",
      "Rate : 30 [48640/49000 (99%)]\tLoss: 0.133980\t Accuracy:98.319%\n",
      "Rate : 40 [0/49000 (0%)]\tLoss: 0.000713\t Accuracy:100.000%\n",
      "Rate : 40 [640/49000 (1%)]\tLoss: 0.226459\t Accuracy:97.619%\n",
      "Rate : 40 [1280/49000 (3%)]\tLoss: 0.034430\t Accuracy:97.790%\n",
      "Rate : 40 [1920/49000 (4%)]\tLoss: 0.000074\t Accuracy:98.002%\n",
      "Rate : 40 [2560/49000 (5%)]\tLoss: 0.049367\t Accuracy:97.840%\n",
      "Rate : 40 [3200/49000 (7%)]\tLoss: 0.107128\t Accuracy:98.020%\n",
      "Rate : 40 [3840/49000 (8%)]\tLoss: 0.001670\t Accuracy:97.882%\n",
      "Rate : 40 [4480/49000 (9%)]\tLoss: 0.002688\t Accuracy:98.005%\n",
      "Rate : 40 [5120/49000 (10%)]\tLoss: 0.218041\t Accuracy:98.059%\n",
      "Rate : 40 [5760/49000 (12%)]\tLoss: 0.009335\t Accuracy:98.135%\n",
      "Rate : 40 [6400/49000 (13%)]\tLoss: 0.109970\t Accuracy:98.119%\n",
      "Rate : 40 [7040/49000 (14%)]\tLoss: 0.007004\t Accuracy:98.247%\n",
      "Rate : 40 [7680/49000 (16%)]\tLoss: 0.070284\t Accuracy:98.262%\n",
      "Rate : 40 [8320/49000 (17%)]\tLoss: 0.079069\t Accuracy:98.276%\n",
      "Rate : 40 [8960/49000 (18%)]\tLoss: 0.004937\t Accuracy:98.287%\n",
      "Rate : 40 [9600/49000 (20%)]\tLoss: 0.000140\t Accuracy:98.339%\n",
      "Rate : 40 [10240/49000 (21%)]\tLoss: 0.117213\t Accuracy:98.364%\n",
      "Rate : 40 [10880/49000 (22%)]\tLoss: 0.022256\t Accuracy:98.350%\n",
      "Rate : 40 [11520/49000 (23%)]\tLoss: 0.001857\t Accuracy:98.373%\n",
      "Rate : 40 [12160/49000 (25%)]\tLoss: 0.049902\t Accuracy:98.384%\n",
      "Rate : 40 [12800/49000 (26%)]\tLoss: 0.154454\t Accuracy:98.363%\n",
      "Rate : 40 [13440/49000 (27%)]\tLoss: 0.033776\t Accuracy:98.360%\n",
      "Rate : 40 [14080/49000 (29%)]\tLoss: 0.055129\t Accuracy:98.342%\n",
      "Rate : 40 [14720/49000 (30%)]\tLoss: 0.033037\t Accuracy:98.360%\n",
      "Rate : 40 [15360/49000 (31%)]\tLoss: 0.016014\t Accuracy:98.363%\n",
      "Rate : 40 [16000/49000 (33%)]\tLoss: 0.008467\t Accuracy:98.353%\n",
      "Rate : 40 [16640/49000 (34%)]\tLoss: 0.005226\t Accuracy:98.369%\n",
      "Rate : 40 [17280/49000 (35%)]\tLoss: 0.006803\t Accuracy:98.394%\n",
      "Rate : 40 [17920/49000 (37%)]\tLoss: 0.007277\t Accuracy:98.396%\n",
      "Rate : 40 [18560/49000 (38%)]\tLoss: 0.005433\t Accuracy:98.386%\n",
      "Rate : 40 [19200/49000 (39%)]\tLoss: 0.201014\t Accuracy:98.378%\n",
      "Rate : 40 [19840/49000 (40%)]\tLoss: 0.221249\t Accuracy:98.380%\n",
      "Rate : 40 [20480/49000 (42%)]\tLoss: 0.000365\t Accuracy:98.391%\n",
      "Rate : 40 [21120/49000 (43%)]\tLoss: 0.073823\t Accuracy:98.397%\n",
      "Rate : 40 [21760/49000 (44%)]\tLoss: 0.065357\t Accuracy:98.385%\n",
      "Rate : 40 [22400/49000 (46%)]\tLoss: 0.078905\t Accuracy:98.395%\n",
      "Rate : 40 [23040/49000 (47%)]\tLoss: 0.343112\t Accuracy:98.383%\n",
      "Rate : 40 [23680/49000 (48%)]\tLoss: 0.218455\t Accuracy:98.359%\n",
      "Rate : 40 [24320/49000 (50%)]\tLoss: 0.026886\t Accuracy:98.349%\n",
      "Rate : 40 [24960/49000 (51%)]\tLoss: 0.005869\t Accuracy:98.339%\n",
      "Rate : 40 [25600/49000 (52%)]\tLoss: 0.072865\t Accuracy:98.307%\n",
      "Rate : 40 [26240/49000 (54%)]\tLoss: 0.013536\t Accuracy:98.325%\n",
      "Rate : 40 [26880/49000 (55%)]\tLoss: 0.092743\t Accuracy:98.317%\n",
      "Rate : 40 [27520/49000 (56%)]\tLoss: 0.000115\t Accuracy:98.320%\n",
      "Rate : 40 [28160/49000 (57%)]\tLoss: 0.021726\t Accuracy:98.347%\n",
      "Rate : 40 [28800/49000 (59%)]\tLoss: 0.002629\t Accuracy:98.359%\n",
      "Rate : 40 [29440/49000 (60%)]\tLoss: 0.006142\t Accuracy:98.371%\n",
      "Rate : 40 [30080/49000 (61%)]\tLoss: 0.008787\t Accuracy:98.373%\n",
      "Rate : 40 [30720/49000 (63%)]\tLoss: 0.004448\t Accuracy:98.394%\n",
      "Rate : 40 [31360/49000 (64%)]\tLoss: 0.201739\t Accuracy:98.401%\n",
      "Rate : 40 [32000/49000 (65%)]\tLoss: 0.040506\t Accuracy:98.420%\n",
      "Rate : 40 [32640/49000 (67%)]\tLoss: 0.045853\t Accuracy:98.405%\n",
      "Rate : 40 [33280/49000 (68%)]\tLoss: 0.206820\t Accuracy:98.403%\n",
      "Rate : 40 [33920/49000 (69%)]\tLoss: 0.001824\t Accuracy:98.398%\n",
      "Rate : 40 [34560/49000 (70%)]\tLoss: 0.043636\t Accuracy:98.396%\n",
      "Rate : 40 [35200/49000 (72%)]\tLoss: 0.031676\t Accuracy:98.396%\n",
      "Rate : 40 [35840/49000 (73%)]\tLoss: 0.029934\t Accuracy:98.389%\n",
      "Rate : 40 [36480/49000 (74%)]\tLoss: 0.006243\t Accuracy:98.406%\n",
      "Rate : 40 [37120/49000 (76%)]\tLoss: 0.011960\t Accuracy:98.420%\n",
      "Rate : 40 [37760/49000 (77%)]\tLoss: 0.058667\t Accuracy:98.420%\n",
      "Rate : 40 [38400/49000 (78%)]\tLoss: 0.005278\t Accuracy:98.439%\n",
      "Rate : 40 [39040/49000 (80%)]\tLoss: 0.007595\t Accuracy:98.444%\n",
      "Rate : 40 [39680/49000 (81%)]\tLoss: 0.000952\t Accuracy:98.451%\n",
      "Rate : 40 [40320/49000 (82%)]\tLoss: 0.050325\t Accuracy:98.454%\n",
      "Rate : 40 [40960/49000 (84%)]\tLoss: 0.130205\t Accuracy:98.458%\n",
      "Rate : 40 [41600/49000 (85%)]\tLoss: 0.000017\t Accuracy:98.470%\n",
      "Rate : 40 [42240/49000 (86%)]\tLoss: 0.000178\t Accuracy:98.481%\n",
      "Rate : 40 [42880/49000 (87%)]\tLoss: 0.039351\t Accuracy:98.483%\n",
      "Rate : 40 [43520/49000 (89%)]\tLoss: 0.006483\t Accuracy:98.494%\n",
      "Rate : 40 [44160/49000 (90%)]\tLoss: 0.036766\t Accuracy:98.495%\n",
      "Rate : 40 [44800/49000 (91%)]\tLoss: 0.119224\t Accuracy:98.485%\n",
      "Rate : 40 [45440/49000 (93%)]\tLoss: 0.008269\t Accuracy:98.491%\n",
      "Rate : 40 [46080/49000 (94%)]\tLoss: 0.059235\t Accuracy:98.491%\n",
      "Rate : 40 [46720/49000 (95%)]\tLoss: 0.042620\t Accuracy:98.498%\n",
      "Rate : 40 [47360/49000 (97%)]\tLoss: 0.001750\t Accuracy:98.510%\n",
      "Rate : 40 [48000/49000 (98%)]\tLoss: 0.022500\t Accuracy:98.518%\n",
      "Rate : 40 [48640/49000 (99%)]\tLoss: 0.082090\t Accuracy:98.517%\n",
      "Rate : 50 [0/49000 (0%)]\tLoss: 0.001182\t Accuracy:100.000%\n",
      "Rate : 50 [640/49000 (1%)]\tLoss: 0.010626\t Accuracy:98.512%\n",
      "Rate : 50 [1280/49000 (3%)]\tLoss: 0.104825\t Accuracy:98.171%\n",
      "Rate : 50 [1920/49000 (4%)]\tLoss: 0.008710\t Accuracy:98.053%\n",
      "Rate : 50 [2560/49000 (5%)]\tLoss: 0.117934\t Accuracy:98.341%\n",
      "Rate : 50 [3200/49000 (7%)]\tLoss: 0.001469\t Accuracy:98.391%\n",
      "Rate : 50 [3840/49000 (8%)]\tLoss: 0.013140\t Accuracy:98.373%\n",
      "Rate : 50 [4480/49000 (9%)]\tLoss: 0.000388\t Accuracy:98.426%\n",
      "Rate : 50 [5120/49000 (10%)]\tLoss: 0.000939\t Accuracy:98.467%\n",
      "Rate : 50 [5760/49000 (12%)]\tLoss: 0.010567\t Accuracy:98.567%\n",
      "Rate : 50 [6400/49000 (13%)]\tLoss: 0.053245\t Accuracy:98.554%\n",
      "Rate : 50 [7040/49000 (14%)]\tLoss: 0.137031\t Accuracy:98.515%\n",
      "Rate : 50 [7680/49000 (16%)]\tLoss: 0.009895\t Accuracy:98.496%\n",
      "Rate : 50 [8320/49000 (17%)]\tLoss: 0.015547\t Accuracy:98.503%\n",
      "Rate : 50 [8960/49000 (18%)]\tLoss: 0.040738\t Accuracy:98.432%\n",
      "Rate : 50 [9600/49000 (20%)]\tLoss: 0.000595\t Accuracy:98.422%\n",
      "Rate : 50 [10240/49000 (21%)]\tLoss: 0.211112\t Accuracy:98.403%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate : 50 [10880/49000 (22%)]\tLoss: 0.046897\t Accuracy:98.405%\n",
      "Rate : 50 [11520/49000 (23%)]\tLoss: 0.002434\t Accuracy:98.373%\n",
      "Rate : 50 [12160/49000 (25%)]\tLoss: 0.049466\t Accuracy:98.368%\n",
      "Rate : 50 [12800/49000 (26%)]\tLoss: 0.076695\t Accuracy:98.356%\n",
      "Rate : 50 [13440/49000 (27%)]\tLoss: 0.001654\t Accuracy:98.360%\n",
      "Rate : 50 [14080/49000 (29%)]\tLoss: 0.041311\t Accuracy:98.342%\n",
      "Rate : 50 [14720/49000 (30%)]\tLoss: 0.038717\t Accuracy:98.366%\n",
      "Rate : 50 [15360/49000 (31%)]\tLoss: 0.003659\t Accuracy:98.389%\n",
      "Rate : 50 [16000/49000 (33%)]\tLoss: 0.005589\t Accuracy:98.378%\n",
      "Rate : 50 [16640/49000 (34%)]\tLoss: 0.047885\t Accuracy:98.399%\n",
      "Rate : 50 [17280/49000 (35%)]\tLoss: 0.004129\t Accuracy:98.423%\n",
      "Rate : 50 [17920/49000 (37%)]\tLoss: 0.008827\t Accuracy:98.407%\n",
      "Rate : 50 [18560/49000 (38%)]\tLoss: 0.012986\t Accuracy:98.424%\n",
      "Rate : 50 [19200/49000 (39%)]\tLoss: 0.285066\t Accuracy:98.414%\n",
      "Rate : 50 [19840/49000 (40%)]\tLoss: 0.035836\t Accuracy:98.390%\n",
      "Rate : 50 [20480/49000 (42%)]\tLoss: 0.026060\t Accuracy:98.401%\n",
      "Rate : 50 [21120/49000 (43%)]\tLoss: 0.140354\t Accuracy:98.407%\n",
      "Rate : 50 [21760/49000 (44%)]\tLoss: 0.354444\t Accuracy:98.421%\n",
      "Rate : 50 [22400/49000 (46%)]\tLoss: 0.176445\t Accuracy:98.426%\n",
      "Rate : 50 [23040/49000 (47%)]\tLoss: 0.009893\t Accuracy:98.405%\n",
      "Rate : 50 [23680/49000 (48%)]\tLoss: 0.002273\t Accuracy:98.397%\n",
      "Rate : 50 [24320/49000 (50%)]\tLoss: 0.006521\t Accuracy:98.419%\n",
      "Rate : 50 [24960/49000 (51%)]\tLoss: 0.044972\t Accuracy:98.444%\n",
      "Rate : 50 [25600/49000 (52%)]\tLoss: 0.000552\t Accuracy:98.447%\n",
      "Rate : 50 [26240/49000 (54%)]\tLoss: 0.002400\t Accuracy:98.470%\n",
      "Rate : 50 [26880/49000 (55%)]\tLoss: 0.000382\t Accuracy:98.477%\n",
      "Rate : 50 [27520/49000 (56%)]\tLoss: 0.000004\t Accuracy:98.490%\n",
      "Rate : 50 [28160/49000 (57%)]\tLoss: 0.146520\t Accuracy:98.510%\n",
      "Rate : 50 [28800/49000 (59%)]\tLoss: 0.003124\t Accuracy:98.512%\n",
      "Rate : 50 [29440/49000 (60%)]\tLoss: 0.015276\t Accuracy:98.517%\n",
      "Rate : 50 [30080/49000 (61%)]\tLoss: 0.015740\t Accuracy:98.522%\n",
      "Rate : 50 [30720/49000 (63%)]\tLoss: 0.000266\t Accuracy:98.533%\n",
      "Rate : 50 [31360/49000 (64%)]\tLoss: 0.031880\t Accuracy:98.541%\n",
      "Rate : 50 [32000/49000 (65%)]\tLoss: 0.000705\t Accuracy:98.548%\n",
      "Rate : 50 [32640/49000 (67%)]\tLoss: 0.014775\t Accuracy:98.552%\n",
      "Rate : 50 [33280/49000 (68%)]\tLoss: 0.000433\t Accuracy:98.553%\n",
      "Rate : 50 [33920/49000 (69%)]\tLoss: 0.091417\t Accuracy:98.551%\n",
      "Rate : 50 [34560/49000 (70%)]\tLoss: 0.009666\t Accuracy:98.537%\n",
      "Rate : 50 [35200/49000 (72%)]\tLoss: 0.001936\t Accuracy:98.555%\n",
      "Rate : 50 [35840/49000 (73%)]\tLoss: 0.004347\t Accuracy:98.570%\n",
      "Rate : 50 [36480/49000 (74%)]\tLoss: 0.000150\t Accuracy:98.584%\n",
      "Rate : 50 [37120/49000 (76%)]\tLoss: 0.001867\t Accuracy:98.590%\n",
      "Rate : 50 [37760/49000 (77%)]\tLoss: 0.151345\t Accuracy:98.590%\n",
      "Rate : 50 [38400/49000 (78%)]\tLoss: 0.160308\t Accuracy:98.598%\n",
      "Rate : 50 [39040/49000 (80%)]\tLoss: 0.001972\t Accuracy:98.597%\n",
      "Rate : 50 [39680/49000 (81%)]\tLoss: 0.000233\t Accuracy:98.613%\n",
      "Rate : 50 [40320/49000 (82%)]\tLoss: 0.339445\t Accuracy:98.617%\n",
      "Rate : 50 [40960/49000 (84%)]\tLoss: 0.018452\t Accuracy:98.627%\n",
      "Rate : 50 [41600/49000 (85%)]\tLoss: 0.000003\t Accuracy:98.636%\n",
      "Rate : 50 [42240/49000 (86%)]\tLoss: 0.000126\t Accuracy:98.649%\n",
      "Rate : 50 [42880/49000 (87%)]\tLoss: 0.005688\t Accuracy:98.658%\n",
      "Rate : 50 [43520/49000 (89%)]\tLoss: 0.000324\t Accuracy:98.666%\n",
      "Rate : 50 [44160/49000 (90%)]\tLoss: 0.000366\t Accuracy:98.669%\n",
      "Rate : 50 [44800/49000 (91%)]\tLoss: 0.019426\t Accuracy:98.671%\n",
      "Rate : 50 [45440/49000 (93%)]\tLoss: 0.046099\t Accuracy:98.678%\n",
      "Rate : 50 [46080/49000 (94%)]\tLoss: 0.007923\t Accuracy:98.684%\n",
      "Rate : 50 [46720/49000 (95%)]\tLoss: 0.001261\t Accuracy:98.693%\n",
      "Rate : 50 [47360/49000 (97%)]\tLoss: 0.000119\t Accuracy:98.700%\n",
      "Rate : 50 [48000/49000 (98%)]\tLoss: 0.136734\t Accuracy:98.705%\n",
      "Rate : 50 [48640/49000 (99%)]\tLoss: 0.131947\t Accuracy:98.708%\n",
      "Rate : 60 [0/49000 (0%)]\tLoss: 0.086322\t Accuracy:96.875%\n",
      "Rate : 60 [640/49000 (1%)]\tLoss: 0.056085\t Accuracy:98.065%\n",
      "Rate : 60 [1280/49000 (3%)]\tLoss: 0.030364\t Accuracy:97.866%\n",
      "Rate : 60 [1920/49000 (4%)]\tLoss: 0.055694\t Accuracy:97.848%\n",
      "Rate : 60 [2560/49000 (5%)]\tLoss: 0.011128\t Accuracy:97.994%\n",
      "Rate : 60 [3200/49000 (7%)]\tLoss: 0.014908\t Accuracy:98.113%\n",
      "Rate : 60 [3840/49000 (8%)]\tLoss: 0.000544\t Accuracy:98.244%\n",
      "Rate : 60 [4480/49000 (9%)]\tLoss: 0.006049\t Accuracy:98.227%\n",
      "Rate : 60 [5120/49000 (10%)]\tLoss: 0.163260\t Accuracy:98.234%\n",
      "Rate : 60 [5760/49000 (12%)]\tLoss: 0.078286\t Accuracy:98.325%\n",
      "Rate : 60 [6400/49000 (13%)]\tLoss: 0.035331\t Accuracy:98.352%\n",
      "Rate : 60 [7040/49000 (14%)]\tLoss: 0.041578\t Accuracy:98.416%\n",
      "Rate : 60 [7680/49000 (16%)]\tLoss: 0.000773\t Accuracy:98.509%\n",
      "Rate : 60 [8320/49000 (17%)]\tLoss: 0.132019\t Accuracy:98.479%\n",
      "Rate : 60 [8960/49000 (18%)]\tLoss: 0.107365\t Accuracy:98.521%\n",
      "Rate : 60 [9600/49000 (20%)]\tLoss: 0.000453\t Accuracy:98.547%\n",
      "Rate : 60 [10240/49000 (21%)]\tLoss: 0.000047\t Accuracy:98.579%\n",
      "Rate : 60 [10880/49000 (22%)]\tLoss: 0.012215\t Accuracy:98.580%\n",
      "Rate : 60 [11520/49000 (23%)]\tLoss: 0.015676\t Accuracy:98.589%\n",
      "Rate : 60 [12160/49000 (25%)]\tLoss: 0.063460\t Accuracy:98.638%\n",
      "Rate : 60 [12800/49000 (26%)]\tLoss: 0.139566\t Accuracy:98.628%\n",
      "Rate : 60 [13440/49000 (27%)]\tLoss: 0.000059\t Accuracy:98.649%\n",
      "Rate : 60 [14080/49000 (29%)]\tLoss: 0.038787\t Accuracy:98.661%\n",
      "Rate : 60 [14720/49000 (30%)]\tLoss: 0.045681\t Accuracy:98.685%\n",
      "Rate : 60 [15360/49000 (31%)]\tLoss: 0.000909\t Accuracy:98.707%\n",
      "Rate : 60 [16000/49000 (33%)]\tLoss: 0.012680\t Accuracy:98.721%\n",
      "Rate : 60 [16640/49000 (34%)]\tLoss: 0.054579\t Accuracy:98.728%\n",
      "Rate : 60 [17280/49000 (35%)]\tLoss: 0.087676\t Accuracy:98.729%\n",
      "Rate : 60 [17920/49000 (37%)]\tLoss: 0.010838\t Accuracy:98.736%\n",
      "Rate : 60 [18560/49000 (38%)]\tLoss: 0.000433\t Accuracy:98.758%\n",
      "Rate : 60 [19200/49000 (39%)]\tLoss: 0.131791\t Accuracy:98.768%\n",
      "Rate : 60 [19840/49000 (40%)]\tLoss: 0.019185\t Accuracy:98.767%\n",
      "Rate : 60 [20480/49000 (42%)]\tLoss: 0.003840\t Accuracy:98.767%\n",
      "Rate : 60 [21120/49000 (43%)]\tLoss: 0.142467\t Accuracy:98.766%\n",
      "Rate : 60 [21760/49000 (44%)]\tLoss: 0.063917\t Accuracy:98.752%\n",
      "Rate : 60 [22400/49000 (46%)]\tLoss: 0.045364\t Accuracy:98.774%\n",
      "Rate : 60 [23040/49000 (47%)]\tLoss: 0.016060\t Accuracy:98.752%\n",
      "Rate : 60 [23680/49000 (48%)]\tLoss: 0.003435\t Accuracy:98.752%\n",
      "Rate : 60 [24320/49000 (50%)]\tLoss: 0.000122\t Accuracy:98.752%\n",
      "Rate : 60 [24960/49000 (51%)]\tLoss: 0.127276\t Accuracy:98.752%\n",
      "Rate : 60 [25600/49000 (52%)]\tLoss: 0.000028\t Accuracy:98.736%\n",
      "Rate : 60 [26240/49000 (54%)]\tLoss: 0.019780\t Accuracy:98.732%\n",
      "Rate : 60 [26880/49000 (55%)]\tLoss: 0.000775\t Accuracy:98.733%\n",
      "Rate : 60 [27520/49000 (56%)]\tLoss: 0.000016\t Accuracy:98.741%\n",
      "Rate : 60 [28160/49000 (57%)]\tLoss: 0.080749\t Accuracy:98.744%\n",
      "Rate : 60 [28800/49000 (59%)]\tLoss: 0.003766\t Accuracy:98.741%\n",
      "Rate : 60 [29440/49000 (60%)]\tLoss: 0.183112\t Accuracy:98.734%\n",
      "Rate : 60 [30080/49000 (61%)]\tLoss: 0.213530\t Accuracy:98.731%\n",
      "Rate : 60 [30720/49000 (63%)]\tLoss: 0.000617\t Accuracy:98.725%\n",
      "Rate : 60 [31360/49000 (64%)]\tLoss: 0.149984\t Accuracy:98.742%\n",
      "Rate : 60 [32000/49000 (65%)]\tLoss: 0.000167\t Accuracy:98.761%\n",
      "Rate : 60 [32640/49000 (67%)]\tLoss: 0.004216\t Accuracy:98.763%\n",
      "Rate : 60 [33280/49000 (68%)]\tLoss: 0.000002\t Accuracy:98.751%\n",
      "Rate : 60 [33920/49000 (69%)]\tLoss: 0.003006\t Accuracy:98.751%\n",
      "Rate : 60 [34560/49000 (70%)]\tLoss: 0.021654\t Accuracy:98.745%\n",
      "Rate : 60 [35200/49000 (72%)]\tLoss: 0.003099\t Accuracy:98.745%\n",
      "Rate : 60 [35840/49000 (73%)]\tLoss: 0.013414\t Accuracy:98.754%\n",
      "Rate : 60 [36480/49000 (74%)]\tLoss: 0.008030\t Accuracy:98.759%\n",
      "Rate : 60 [37120/49000 (76%)]\tLoss: 0.001906\t Accuracy:98.765%\n",
      "Rate : 60 [37760/49000 (77%)]\tLoss: 0.003255\t Accuracy:98.764%\n",
      "Rate : 60 [38400/49000 (78%)]\tLoss: 0.008544\t Accuracy:98.780%\n",
      "Rate : 60 [39040/49000 (80%)]\tLoss: 0.005935\t Accuracy:98.777%\n",
      "Rate : 60 [39680/49000 (81%)]\tLoss: 0.000310\t Accuracy:98.781%\n",
      "Rate : 60 [40320/49000 (82%)]\tLoss: 0.228636\t Accuracy:98.791%\n",
      "Rate : 60 [40960/49000 (84%)]\tLoss: 0.004601\t Accuracy:98.792%\n",
      "Rate : 60 [41600/49000 (85%)]\tLoss: 0.000169\t Accuracy:98.804%\n",
      "Rate : 60 [42240/49000 (86%)]\tLoss: 0.000048\t Accuracy:98.798%\n",
      "Rate : 60 [42880/49000 (87%)]\tLoss: 0.176808\t Accuracy:98.795%\n",
      "Rate : 60 [43520/49000 (89%)]\tLoss: 0.000357\t Accuracy:98.797%\n",
      "Rate : 60 [44160/49000 (90%)]\tLoss: 0.001052\t Accuracy:98.794%\n",
      "Rate : 60 [44800/49000 (91%)]\tLoss: 0.004039\t Accuracy:98.791%\n",
      "Rate : 60 [45440/49000 (93%)]\tLoss: 0.001009\t Accuracy:98.795%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate : 60 [46080/49000 (94%)]\tLoss: 0.000977\t Accuracy:98.801%\n",
      "Rate : 60 [46720/49000 (95%)]\tLoss: 0.001470\t Accuracy:98.811%\n",
      "Rate : 60 [47360/49000 (97%)]\tLoss: 0.001485\t Accuracy:98.816%\n",
      "Rate : 60 [48000/49000 (98%)]\tLoss: 0.253610\t Accuracy:98.811%\n",
      "Rate : 60 [48640/49000 (99%)]\tLoss: 0.020339\t Accuracy:98.812%\n",
      "Rate : 70 [0/49000 (0%)]\tLoss: 0.026265\t Accuracy:100.000%\n",
      "Rate : 70 [640/49000 (1%)]\tLoss: 0.030428\t Accuracy:97.024%\n",
      "Rate : 70 [1280/49000 (3%)]\tLoss: 0.436806\t Accuracy:97.637%\n",
      "Rate : 70 [1920/49000 (4%)]\tLoss: 0.000358\t Accuracy:97.695%\n",
      "Rate : 70 [2560/49000 (5%)]\tLoss: 0.004122\t Accuracy:97.955%\n",
      "Rate : 70 [3200/49000 (7%)]\tLoss: 0.023337\t Accuracy:98.113%\n",
      "Rate : 70 [3840/49000 (8%)]\tLoss: 0.002239\t Accuracy:98.037%\n",
      "Rate : 70 [4480/49000 (9%)]\tLoss: 0.001476\t Accuracy:97.961%\n",
      "Rate : 70 [5120/49000 (10%)]\tLoss: 0.017487\t Accuracy:97.962%\n",
      "Rate : 70 [5760/49000 (12%)]\tLoss: 0.015835\t Accuracy:98.049%\n",
      "Rate : 70 [6400/49000 (13%)]\tLoss: 0.027883\t Accuracy:98.119%\n",
      "Rate : 70 [7040/49000 (14%)]\tLoss: 0.000468\t Accuracy:98.190%\n",
      "Rate : 70 [7680/49000 (16%)]\tLoss: 0.021971\t Accuracy:98.249%\n",
      "Rate : 70 [8320/49000 (17%)]\tLoss: 0.156131\t Accuracy:98.192%\n",
      "Rate : 70 [8960/49000 (18%)]\tLoss: 0.008158\t Accuracy:98.232%\n",
      "Rate : 70 [9600/49000 (20%)]\tLoss: 0.001478\t Accuracy:98.318%\n",
      "Rate : 70 [10240/49000 (21%)]\tLoss: 0.008445\t Accuracy:98.355%\n",
      "Rate : 70 [10880/49000 (22%)]\tLoss: 0.069245\t Accuracy:98.360%\n",
      "Rate : 70 [11520/49000 (23%)]\tLoss: 0.052899\t Accuracy:98.355%\n",
      "Rate : 70 [12160/49000 (25%)]\tLoss: 0.134614\t Accuracy:98.384%\n",
      "Rate : 70 [12800/49000 (26%)]\tLoss: 0.101203\t Accuracy:98.387%\n",
      "Rate : 70 [13440/49000 (27%)]\tLoss: 0.006618\t Accuracy:98.367%\n",
      "Rate : 70 [14080/49000 (29%)]\tLoss: 0.063100\t Accuracy:98.349%\n",
      "Rate : 70 [14720/49000 (30%)]\tLoss: 0.191805\t Accuracy:98.346%\n",
      "Rate : 70 [15360/49000 (31%)]\tLoss: 0.000469\t Accuracy:98.363%\n",
      "Rate : 70 [16000/49000 (33%)]\tLoss: 0.007405\t Accuracy:98.378%\n",
      "Rate : 70 [16640/49000 (34%)]\tLoss: 0.051785\t Accuracy:98.381%\n",
      "Rate : 70 [17280/49000 (35%)]\tLoss: 0.002867\t Accuracy:98.394%\n",
      "Rate : 70 [17920/49000 (37%)]\tLoss: 0.009408\t Accuracy:98.407%\n",
      "Rate : 70 [18560/49000 (38%)]\tLoss: 0.005952\t Accuracy:98.419%\n",
      "Rate : 70 [19200/49000 (39%)]\tLoss: 0.964427\t Accuracy:98.414%\n",
      "Rate : 70 [19840/49000 (40%)]\tLoss: 0.014456\t Accuracy:98.420%\n",
      "Rate : 70 [20480/49000 (42%)]\tLoss: 0.015339\t Accuracy:98.420%\n",
      "Rate : 70 [21120/49000 (43%)]\tLoss: 0.007672\t Accuracy:98.430%\n",
      "Rate : 70 [21760/49000 (44%)]\tLoss: 0.025178\t Accuracy:98.435%\n",
      "Rate : 70 [22400/49000 (46%)]\tLoss: 0.254579\t Accuracy:98.426%\n",
      "Rate : 70 [23040/49000 (47%)]\tLoss: 0.003148\t Accuracy:98.418%\n",
      "Rate : 70 [23680/49000 (48%)]\tLoss: 0.001349\t Accuracy:98.435%\n",
      "Rate : 70 [24320/49000 (50%)]\tLoss: 0.000371\t Accuracy:98.460%\n",
      "Rate : 70 [24960/49000 (51%)]\tLoss: 0.059956\t Accuracy:98.468%\n",
      "Rate : 70 [25600/49000 (52%)]\tLoss: 0.000736\t Accuracy:98.455%\n",
      "Rate : 70 [26240/49000 (54%)]\tLoss: 0.019910\t Accuracy:98.470%\n",
      "Rate : 70 [26880/49000 (55%)]\tLoss: 0.002659\t Accuracy:98.477%\n",
      "Rate : 70 [27520/49000 (56%)]\tLoss: 0.000459\t Accuracy:98.490%\n",
      "Rate : 70 [28160/49000 (57%)]\tLoss: 0.036436\t Accuracy:98.514%\n",
      "Rate : 70 [28800/49000 (59%)]\tLoss: 0.012623\t Accuracy:98.533%\n",
      "Rate : 70 [29440/49000 (60%)]\tLoss: 0.051984\t Accuracy:98.544%\n",
      "Rate : 70 [30080/49000 (61%)]\tLoss: 0.002094\t Accuracy:98.552%\n",
      "Rate : 70 [30720/49000 (63%)]\tLoss: 0.000449\t Accuracy:98.556%\n",
      "Rate : 70 [31360/49000 (64%)]\tLoss: 0.281565\t Accuracy:98.563%\n",
      "Rate : 70 [32000/49000 (65%)]\tLoss: 0.001159\t Accuracy:98.576%\n",
      "Rate : 70 [32640/49000 (67%)]\tLoss: 0.030906\t Accuracy:98.589%\n",
      "Rate : 70 [33280/49000 (68%)]\tLoss: 0.000126\t Accuracy:98.574%\n",
      "Rate : 70 [33920/49000 (69%)]\tLoss: 0.001537\t Accuracy:98.574%\n",
      "Rate : 70 [34560/49000 (70%)]\tLoss: 0.005324\t Accuracy:98.566%\n",
      "Rate : 70 [35200/49000 (72%)]\tLoss: 0.001405\t Accuracy:98.581%\n",
      "Rate : 70 [35840/49000 (73%)]\tLoss: 0.051456\t Accuracy:98.587%\n",
      "Rate : 70 [36480/49000 (74%)]\tLoss: 0.001610\t Accuracy:98.590%\n",
      "Rate : 70 [37120/49000 (76%)]\tLoss: 0.002870\t Accuracy:98.606%\n",
      "Rate : 70 [37760/49000 (77%)]\tLoss: 0.000101\t Accuracy:98.621%\n",
      "Rate : 70 [38400/49000 (78%)]\tLoss: 0.015402\t Accuracy:98.642%\n",
      "Rate : 70 [39040/49000 (80%)]\tLoss: 0.045228\t Accuracy:98.644%\n",
      "Rate : 70 [39680/49000 (81%)]\tLoss: 0.000059\t Accuracy:98.663%\n",
      "Rate : 70 [40320/49000 (82%)]\tLoss: 0.088490\t Accuracy:98.669%\n",
      "Rate : 70 [40960/49000 (84%)]\tLoss: 0.086058\t Accuracy:98.670%\n",
      "Rate : 70 [41600/49000 (85%)]\tLoss: 0.000705\t Accuracy:98.679%\n",
      "Rate : 70 [42240/49000 (86%)]\tLoss: 0.001309\t Accuracy:98.682%\n",
      "Rate : 70 [42880/49000 (87%)]\tLoss: 0.014730\t Accuracy:98.695%\n",
      "Rate : 70 [43520/49000 (89%)]\tLoss: 0.000393\t Accuracy:98.705%\n",
      "Rate : 70 [44160/49000 (90%)]\tLoss: 0.041179\t Accuracy:98.712%\n",
      "Rate : 70 [44800/49000 (91%)]\tLoss: 0.012261\t Accuracy:98.720%\n",
      "Rate : 70 [45440/49000 (93%)]\tLoss: 0.006397\t Accuracy:98.718%\n",
      "Rate : 70 [46080/49000 (94%)]\tLoss: 0.003696\t Accuracy:98.729%\n",
      "Rate : 70 [46720/49000 (95%)]\tLoss: 0.001465\t Accuracy:98.740%\n",
      "Rate : 70 [47360/49000 (97%)]\tLoss: 0.000283\t Accuracy:98.749%\n",
      "Rate : 70 [48000/49000 (98%)]\tLoss: 0.170945\t Accuracy:98.755%\n",
      "Rate : 70 [48640/49000 (99%)]\tLoss: 0.061882\t Accuracy:98.759%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "pruned_model = smart_prune(acmlp, train_loader, 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.973% \n"
     ]
    }
   ],
   "source": [
    "evaluate(pruned_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На моем компьютере получилось качество около 0.97. Формально это даже чуточку лучше, чем оригинальная модель! Получается, что лишние веса в оригинальной модели могли мешали выявить зависимость в данных. \n",
    "\n",
    "Давайте посчитаем количество ненулевых весов в модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222000"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_pruned_weights(acmlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66600"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_pruned_weights(pruned_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оставили около 60 000 весов, мы получили почти такое же качество для модели!\n",
    "\n",
    "Можем ли мы таким же образом выкинуть 90%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate : 0 [0/49000 (0%)]\tLoss: 0.031662\t Accuracy:100.000%\n",
      "Rate : 0 [640/49000 (1%)]\tLoss: 0.131079\t Accuracy:96.726%\n",
      "Rate : 0 [1280/49000 (3%)]\tLoss: 0.260266\t Accuracy:96.494%\n",
      "Rate : 0 [1920/49000 (4%)]\tLoss: 0.021289\t Accuracy:96.516%\n",
      "Rate : 0 [2560/49000 (5%)]\tLoss: 0.220054\t Accuracy:96.489%\n",
      "Rate : 0 [3200/49000 (7%)]\tLoss: 0.141963\t Accuracy:96.844%\n",
      "Rate : 0 [3840/49000 (8%)]\tLoss: 0.002043\t Accuracy:96.823%\n",
      "Rate : 0 [4480/49000 (9%)]\tLoss: 0.401672\t Accuracy:96.764%\n",
      "Rate : 0 [5120/49000 (10%)]\tLoss: 0.790331\t Accuracy:96.894%\n",
      "Rate : 0 [5760/49000 (12%)]\tLoss: 0.257273\t Accuracy:97.065%\n",
      "Rate : 0 [6400/49000 (13%)]\tLoss: 0.094841\t Accuracy:96.999%\n",
      "Rate : 0 [7040/49000 (14%)]\tLoss: 0.193954\t Accuracy:97.002%\n",
      "Rate : 0 [7680/49000 (16%)]\tLoss: 0.311790\t Accuracy:96.940%\n",
      "Rate : 0 [8320/49000 (17%)]\tLoss: 0.260763\t Accuracy:96.887%\n",
      "Rate : 0 [8960/49000 (18%)]\tLoss: 0.022661\t Accuracy:96.875%\n",
      "Rate : 0 [9600/49000 (20%)]\tLoss: 0.021327\t Accuracy:96.865%\n",
      "Rate : 0 [10240/49000 (21%)]\tLoss: 0.099176\t Accuracy:96.885%\n",
      "Rate : 0 [10880/49000 (22%)]\tLoss: 0.380611\t Accuracy:96.912%\n",
      "Rate : 0 [11520/49000 (23%)]\tLoss: 0.088750\t Accuracy:96.910%\n",
      "Rate : 0 [12160/49000 (25%)]\tLoss: 0.140996\t Accuracy:96.932%\n",
      "Rate : 0 [12800/49000 (26%)]\tLoss: 0.125966\t Accuracy:96.914%\n",
      "Rate : 0 [13440/49000 (27%)]\tLoss: 0.201027\t Accuracy:96.920%\n",
      "Rate : 0 [14080/49000 (29%)]\tLoss: 0.057506\t Accuracy:96.896%\n",
      "Rate : 0 [14720/49000 (30%)]\tLoss: 0.145320\t Accuracy:96.909%\n",
      "Rate : 0 [15360/49000 (31%)]\tLoss: 0.045877\t Accuracy:96.946%\n",
      "Rate : 0 [16000/49000 (33%)]\tLoss: 0.081683\t Accuracy:96.900%\n",
      "Rate : 0 [16640/49000 (34%)]\tLoss: 0.108142\t Accuracy:96.923%\n",
      "Rate : 0 [17280/49000 (35%)]\tLoss: 0.173778\t Accuracy:96.962%\n",
      "Rate : 0 [17920/49000 (37%)]\tLoss: 0.008849\t Accuracy:96.959%\n",
      "Rate : 0 [18560/49000 (38%)]\tLoss: 0.102910\t Accuracy:96.977%\n",
      "Rate : 0 [19200/49000 (39%)]\tLoss: 0.630267\t Accuracy:96.969%\n",
      "Rate : 0 [19840/49000 (40%)]\tLoss: 0.120984\t Accuracy:96.910%\n",
      "Rate : 0 [20480/49000 (42%)]\tLoss: 0.020432\t Accuracy:96.885%\n",
      "Rate : 0 [21120/49000 (43%)]\tLoss: 0.024543\t Accuracy:96.899%\n",
      "Rate : 0 [21760/49000 (44%)]\tLoss: 0.092338\t Accuracy:96.884%\n",
      "Rate : 0 [22400/49000 (46%)]\tLoss: 0.281558\t Accuracy:96.857%\n",
      "Rate : 0 [23040/49000 (47%)]\tLoss: 0.497485\t Accuracy:96.832%\n",
      "Rate : 0 [23680/49000 (48%)]\tLoss: 0.025263\t Accuracy:96.808%\n",
      "Rate : 0 [24320/49000 (50%)]\tLoss: 0.055038\t Accuracy:96.809%\n",
      "Rate : 0 [24960/49000 (51%)]\tLoss: 0.033286\t Accuracy:96.807%\n",
      "Rate : 0 [25600/49000 (52%)]\tLoss: 0.027881\t Accuracy:96.762%\n",
      "Rate : 0 [26240/49000 (54%)]\tLoss: 0.114512\t Accuracy:96.749%\n",
      "Rate : 0 [26880/49000 (55%)]\tLoss: 0.011880\t Accuracy:96.756%\n",
      "Rate : 0 [27520/49000 (56%)]\tLoss: 0.002571\t Accuracy:96.755%\n",
      "Rate : 0 [28160/49000 (57%)]\tLoss: 0.099591\t Accuracy:96.790%\n",
      "Rate : 0 [28800/49000 (59%)]\tLoss: 0.171728\t Accuracy:96.799%\n",
      "Rate : 0 [29440/49000 (60%)]\tLoss: 0.106021\t Accuracy:96.797%\n",
      "Rate : 0 [30080/49000 (61%)]\tLoss: 0.066594\t Accuracy:96.795%\n",
      "Rate : 0 [30720/49000 (63%)]\tLoss: 0.000095\t Accuracy:96.794%\n",
      "Rate : 0 [31360/49000 (64%)]\tLoss: 0.075751\t Accuracy:96.805%\n",
      "Rate : 0 [32000/49000 (65%)]\tLoss: 0.125015\t Accuracy:96.809%\n",
      "Rate : 0 [32640/49000 (67%)]\tLoss: 0.114442\t Accuracy:96.814%\n",
      "Rate : 0 [33280/49000 (68%)]\tLoss: 0.000505\t Accuracy:96.812%\n",
      "Rate : 0 [33920/49000 (69%)]\tLoss: 0.026177\t Accuracy:96.813%\n",
      "Rate : 0 [34560/49000 (70%)]\tLoss: 0.073853\t Accuracy:96.811%\n",
      "Rate : 0 [35200/49000 (72%)]\tLoss: 0.063447\t Accuracy:96.813%\n",
      "Rate : 0 [35840/49000 (73%)]\tLoss: 0.001290\t Accuracy:96.814%\n",
      "Rate : 0 [36480/49000 (74%)]\tLoss: 0.062354\t Accuracy:96.823%\n",
      "Rate : 0 [37120/49000 (76%)]\tLoss: 0.079900\t Accuracy:96.827%\n",
      "Rate : 0 [37760/49000 (77%)]\tLoss: 0.074925\t Accuracy:96.830%\n",
      "Rate : 0 [38400/49000 (78%)]\tLoss: 0.004848\t Accuracy:96.849%\n",
      "Rate : 0 [39040/49000 (80%)]\tLoss: 0.029781\t Accuracy:96.862%\n",
      "Rate : 0 [39680/49000 (81%)]\tLoss: 0.071446\t Accuracy:96.855%\n",
      "Rate : 0 [40320/49000 (82%)]\tLoss: 0.039463\t Accuracy:96.868%\n",
      "Rate : 0 [40960/49000 (84%)]\tLoss: 0.118597\t Accuracy:96.868%\n",
      "Rate : 0 [41600/49000 (85%)]\tLoss: 0.178873\t Accuracy:96.880%\n",
      "Rate : 0 [42240/49000 (86%)]\tLoss: 0.000623\t Accuracy:96.894%\n",
      "Rate : 0 [42880/49000 (87%)]\tLoss: 0.086130\t Accuracy:96.894%\n",
      "Rate : 0 [43520/49000 (89%)]\tLoss: 0.080527\t Accuracy:96.898%\n",
      "Rate : 0 [44160/49000 (90%)]\tLoss: 0.239529\t Accuracy:96.893%\n",
      "Rate : 0 [44800/49000 (91%)]\tLoss: 0.193887\t Accuracy:96.879%\n",
      "Rate : 0 [45440/49000 (93%)]\tLoss: 0.055580\t Accuracy:96.884%\n",
      "Rate : 0 [46080/49000 (94%)]\tLoss: 0.020557\t Accuracy:96.886%\n",
      "Rate : 0 [46720/49000 (95%)]\tLoss: 0.011671\t Accuracy:96.905%\n",
      "Rate : 0 [47360/49000 (97%)]\tLoss: 0.007573\t Accuracy:96.913%\n",
      "Rate : 0 [48000/49000 (98%)]\tLoss: 0.012418\t Accuracy:96.910%\n",
      "Rate : 0 [48640/49000 (99%)]\tLoss: 0.186314\t Accuracy:96.908%\n",
      "Rate : 10 [0/49000 (0%)]\tLoss: 0.130172\t Accuracy:96.875%\n",
      "Rate : 10 [640/49000 (1%)]\tLoss: 0.070500\t Accuracy:97.173%\n",
      "Rate : 10 [1280/49000 (3%)]\tLoss: 0.045693\t Accuracy:97.485%\n",
      "Rate : 10 [1920/49000 (4%)]\tLoss: 0.046309\t Accuracy:97.541%\n",
      "Rate : 10 [2560/49000 (5%)]\tLoss: 0.016294\t Accuracy:97.840%\n",
      "Rate : 10 [3200/49000 (7%)]\tLoss: 0.018504\t Accuracy:98.020%\n",
      "Rate : 10 [3840/49000 (8%)]\tLoss: 0.081994\t Accuracy:97.856%\n",
      "Rate : 10 [4480/49000 (9%)]\tLoss: 0.050444\t Accuracy:97.739%\n",
      "Rate : 10 [5120/49000 (10%)]\tLoss: 0.013676\t Accuracy:97.729%\n",
      "Rate : 10 [5760/49000 (12%)]\tLoss: 0.057164\t Accuracy:97.756%\n",
      "Rate : 10 [6400/49000 (13%)]\tLoss: 0.150604\t Accuracy:97.761%\n",
      "Rate : 10 [7040/49000 (14%)]\tLoss: 0.054256\t Accuracy:97.837%\n",
      "Rate : 10 [7680/49000 (16%)]\tLoss: 0.104461\t Accuracy:97.796%\n",
      "Rate : 10 [8320/49000 (17%)]\tLoss: 0.190020\t Accuracy:97.797%\n",
      "Rate : 10 [8960/49000 (18%)]\tLoss: 0.048253\t Accuracy:97.742%\n",
      "Rate : 10 [9600/49000 (20%)]\tLoss: 0.000412\t Accuracy:97.737%\n",
      "Rate : 10 [10240/49000 (21%)]\tLoss: 0.026874\t Accuracy:97.771%\n",
      "Rate : 10 [10880/49000 (22%)]\tLoss: 0.155573\t Accuracy:97.764%\n",
      "Rate : 10 [11520/49000 (23%)]\tLoss: 0.034213\t Accuracy:97.741%\n",
      "Rate : 10 [12160/49000 (25%)]\tLoss: 0.152737\t Accuracy:97.720%\n",
      "Rate : 10 [12800/49000 (26%)]\tLoss: 0.171847\t Accuracy:97.693%\n",
      "Rate : 10 [13440/49000 (27%)]\tLoss: 0.195356\t Accuracy:97.714%\n",
      "Rate : 10 [14080/49000 (29%)]\tLoss: 0.201085\t Accuracy:97.718%\n",
      "Rate : 10 [14720/49000 (30%)]\tLoss: 0.116481\t Accuracy:97.729%\n",
      "Rate : 10 [15360/49000 (31%)]\tLoss: 0.003904\t Accuracy:97.694%\n",
      "Rate : 10 [16000/49000 (33%)]\tLoss: 0.078976\t Accuracy:97.680%\n",
      "Rate : 10 [16640/49000 (34%)]\tLoss: 0.099231\t Accuracy:97.685%\n",
      "Rate : 10 [17280/49000 (35%)]\tLoss: 0.013693\t Accuracy:97.684%\n",
      "Rate : 10 [17920/49000 (37%)]\tLoss: 0.009308\t Accuracy:97.683%\n",
      "Rate : 10 [18560/49000 (38%)]\tLoss: 0.015774\t Accuracy:97.660%\n",
      "Rate : 10 [19200/49000 (39%)]\tLoss: 0.228800\t Accuracy:97.639%\n",
      "Rate : 10 [19840/49000 (40%)]\tLoss: 0.126839\t Accuracy:97.620%\n",
      "Rate : 10 [20480/49000 (42%)]\tLoss: 0.191691\t Accuracy:97.621%\n",
      "Rate : 10 [21120/49000 (43%)]\tLoss: 0.218321\t Accuracy:97.631%\n",
      "Rate : 10 [21760/49000 (44%)]\tLoss: 0.083452\t Accuracy:97.614%\n",
      "Rate : 10 [22400/49000 (46%)]\tLoss: 0.197259\t Accuracy:97.602%\n",
      "Rate : 10 [23040/49000 (47%)]\tLoss: 0.365309\t Accuracy:97.581%\n",
      "Rate : 10 [23680/49000 (48%)]\tLoss: 0.000575\t Accuracy:97.575%\n",
      "Rate : 10 [24320/49000 (50%)]\tLoss: 0.007291\t Accuracy:97.573%\n",
      "Rate : 10 [24960/49000 (51%)]\tLoss: 0.045943\t Accuracy:97.587%\n",
      "Rate : 10 [25600/49000 (52%)]\tLoss: 0.002280\t Accuracy:97.585%\n",
      "Rate : 10 [26240/49000 (54%)]\tLoss: 0.010286\t Accuracy:97.613%\n",
      "Rate : 10 [26880/49000 (55%)]\tLoss: 0.000270\t Accuracy:97.614%\n",
      "Rate : 10 [27520/49000 (56%)]\tLoss: 0.005613\t Accuracy:97.623%\n",
      "Rate : 10 [28160/49000 (57%)]\tLoss: 0.316800\t Accuracy:97.623%\n",
      "Rate : 10 [28800/49000 (59%)]\tLoss: 0.039651\t Accuracy:97.638%\n",
      "Rate : 10 [29440/49000 (60%)]\tLoss: 0.013408\t Accuracy:97.625%\n",
      "Rate : 10 [30080/49000 (61%)]\tLoss: 0.019781\t Accuracy:97.619%\n",
      "Rate : 10 [30720/49000 (63%)]\tLoss: 0.001216\t Accuracy:97.629%\n",
      "Rate : 10 [31360/49000 (64%)]\tLoss: 0.286940\t Accuracy:97.640%\n",
      "Rate : 10 [32000/49000 (65%)]\tLoss: 0.012899\t Accuracy:97.655%\n",
      "Rate : 10 [32640/49000 (67%)]\tLoss: 0.017369\t Accuracy:97.652%\n",
      "Rate : 10 [33280/49000 (68%)]\tLoss: 0.011355\t Accuracy:97.659%\n",
      "Rate : 10 [33920/49000 (69%)]\tLoss: 0.010502\t Accuracy:97.647%\n",
      "Rate : 10 [34560/49000 (70%)]\tLoss: 0.614559\t Accuracy:97.647%\n",
      "Rate : 10 [35200/49000 (72%)]\tLoss: 0.172922\t Accuracy:97.630%\n",
      "Rate : 10 [35840/49000 (73%)]\tLoss: 0.028295\t Accuracy:97.642%\n",
      "Rate : 10 [36480/49000 (74%)]\tLoss: 0.004890\t Accuracy:97.636%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate : 10 [37120/49000 (76%)]\tLoss: 0.004724\t Accuracy:97.648%\n",
      "Rate : 10 [37760/49000 (77%)]\tLoss: 0.010148\t Accuracy:97.664%\n",
      "Rate : 10 [38400/49000 (78%)]\tLoss: 0.180426\t Accuracy:97.674%\n",
      "Rate : 10 [39040/49000 (80%)]\tLoss: 0.026186\t Accuracy:97.676%\n",
      "Rate : 10 [39680/49000 (81%)]\tLoss: 0.006909\t Accuracy:97.681%\n",
      "Rate : 10 [40320/49000 (82%)]\tLoss: 0.015055\t Accuracy:97.703%\n",
      "Rate : 10 [40960/49000 (84%)]\tLoss: 0.019710\t Accuracy:97.704%\n",
      "Rate : 10 [41600/49000 (85%)]\tLoss: 0.130606\t Accuracy:97.713%\n",
      "Rate : 10 [42240/49000 (86%)]\tLoss: 0.029194\t Accuracy:97.708%\n",
      "Rate : 10 [42880/49000 (87%)]\tLoss: 0.044540\t Accuracy:97.695%\n",
      "Rate : 10 [43520/49000 (89%)]\tLoss: 0.014154\t Accuracy:97.699%\n",
      "Rate : 10 [44160/49000 (90%)]\tLoss: 0.025861\t Accuracy:97.694%\n",
      "Rate : 10 [44800/49000 (91%)]\tLoss: 0.049685\t Accuracy:97.689%\n",
      "Rate : 10 [45440/49000 (93%)]\tLoss: 0.088772\t Accuracy:97.689%\n",
      "Rate : 10 [46080/49000 (94%)]\tLoss: 0.094827\t Accuracy:97.688%\n",
      "Rate : 10 [46720/49000 (95%)]\tLoss: 0.048499\t Accuracy:97.690%\n",
      "Rate : 10 [47360/49000 (97%)]\tLoss: 0.005357\t Accuracy:97.698%\n",
      "Rate : 10 [48000/49000 (98%)]\tLoss: 0.239996\t Accuracy:97.689%\n",
      "Rate : 10 [48640/49000 (99%)]\tLoss: 0.266902\t Accuracy:97.682%\n",
      "Rate : 20 [0/49000 (0%)]\tLoss: 0.018176\t Accuracy:100.000%\n",
      "Rate : 20 [640/49000 (1%)]\tLoss: 0.007665\t Accuracy:97.619%\n",
      "Rate : 20 [1280/49000 (3%)]\tLoss: 0.045438\t Accuracy:97.637%\n",
      "Rate : 20 [1920/49000 (4%)]\tLoss: 0.016563\t Accuracy:97.643%\n",
      "Rate : 20 [2560/49000 (5%)]\tLoss: 0.041265\t Accuracy:97.569%\n",
      "Rate : 20 [3200/49000 (7%)]\tLoss: 0.105213\t Accuracy:97.679%\n",
      "Rate : 20 [3840/49000 (8%)]\tLoss: 0.034302\t Accuracy:97.676%\n",
      "Rate : 20 [4480/49000 (9%)]\tLoss: 0.006492\t Accuracy:97.784%\n",
      "Rate : 20 [5120/49000 (10%)]\tLoss: 0.133934\t Accuracy:97.768%\n",
      "Rate : 20 [5760/49000 (12%)]\tLoss: 0.019055\t Accuracy:97.807%\n",
      "Rate : 20 [6400/49000 (13%)]\tLoss: 0.008506\t Accuracy:97.808%\n",
      "Rate : 20 [7040/49000 (14%)]\tLoss: 0.015899\t Accuracy:97.822%\n",
      "Rate : 20 [7680/49000 (16%)]\tLoss: 0.047409\t Accuracy:97.705%\n",
      "Rate : 20 [8320/49000 (17%)]\tLoss: 0.136833\t Accuracy:97.641%\n",
      "Rate : 20 [8960/49000 (18%)]\tLoss: 0.063089\t Accuracy:97.687%\n",
      "Rate : 20 [9600/49000 (20%)]\tLoss: 0.122201\t Accuracy:97.726%\n",
      "Rate : 20 [10240/49000 (21%)]\tLoss: 0.053441\t Accuracy:97.751%\n",
      "Rate : 20 [10880/49000 (22%)]\tLoss: 0.244249\t Accuracy:97.718%\n",
      "Rate : 20 [11520/49000 (23%)]\tLoss: 0.001550\t Accuracy:97.732%\n",
      "Rate : 20 [12160/49000 (25%)]\tLoss: 0.073823\t Accuracy:97.744%\n",
      "Rate : 20 [12800/49000 (26%)]\tLoss: 0.138365\t Accuracy:97.763%\n",
      "Rate : 20 [13440/49000 (27%)]\tLoss: 0.047745\t Accuracy:97.773%\n",
      "Rate : 20 [14080/49000 (29%)]\tLoss: 0.306271\t Accuracy:97.747%\n",
      "Rate : 20 [14720/49000 (30%)]\tLoss: 0.177596\t Accuracy:97.777%\n",
      "Rate : 20 [15360/49000 (31%)]\tLoss: 0.076251\t Accuracy:97.791%\n",
      "Rate : 20 [16000/49000 (33%)]\tLoss: 0.016088\t Accuracy:97.792%\n",
      "Rate : 20 [16640/49000 (34%)]\tLoss: 0.164912\t Accuracy:97.811%\n",
      "Rate : 20 [17280/49000 (35%)]\tLoss: 0.112924\t Accuracy:97.840%\n",
      "Rate : 20 [17920/49000 (37%)]\tLoss: 0.046814\t Accuracy:97.822%\n",
      "Rate : 20 [18560/49000 (38%)]\tLoss: 0.001347\t Accuracy:97.822%\n",
      "Rate : 20 [19200/49000 (39%)]\tLoss: 0.229921\t Accuracy:97.816%\n",
      "Rate : 20 [19840/49000 (40%)]\tLoss: 0.048843\t Accuracy:97.831%\n",
      "Rate : 20 [20480/49000 (42%)]\tLoss: 0.008912\t Accuracy:97.850%\n",
      "Rate : 20 [21120/49000 (43%)]\tLoss: 0.475268\t Accuracy:97.844%\n",
      "Rate : 20 [21760/49000 (44%)]\tLoss: 0.676076\t Accuracy:97.816%\n",
      "Rate : 20 [22400/49000 (46%)]\tLoss: 0.174200\t Accuracy:97.807%\n",
      "Rate : 20 [23040/49000 (47%)]\tLoss: 0.414640\t Accuracy:97.790%\n",
      "Rate : 20 [23680/49000 (48%)]\tLoss: 0.008077\t Accuracy:97.794%\n",
      "Rate : 20 [24320/49000 (50%)]\tLoss: 0.189149\t Accuracy:97.803%\n",
      "Rate : 20 [24960/49000 (51%)]\tLoss: 0.007701\t Accuracy:97.819%\n",
      "Rate : 20 [25600/49000 (52%)]\tLoss: 0.000258\t Accuracy:97.804%\n",
      "Rate : 20 [26240/49000 (54%)]\tLoss: 0.003974\t Accuracy:97.804%\n",
      "Rate : 20 [26880/49000 (55%)]\tLoss: 0.001206\t Accuracy:97.800%\n",
      "Rate : 20 [27520/49000 (56%)]\tLoss: 0.031200\t Accuracy:97.811%\n",
      "Rate : 20 [28160/49000 (57%)]\tLoss: 0.082282\t Accuracy:97.833%\n",
      "Rate : 20 [28800/49000 (59%)]\tLoss: 0.029022\t Accuracy:97.829%\n",
      "Rate : 20 [29440/49000 (60%)]\tLoss: 0.017293\t Accuracy:97.832%\n",
      "Rate : 20 [30080/49000 (61%)]\tLoss: 0.006983\t Accuracy:97.835%\n",
      "Rate : 20 [30720/49000 (63%)]\tLoss: 0.000129\t Accuracy:97.857%\n",
      "Rate : 20 [31360/49000 (64%)]\tLoss: 0.046053\t Accuracy:97.863%\n",
      "Rate : 20 [32000/49000 (65%)]\tLoss: 0.034517\t Accuracy:97.880%\n",
      "Rate : 20 [32640/49000 (67%)]\tLoss: 0.083302\t Accuracy:97.876%\n",
      "Rate : 20 [33280/49000 (68%)]\tLoss: 0.000935\t Accuracy:97.872%\n",
      "Rate : 20 [33920/49000 (69%)]\tLoss: 0.004944\t Accuracy:97.868%\n",
      "Rate : 20 [34560/49000 (70%)]\tLoss: 0.075139\t Accuracy:97.872%\n",
      "Rate : 20 [35200/49000 (72%)]\tLoss: 0.006896\t Accuracy:97.849%\n",
      "Rate : 20 [35840/49000 (73%)]\tLoss: 0.022994\t Accuracy:97.867%\n",
      "Rate : 20 [36480/49000 (74%)]\tLoss: 0.032636\t Accuracy:97.877%\n",
      "Rate : 20 [37120/49000 (76%)]\tLoss: 0.020873\t Accuracy:97.882%\n",
      "Rate : 20 [37760/49000 (77%)]\tLoss: 0.066126\t Accuracy:97.875%\n",
      "Rate : 20 [38400/49000 (78%)]\tLoss: 0.021631\t Accuracy:97.890%\n",
      "Rate : 20 [39040/49000 (80%)]\tLoss: 0.050049\t Accuracy:97.886%\n",
      "Rate : 20 [39680/49000 (81%)]\tLoss: 0.011659\t Accuracy:97.902%\n",
      "Rate : 20 [40320/49000 (82%)]\tLoss: 0.126381\t Accuracy:97.911%\n",
      "Rate : 20 [40960/49000 (84%)]\tLoss: 0.037759\t Accuracy:97.926%\n",
      "Rate : 20 [41600/49000 (85%)]\tLoss: 0.000041\t Accuracy:97.944%\n",
      "Rate : 20 [42240/49000 (86%)]\tLoss: 0.000165\t Accuracy:97.958%\n",
      "Rate : 20 [42880/49000 (87%)]\tLoss: 0.012218\t Accuracy:97.961%\n",
      "Rate : 20 [43520/49000 (89%)]\tLoss: 0.002749\t Accuracy:97.975%\n",
      "Rate : 20 [44160/49000 (90%)]\tLoss: 0.004959\t Accuracy:97.979%\n",
      "Rate : 20 [44800/49000 (91%)]\tLoss: 0.205176\t Accuracy:97.966%\n",
      "Rate : 20 [45440/49000 (93%)]\tLoss: 0.005878\t Accuracy:97.970%\n",
      "Rate : 20 [46080/49000 (94%)]\tLoss: 0.003805\t Accuracy:97.979%\n",
      "Rate : 20 [46720/49000 (95%)]\tLoss: 0.001590\t Accuracy:98.000%\n",
      "Rate : 20 [47360/49000 (97%)]\tLoss: 0.012672\t Accuracy:98.014%\n",
      "Rate : 20 [48000/49000 (98%)]\tLoss: 0.098739\t Accuracy:98.018%\n",
      "Rate : 20 [48640/49000 (99%)]\tLoss: 0.020585\t Accuracy:98.026%\n",
      "Rate : 30 [0/49000 (0%)]\tLoss: 0.035376\t Accuracy:100.000%\n",
      "Rate : 30 [640/49000 (1%)]\tLoss: 0.000935\t Accuracy:97.917%\n",
      "Rate : 30 [1280/49000 (3%)]\tLoss: 0.228531\t Accuracy:97.561%\n",
      "Rate : 30 [1920/49000 (4%)]\tLoss: 0.005565\t Accuracy:97.592%\n",
      "Rate : 30 [2560/49000 (5%)]\tLoss: 0.106431\t Accuracy:97.492%\n",
      "Rate : 30 [3200/49000 (7%)]\tLoss: 0.003466\t Accuracy:97.772%\n",
      "Rate : 30 [3840/49000 (8%)]\tLoss: 0.000575\t Accuracy:97.676%\n",
      "Rate : 30 [4480/49000 (9%)]\tLoss: 0.050802\t Accuracy:97.762%\n",
      "Rate : 30 [5120/49000 (10%)]\tLoss: 0.159722\t Accuracy:97.748%\n",
      "Rate : 30 [5760/49000 (12%)]\tLoss: 0.042979\t Accuracy:97.876%\n",
      "Rate : 30 [6400/49000 (13%)]\tLoss: 0.000538\t Accuracy:97.979%\n",
      "Rate : 30 [7040/49000 (14%)]\tLoss: 0.093829\t Accuracy:98.006%\n",
      "Rate : 30 [7680/49000 (16%)]\tLoss: 0.061130\t Accuracy:97.990%\n",
      "Rate : 30 [8320/49000 (17%)]\tLoss: 0.083032\t Accuracy:98.012%\n",
      "Rate : 30 [8960/49000 (18%)]\tLoss: 0.068017\t Accuracy:98.032%\n",
      "Rate : 30 [9600/49000 (20%)]\tLoss: 0.000363\t Accuracy:98.038%\n",
      "Rate : 30 [10240/49000 (21%)]\tLoss: 0.006010\t Accuracy:98.053%\n",
      "Rate : 30 [10880/49000 (22%)]\tLoss: 0.185848\t Accuracy:98.066%\n",
      "Rate : 30 [11520/49000 (23%)]\tLoss: 0.003007\t Accuracy:98.104%\n",
      "Rate : 30 [12160/49000 (25%)]\tLoss: 0.055719\t Accuracy:98.163%\n",
      "Rate : 30 [12800/49000 (26%)]\tLoss: 0.191642\t Accuracy:98.137%\n",
      "Rate : 30 [13440/49000 (27%)]\tLoss: 0.008630\t Accuracy:98.137%\n",
      "Rate : 30 [14080/49000 (29%)]\tLoss: 0.066988\t Accuracy:98.101%\n",
      "Rate : 30 [14720/49000 (30%)]\tLoss: 0.077559\t Accuracy:98.136%\n",
      "Rate : 30 [15360/49000 (31%)]\tLoss: 0.000277\t Accuracy:98.174%\n",
      "Rate : 30 [16000/49000 (33%)]\tLoss: 0.035449\t Accuracy:98.166%\n",
      "Rate : 30 [16640/49000 (34%)]\tLoss: 0.001090\t Accuracy:98.195%\n",
      "Rate : 30 [17280/49000 (35%)]\tLoss: 0.001895\t Accuracy:98.227%\n",
      "Rate : 30 [17920/49000 (37%)]\tLoss: 0.034852\t Accuracy:98.223%\n",
      "Rate : 30 [18560/49000 (38%)]\tLoss: 0.007322\t Accuracy:98.236%\n",
      "Rate : 30 [19200/49000 (39%)]\tLoss: 0.509448\t Accuracy:98.191%\n",
      "Rate : 30 [19840/49000 (40%)]\tLoss: 0.228519\t Accuracy:98.178%\n",
      "Rate : 30 [20480/49000 (42%)]\tLoss: 0.157045\t Accuracy:98.162%\n",
      "Rate : 30 [21120/49000 (43%)]\tLoss: 0.010604\t Accuracy:98.189%\n",
      "Rate : 30 [21760/49000 (44%)]\tLoss: 0.111506\t Accuracy:98.151%\n",
      "Rate : 30 [22400/49000 (46%)]\tLoss: 0.154484\t Accuracy:98.137%\n",
      "Rate : 30 [23040/49000 (47%)]\tLoss: 0.535011\t Accuracy:98.128%\n",
      "Rate : 30 [23680/49000 (48%)]\tLoss: 0.031808\t Accuracy:98.144%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate : 30 [24320/49000 (50%)]\tLoss: 0.027562\t Accuracy:98.160%\n",
      "Rate : 30 [24960/49000 (51%)]\tLoss: 0.015172\t Accuracy:98.155%\n",
      "Rate : 30 [25600/49000 (52%)]\tLoss: 0.005900\t Accuracy:98.139%\n",
      "Rate : 30 [26240/49000 (54%)]\tLoss: 0.006174\t Accuracy:98.116%\n",
      "Rate : 30 [26880/49000 (55%)]\tLoss: 0.013263\t Accuracy:98.127%\n",
      "Rate : 30 [27520/49000 (56%)]\tLoss: 0.000700\t Accuracy:98.131%\n",
      "Rate : 30 [28160/49000 (57%)]\tLoss: 0.193057\t Accuracy:98.145%\n",
      "Rate : 30 [28800/49000 (59%)]\tLoss: 0.026822\t Accuracy:98.165%\n",
      "Rate : 30 [29440/49000 (60%)]\tLoss: 0.052132\t Accuracy:98.168%\n",
      "Rate : 30 [30080/49000 (61%)]\tLoss: 0.064794\t Accuracy:98.164%\n",
      "Rate : 30 [30720/49000 (63%)]\tLoss: 0.000068\t Accuracy:98.163%\n",
      "Rate : 30 [31360/49000 (64%)]\tLoss: 0.373646\t Accuracy:98.175%\n",
      "Rate : 30 [32000/49000 (65%)]\tLoss: 0.001499\t Accuracy:98.192%\n",
      "Rate : 30 [32640/49000 (67%)]\tLoss: 0.007683\t Accuracy:98.182%\n",
      "Rate : 30 [33280/49000 (68%)]\tLoss: 0.004379\t Accuracy:98.175%\n",
      "Rate : 30 [33920/49000 (69%)]\tLoss: 0.018445\t Accuracy:98.174%\n",
      "Rate : 30 [34560/49000 (70%)]\tLoss: 0.024781\t Accuracy:98.173%\n",
      "Rate : 30 [35200/49000 (72%)]\tLoss: 0.031076\t Accuracy:98.164%\n",
      "Rate : 30 [35840/49000 (73%)]\tLoss: 0.298003\t Accuracy:98.171%\n",
      "Rate : 30 [36480/49000 (74%)]\tLoss: 0.005579\t Accuracy:98.181%\n",
      "Rate : 30 [37120/49000 (76%)]\tLoss: 0.017544\t Accuracy:98.202%\n",
      "Rate : 30 [37760/49000 (77%)]\tLoss: 0.127158\t Accuracy:98.203%\n",
      "Rate : 30 [38400/49000 (78%)]\tLoss: 0.023017\t Accuracy:98.215%\n",
      "Rate : 30 [39040/49000 (80%)]\tLoss: 0.016264\t Accuracy:98.221%\n",
      "Rate : 30 [39680/49000 (81%)]\tLoss: 0.005018\t Accuracy:98.230%\n",
      "Rate : 30 [40320/49000 (82%)]\tLoss: 0.000798\t Accuracy:98.240%\n",
      "Rate : 30 [40960/49000 (84%)]\tLoss: 0.059541\t Accuracy:98.248%\n",
      "Rate : 30 [41600/49000 (85%)]\tLoss: 0.000029\t Accuracy:98.266%\n",
      "Rate : 30 [42240/49000 (86%)]\tLoss: 0.003160\t Accuracy:98.273%\n",
      "Rate : 30 [42880/49000 (87%)]\tLoss: 0.167323\t Accuracy:98.269%\n",
      "Rate : 30 [43520/49000 (89%)]\tLoss: 0.169124\t Accuracy:98.273%\n",
      "Rate : 30 [44160/49000 (90%)]\tLoss: 0.005129\t Accuracy:98.285%\n",
      "Rate : 30 [44800/49000 (91%)]\tLoss: 0.056921\t Accuracy:98.289%\n",
      "Rate : 30 [45440/49000 (93%)]\tLoss: 0.065756\t Accuracy:98.293%\n",
      "Rate : 30 [46080/49000 (94%)]\tLoss: 0.006368\t Accuracy:98.298%\n",
      "Rate : 30 [46720/49000 (95%)]\tLoss: 0.003316\t Accuracy:98.308%\n",
      "Rate : 30 [47360/49000 (97%)]\tLoss: 0.000822\t Accuracy:98.325%\n",
      "Rate : 30 [48000/49000 (98%)]\tLoss: 0.015576\t Accuracy:98.324%\n",
      "Rate : 30 [48640/49000 (99%)]\tLoss: 0.133980\t Accuracy:98.319%\n",
      "Rate : 40 [0/49000 (0%)]\tLoss: 0.000713\t Accuracy:100.000%\n",
      "Rate : 40 [640/49000 (1%)]\tLoss: 0.226459\t Accuracy:97.619%\n",
      "Rate : 40 [1280/49000 (3%)]\tLoss: 0.034430\t Accuracy:97.790%\n",
      "Rate : 40 [1920/49000 (4%)]\tLoss: 0.000074\t Accuracy:98.002%\n",
      "Rate : 40 [2560/49000 (5%)]\tLoss: 0.049367\t Accuracy:97.840%\n",
      "Rate : 40 [3200/49000 (7%)]\tLoss: 0.107128\t Accuracy:98.020%\n",
      "Rate : 40 [3840/49000 (8%)]\tLoss: 0.001670\t Accuracy:97.882%\n",
      "Rate : 40 [4480/49000 (9%)]\tLoss: 0.002688\t Accuracy:98.005%\n",
      "Rate : 40 [5120/49000 (10%)]\tLoss: 0.218041\t Accuracy:98.059%\n",
      "Rate : 40 [5760/49000 (12%)]\tLoss: 0.009335\t Accuracy:98.135%\n",
      "Rate : 40 [6400/49000 (13%)]\tLoss: 0.109970\t Accuracy:98.119%\n",
      "Rate : 40 [7040/49000 (14%)]\tLoss: 0.007004\t Accuracy:98.247%\n",
      "Rate : 40 [7680/49000 (16%)]\tLoss: 0.070284\t Accuracy:98.262%\n",
      "Rate : 40 [8320/49000 (17%)]\tLoss: 0.079069\t Accuracy:98.276%\n",
      "Rate : 40 [8960/49000 (18%)]\tLoss: 0.004937\t Accuracy:98.287%\n",
      "Rate : 40 [9600/49000 (20%)]\tLoss: 0.000140\t Accuracy:98.339%\n",
      "Rate : 40 [10240/49000 (21%)]\tLoss: 0.117213\t Accuracy:98.364%\n",
      "Rate : 40 [10880/49000 (22%)]\tLoss: 0.022256\t Accuracy:98.350%\n",
      "Rate : 40 [11520/49000 (23%)]\tLoss: 0.001857\t Accuracy:98.373%\n",
      "Rate : 40 [12160/49000 (25%)]\tLoss: 0.049902\t Accuracy:98.384%\n",
      "Rate : 40 [12800/49000 (26%)]\tLoss: 0.154454\t Accuracy:98.363%\n",
      "Rate : 40 [13440/49000 (27%)]\tLoss: 0.033776\t Accuracy:98.360%\n",
      "Rate : 40 [14080/49000 (29%)]\tLoss: 0.055129\t Accuracy:98.342%\n",
      "Rate : 40 [14720/49000 (30%)]\tLoss: 0.033037\t Accuracy:98.360%\n",
      "Rate : 40 [15360/49000 (31%)]\tLoss: 0.016014\t Accuracy:98.363%\n",
      "Rate : 40 [16000/49000 (33%)]\tLoss: 0.008467\t Accuracy:98.353%\n",
      "Rate : 40 [16640/49000 (34%)]\tLoss: 0.005226\t Accuracy:98.369%\n",
      "Rate : 40 [17280/49000 (35%)]\tLoss: 0.006803\t Accuracy:98.394%\n",
      "Rate : 40 [17920/49000 (37%)]\tLoss: 0.007277\t Accuracy:98.396%\n",
      "Rate : 40 [18560/49000 (38%)]\tLoss: 0.005433\t Accuracy:98.386%\n",
      "Rate : 40 [19200/49000 (39%)]\tLoss: 0.201014\t Accuracy:98.378%\n",
      "Rate : 40 [19840/49000 (40%)]\tLoss: 0.221249\t Accuracy:98.380%\n",
      "Rate : 40 [20480/49000 (42%)]\tLoss: 0.000365\t Accuracy:98.391%\n",
      "Rate : 40 [21120/49000 (43%)]\tLoss: 0.073823\t Accuracy:98.397%\n",
      "Rate : 40 [21760/49000 (44%)]\tLoss: 0.065357\t Accuracy:98.385%\n",
      "Rate : 40 [22400/49000 (46%)]\tLoss: 0.078905\t Accuracy:98.395%\n",
      "Rate : 40 [23040/49000 (47%)]\tLoss: 0.343112\t Accuracy:98.383%\n",
      "Rate : 40 [23680/49000 (48%)]\tLoss: 0.218455\t Accuracy:98.359%\n",
      "Rate : 40 [24320/49000 (50%)]\tLoss: 0.026886\t Accuracy:98.349%\n",
      "Rate : 40 [24960/49000 (51%)]\tLoss: 0.005869\t Accuracy:98.339%\n",
      "Rate : 40 [25600/49000 (52%)]\tLoss: 0.072865\t Accuracy:98.307%\n",
      "Rate : 40 [26240/49000 (54%)]\tLoss: 0.013536\t Accuracy:98.325%\n",
      "Rate : 40 [26880/49000 (55%)]\tLoss: 0.092743\t Accuracy:98.317%\n",
      "Rate : 40 [27520/49000 (56%)]\tLoss: 0.000115\t Accuracy:98.320%\n",
      "Rate : 40 [28160/49000 (57%)]\tLoss: 0.021726\t Accuracy:98.347%\n",
      "Rate : 40 [28800/49000 (59%)]\tLoss: 0.002629\t Accuracy:98.359%\n",
      "Rate : 40 [29440/49000 (60%)]\tLoss: 0.006142\t Accuracy:98.371%\n",
      "Rate : 40 [30080/49000 (61%)]\tLoss: 0.008787\t Accuracy:98.373%\n",
      "Rate : 40 [30720/49000 (63%)]\tLoss: 0.004448\t Accuracy:98.394%\n",
      "Rate : 40 [31360/49000 (64%)]\tLoss: 0.201739\t Accuracy:98.401%\n",
      "Rate : 40 [32000/49000 (65%)]\tLoss: 0.040506\t Accuracy:98.420%\n",
      "Rate : 40 [32640/49000 (67%)]\tLoss: 0.045853\t Accuracy:98.405%\n",
      "Rate : 40 [33280/49000 (68%)]\tLoss: 0.206820\t Accuracy:98.403%\n",
      "Rate : 40 [33920/49000 (69%)]\tLoss: 0.001824\t Accuracy:98.398%\n",
      "Rate : 40 [34560/49000 (70%)]\tLoss: 0.043636\t Accuracy:98.396%\n",
      "Rate : 40 [35200/49000 (72%)]\tLoss: 0.031676\t Accuracy:98.396%\n",
      "Rate : 40 [35840/49000 (73%)]\tLoss: 0.029934\t Accuracy:98.389%\n",
      "Rate : 40 [36480/49000 (74%)]\tLoss: 0.006243\t Accuracy:98.406%\n",
      "Rate : 40 [37120/49000 (76%)]\tLoss: 0.011960\t Accuracy:98.420%\n",
      "Rate : 40 [37760/49000 (77%)]\tLoss: 0.058667\t Accuracy:98.420%\n",
      "Rate : 40 [38400/49000 (78%)]\tLoss: 0.005278\t Accuracy:98.439%\n",
      "Rate : 40 [39040/49000 (80%)]\tLoss: 0.007595\t Accuracy:98.444%\n",
      "Rate : 40 [39680/49000 (81%)]\tLoss: 0.000952\t Accuracy:98.451%\n",
      "Rate : 40 [40320/49000 (82%)]\tLoss: 0.050325\t Accuracy:98.454%\n",
      "Rate : 40 [40960/49000 (84%)]\tLoss: 0.130205\t Accuracy:98.458%\n",
      "Rate : 40 [41600/49000 (85%)]\tLoss: 0.000017\t Accuracy:98.470%\n",
      "Rate : 40 [42240/49000 (86%)]\tLoss: 0.000178\t Accuracy:98.481%\n",
      "Rate : 40 [42880/49000 (87%)]\tLoss: 0.039351\t Accuracy:98.483%\n",
      "Rate : 40 [43520/49000 (89%)]\tLoss: 0.006483\t Accuracy:98.494%\n",
      "Rate : 40 [44160/49000 (90%)]\tLoss: 0.036766\t Accuracy:98.495%\n",
      "Rate : 40 [44800/49000 (91%)]\tLoss: 0.119224\t Accuracy:98.485%\n",
      "Rate : 40 [45440/49000 (93%)]\tLoss: 0.008269\t Accuracy:98.491%\n",
      "Rate : 40 [46080/49000 (94%)]\tLoss: 0.059235\t Accuracy:98.491%\n",
      "Rate : 40 [46720/49000 (95%)]\tLoss: 0.042620\t Accuracy:98.498%\n",
      "Rate : 40 [47360/49000 (97%)]\tLoss: 0.001750\t Accuracy:98.510%\n",
      "Rate : 40 [48000/49000 (98%)]\tLoss: 0.022500\t Accuracy:98.518%\n",
      "Rate : 40 [48640/49000 (99%)]\tLoss: 0.082090\t Accuracy:98.517%\n",
      "Rate : 50 [0/49000 (0%)]\tLoss: 0.001182\t Accuracy:100.000%\n",
      "Rate : 50 [640/49000 (1%)]\tLoss: 0.010626\t Accuracy:98.512%\n",
      "Rate : 50 [1280/49000 (3%)]\tLoss: 0.104825\t Accuracy:98.171%\n",
      "Rate : 50 [1920/49000 (4%)]\tLoss: 0.008710\t Accuracy:98.053%\n",
      "Rate : 50 [2560/49000 (5%)]\tLoss: 0.117934\t Accuracy:98.341%\n",
      "Rate : 50 [3200/49000 (7%)]\tLoss: 0.001469\t Accuracy:98.391%\n",
      "Rate : 50 [3840/49000 (8%)]\tLoss: 0.013140\t Accuracy:98.373%\n",
      "Rate : 50 [4480/49000 (9%)]\tLoss: 0.000388\t Accuracy:98.426%\n",
      "Rate : 50 [5120/49000 (10%)]\tLoss: 0.000939\t Accuracy:98.467%\n",
      "Rate : 50 [5760/49000 (12%)]\tLoss: 0.010567\t Accuracy:98.567%\n",
      "Rate : 50 [6400/49000 (13%)]\tLoss: 0.053245\t Accuracy:98.554%\n",
      "Rate : 50 [7040/49000 (14%)]\tLoss: 0.137031\t Accuracy:98.515%\n",
      "Rate : 50 [7680/49000 (16%)]\tLoss: 0.009895\t Accuracy:98.496%\n",
      "Rate : 50 [8320/49000 (17%)]\tLoss: 0.015547\t Accuracy:98.503%\n",
      "Rate : 50 [8960/49000 (18%)]\tLoss: 0.040738\t Accuracy:98.432%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate : 50 [9600/49000 (20%)]\tLoss: 0.000595\t Accuracy:98.422%\n",
      "Rate : 50 [10240/49000 (21%)]\tLoss: 0.211112\t Accuracy:98.403%\n",
      "Rate : 50 [10880/49000 (22%)]\tLoss: 0.046897\t Accuracy:98.405%\n",
      "Rate : 50 [11520/49000 (23%)]\tLoss: 0.002434\t Accuracy:98.373%\n",
      "Rate : 50 [12160/49000 (25%)]\tLoss: 0.049466\t Accuracy:98.368%\n",
      "Rate : 50 [12800/49000 (26%)]\tLoss: 0.076695\t Accuracy:98.356%\n",
      "Rate : 50 [13440/49000 (27%)]\tLoss: 0.001654\t Accuracy:98.360%\n",
      "Rate : 50 [14080/49000 (29%)]\tLoss: 0.041311\t Accuracy:98.342%\n",
      "Rate : 50 [14720/49000 (30%)]\tLoss: 0.038717\t Accuracy:98.366%\n",
      "Rate : 50 [15360/49000 (31%)]\tLoss: 0.003659\t Accuracy:98.389%\n",
      "Rate : 50 [16000/49000 (33%)]\tLoss: 0.005589\t Accuracy:98.378%\n",
      "Rate : 50 [16640/49000 (34%)]\tLoss: 0.047885\t Accuracy:98.399%\n",
      "Rate : 50 [17280/49000 (35%)]\tLoss: 0.004129\t Accuracy:98.423%\n",
      "Rate : 50 [17920/49000 (37%)]\tLoss: 0.008827\t Accuracy:98.407%\n",
      "Rate : 50 [18560/49000 (38%)]\tLoss: 0.012986\t Accuracy:98.424%\n",
      "Rate : 50 [19200/49000 (39%)]\tLoss: 0.285066\t Accuracy:98.414%\n",
      "Rate : 50 [19840/49000 (40%)]\tLoss: 0.035836\t Accuracy:98.390%\n",
      "Rate : 50 [20480/49000 (42%)]\tLoss: 0.026060\t Accuracy:98.401%\n",
      "Rate : 50 [21120/49000 (43%)]\tLoss: 0.140354\t Accuracy:98.407%\n",
      "Rate : 50 [21760/49000 (44%)]\tLoss: 0.354444\t Accuracy:98.421%\n",
      "Rate : 50 [22400/49000 (46%)]\tLoss: 0.176445\t Accuracy:98.426%\n",
      "Rate : 50 [23040/49000 (47%)]\tLoss: 0.009893\t Accuracy:98.405%\n",
      "Rate : 50 [23680/49000 (48%)]\tLoss: 0.002273\t Accuracy:98.397%\n",
      "Rate : 50 [24320/49000 (50%)]\tLoss: 0.006521\t Accuracy:98.419%\n",
      "Rate : 50 [24960/49000 (51%)]\tLoss: 0.044972\t Accuracy:98.444%\n",
      "Rate : 50 [25600/49000 (52%)]\tLoss: 0.000552\t Accuracy:98.447%\n",
      "Rate : 50 [26240/49000 (54%)]\tLoss: 0.002400\t Accuracy:98.470%\n",
      "Rate : 50 [26880/49000 (55%)]\tLoss: 0.000382\t Accuracy:98.477%\n",
      "Rate : 50 [27520/49000 (56%)]\tLoss: 0.000004\t Accuracy:98.490%\n",
      "Rate : 50 [28160/49000 (57%)]\tLoss: 0.146520\t Accuracy:98.510%\n",
      "Rate : 50 [28800/49000 (59%)]\tLoss: 0.003124\t Accuracy:98.512%\n",
      "Rate : 50 [29440/49000 (60%)]\tLoss: 0.015276\t Accuracy:98.517%\n",
      "Rate : 50 [30080/49000 (61%)]\tLoss: 0.015740\t Accuracy:98.522%\n",
      "Rate : 50 [30720/49000 (63%)]\tLoss: 0.000266\t Accuracy:98.533%\n",
      "Rate : 50 [31360/49000 (64%)]\tLoss: 0.031880\t Accuracy:98.541%\n",
      "Rate : 50 [32000/49000 (65%)]\tLoss: 0.000705\t Accuracy:98.548%\n",
      "Rate : 50 [32640/49000 (67%)]\tLoss: 0.014775\t Accuracy:98.552%\n",
      "Rate : 50 [33280/49000 (68%)]\tLoss: 0.000433\t Accuracy:98.553%\n",
      "Rate : 50 [33920/49000 (69%)]\tLoss: 0.091417\t Accuracy:98.551%\n",
      "Rate : 50 [34560/49000 (70%)]\tLoss: 0.009666\t Accuracy:98.537%\n",
      "Rate : 50 [35200/49000 (72%)]\tLoss: 0.001936\t Accuracy:98.555%\n",
      "Rate : 50 [35840/49000 (73%)]\tLoss: 0.004347\t Accuracy:98.570%\n",
      "Rate : 50 [36480/49000 (74%)]\tLoss: 0.000150\t Accuracy:98.584%\n",
      "Rate : 50 [37120/49000 (76%)]\tLoss: 0.001867\t Accuracy:98.590%\n",
      "Rate : 50 [37760/49000 (77%)]\tLoss: 0.151345\t Accuracy:98.590%\n",
      "Rate : 50 [38400/49000 (78%)]\tLoss: 0.160308\t Accuracy:98.598%\n",
      "Rate : 50 [39040/49000 (80%)]\tLoss: 0.001972\t Accuracy:98.597%\n",
      "Rate : 50 [39680/49000 (81%)]\tLoss: 0.000233\t Accuracy:98.613%\n",
      "Rate : 50 [40320/49000 (82%)]\tLoss: 0.339445\t Accuracy:98.617%\n",
      "Rate : 50 [40960/49000 (84%)]\tLoss: 0.018452\t Accuracy:98.627%\n",
      "Rate : 50 [41600/49000 (85%)]\tLoss: 0.000003\t Accuracy:98.636%\n",
      "Rate : 50 [42240/49000 (86%)]\tLoss: 0.000126\t Accuracy:98.649%\n",
      "Rate : 50 [42880/49000 (87%)]\tLoss: 0.005688\t Accuracy:98.658%\n",
      "Rate : 50 [43520/49000 (89%)]\tLoss: 0.000324\t Accuracy:98.666%\n",
      "Rate : 50 [44160/49000 (90%)]\tLoss: 0.000366\t Accuracy:98.669%\n",
      "Rate : 50 [44800/49000 (91%)]\tLoss: 0.019426\t Accuracy:98.671%\n",
      "Rate : 50 [45440/49000 (93%)]\tLoss: 0.046099\t Accuracy:98.678%\n",
      "Rate : 50 [46080/49000 (94%)]\tLoss: 0.007923\t Accuracy:98.684%\n",
      "Rate : 50 [46720/49000 (95%)]\tLoss: 0.001261\t Accuracy:98.693%\n",
      "Rate : 50 [47360/49000 (97%)]\tLoss: 0.000119\t Accuracy:98.700%\n",
      "Rate : 50 [48000/49000 (98%)]\tLoss: 0.136734\t Accuracy:98.705%\n",
      "Rate : 50 [48640/49000 (99%)]\tLoss: 0.131947\t Accuracy:98.708%\n",
      "Rate : 60 [0/49000 (0%)]\tLoss: 0.086322\t Accuracy:96.875%\n",
      "Rate : 60 [640/49000 (1%)]\tLoss: 0.056085\t Accuracy:98.065%\n",
      "Rate : 60 [1280/49000 (3%)]\tLoss: 0.030364\t Accuracy:97.866%\n",
      "Rate : 60 [1920/49000 (4%)]\tLoss: 0.055694\t Accuracy:97.848%\n",
      "Rate : 60 [2560/49000 (5%)]\tLoss: 0.011128\t Accuracy:97.994%\n",
      "Rate : 60 [3200/49000 (7%)]\tLoss: 0.014908\t Accuracy:98.113%\n",
      "Rate : 60 [3840/49000 (8%)]\tLoss: 0.000544\t Accuracy:98.244%\n",
      "Rate : 60 [4480/49000 (9%)]\tLoss: 0.006049\t Accuracy:98.227%\n",
      "Rate : 60 [5120/49000 (10%)]\tLoss: 0.163260\t Accuracy:98.234%\n",
      "Rate : 60 [5760/49000 (12%)]\tLoss: 0.078286\t Accuracy:98.325%\n",
      "Rate : 60 [6400/49000 (13%)]\tLoss: 0.035331\t Accuracy:98.352%\n",
      "Rate : 60 [7040/49000 (14%)]\tLoss: 0.041578\t Accuracy:98.416%\n",
      "Rate : 60 [7680/49000 (16%)]\tLoss: 0.000773\t Accuracy:98.509%\n",
      "Rate : 60 [8320/49000 (17%)]\tLoss: 0.132019\t Accuracy:98.479%\n",
      "Rate : 60 [8960/49000 (18%)]\tLoss: 0.107365\t Accuracy:98.521%\n",
      "Rate : 60 [9600/49000 (20%)]\tLoss: 0.000453\t Accuracy:98.547%\n",
      "Rate : 60 [10240/49000 (21%)]\tLoss: 0.000047\t Accuracy:98.579%\n",
      "Rate : 60 [10880/49000 (22%)]\tLoss: 0.012215\t Accuracy:98.580%\n",
      "Rate : 60 [11520/49000 (23%)]\tLoss: 0.015676\t Accuracy:98.589%\n",
      "Rate : 60 [12160/49000 (25%)]\tLoss: 0.063460\t Accuracy:98.638%\n",
      "Rate : 60 [12800/49000 (26%)]\tLoss: 0.139566\t Accuracy:98.628%\n",
      "Rate : 60 [13440/49000 (27%)]\tLoss: 0.000059\t Accuracy:98.649%\n",
      "Rate : 60 [14080/49000 (29%)]\tLoss: 0.038787\t Accuracy:98.661%\n",
      "Rate : 60 [14720/49000 (30%)]\tLoss: 0.045681\t Accuracy:98.685%\n",
      "Rate : 60 [15360/49000 (31%)]\tLoss: 0.000909\t Accuracy:98.707%\n",
      "Rate : 60 [16000/49000 (33%)]\tLoss: 0.012680\t Accuracy:98.721%\n",
      "Rate : 60 [16640/49000 (34%)]\tLoss: 0.054579\t Accuracy:98.728%\n",
      "Rate : 60 [17280/49000 (35%)]\tLoss: 0.087676\t Accuracy:98.729%\n",
      "Rate : 60 [17920/49000 (37%)]\tLoss: 0.010838\t Accuracy:98.736%\n",
      "Rate : 60 [18560/49000 (38%)]\tLoss: 0.000433\t Accuracy:98.758%\n",
      "Rate : 60 [19200/49000 (39%)]\tLoss: 0.131791\t Accuracy:98.768%\n",
      "Rate : 60 [19840/49000 (40%)]\tLoss: 0.019185\t Accuracy:98.767%\n",
      "Rate : 60 [20480/49000 (42%)]\tLoss: 0.003840\t Accuracy:98.767%\n",
      "Rate : 60 [21120/49000 (43%)]\tLoss: 0.142467\t Accuracy:98.766%\n",
      "Rate : 60 [21760/49000 (44%)]\tLoss: 0.063917\t Accuracy:98.752%\n",
      "Rate : 60 [22400/49000 (46%)]\tLoss: 0.045364\t Accuracy:98.774%\n",
      "Rate : 60 [23040/49000 (47%)]\tLoss: 0.016060\t Accuracy:98.752%\n",
      "Rate : 60 [23680/49000 (48%)]\tLoss: 0.003435\t Accuracy:98.752%\n",
      "Rate : 60 [24320/49000 (50%)]\tLoss: 0.000122\t Accuracy:98.752%\n",
      "Rate : 60 [24960/49000 (51%)]\tLoss: 0.127276\t Accuracy:98.752%\n",
      "Rate : 60 [25600/49000 (52%)]\tLoss: 0.000028\t Accuracy:98.736%\n",
      "Rate : 60 [26240/49000 (54%)]\tLoss: 0.019780\t Accuracy:98.732%\n",
      "Rate : 60 [26880/49000 (55%)]\tLoss: 0.000775\t Accuracy:98.733%\n",
      "Rate : 60 [27520/49000 (56%)]\tLoss: 0.000016\t Accuracy:98.741%\n",
      "Rate : 60 [28160/49000 (57%)]\tLoss: 0.080749\t Accuracy:98.744%\n",
      "Rate : 60 [28800/49000 (59%)]\tLoss: 0.003766\t Accuracy:98.741%\n",
      "Rate : 60 [29440/49000 (60%)]\tLoss: 0.183112\t Accuracy:98.734%\n",
      "Rate : 60 [30080/49000 (61%)]\tLoss: 0.213530\t Accuracy:98.731%\n",
      "Rate : 60 [30720/49000 (63%)]\tLoss: 0.000617\t Accuracy:98.725%\n",
      "Rate : 60 [31360/49000 (64%)]\tLoss: 0.149984\t Accuracy:98.742%\n",
      "Rate : 60 [32000/49000 (65%)]\tLoss: 0.000167\t Accuracy:98.761%\n",
      "Rate : 60 [32640/49000 (67%)]\tLoss: 0.004216\t Accuracy:98.763%\n",
      "Rate : 60 [33280/49000 (68%)]\tLoss: 0.000002\t Accuracy:98.751%\n",
      "Rate : 60 [33920/49000 (69%)]\tLoss: 0.003006\t Accuracy:98.751%\n",
      "Rate : 60 [34560/49000 (70%)]\tLoss: 0.021654\t Accuracy:98.745%\n",
      "Rate : 60 [35200/49000 (72%)]\tLoss: 0.003099\t Accuracy:98.745%\n",
      "Rate : 60 [35840/49000 (73%)]\tLoss: 0.013414\t Accuracy:98.754%\n",
      "Rate : 60 [36480/49000 (74%)]\tLoss: 0.008030\t Accuracy:98.759%\n",
      "Rate : 60 [37120/49000 (76%)]\tLoss: 0.001906\t Accuracy:98.765%\n",
      "Rate : 60 [37760/49000 (77%)]\tLoss: 0.003255\t Accuracy:98.764%\n",
      "Rate : 60 [38400/49000 (78%)]\tLoss: 0.008544\t Accuracy:98.780%\n",
      "Rate : 60 [39040/49000 (80%)]\tLoss: 0.005935\t Accuracy:98.777%\n",
      "Rate : 60 [39680/49000 (81%)]\tLoss: 0.000310\t Accuracy:98.781%\n",
      "Rate : 60 [40320/49000 (82%)]\tLoss: 0.228636\t Accuracy:98.791%\n",
      "Rate : 60 [40960/49000 (84%)]\tLoss: 0.004601\t Accuracy:98.792%\n",
      "Rate : 60 [41600/49000 (85%)]\tLoss: 0.000169\t Accuracy:98.804%\n",
      "Rate : 60 [42240/49000 (86%)]\tLoss: 0.000048\t Accuracy:98.798%\n",
      "Rate : 60 [42880/49000 (87%)]\tLoss: 0.176808\t Accuracy:98.795%\n",
      "Rate : 60 [43520/49000 (89%)]\tLoss: 0.000357\t Accuracy:98.797%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate : 60 [44160/49000 (90%)]\tLoss: 0.001052\t Accuracy:98.794%\n",
      "Rate : 60 [44800/49000 (91%)]\tLoss: 0.004039\t Accuracy:98.791%\n",
      "Rate : 60 [45440/49000 (93%)]\tLoss: 0.001009\t Accuracy:98.795%\n",
      "Rate : 60 [46080/49000 (94%)]\tLoss: 0.000977\t Accuracy:98.801%\n",
      "Rate : 60 [46720/49000 (95%)]\tLoss: 0.001470\t Accuracy:98.811%\n",
      "Rate : 60 [47360/49000 (97%)]\tLoss: 0.001485\t Accuracy:98.816%\n",
      "Rate : 60 [48000/49000 (98%)]\tLoss: 0.253610\t Accuracy:98.811%\n",
      "Rate : 60 [48640/49000 (99%)]\tLoss: 0.020339\t Accuracy:98.812%\n",
      "Rate : 70 [0/49000 (0%)]\tLoss: 0.026265\t Accuracy:100.000%\n",
      "Rate : 70 [640/49000 (1%)]\tLoss: 0.030428\t Accuracy:97.024%\n",
      "Rate : 70 [1280/49000 (3%)]\tLoss: 0.436806\t Accuracy:97.637%\n",
      "Rate : 70 [1920/49000 (4%)]\tLoss: 0.000358\t Accuracy:97.695%\n",
      "Rate : 70 [2560/49000 (5%)]\tLoss: 0.004122\t Accuracy:97.955%\n",
      "Rate : 70 [3200/49000 (7%)]\tLoss: 0.023337\t Accuracy:98.113%\n",
      "Rate : 70 [3840/49000 (8%)]\tLoss: 0.002239\t Accuracy:98.037%\n",
      "Rate : 70 [4480/49000 (9%)]\tLoss: 0.001476\t Accuracy:97.961%\n",
      "Rate : 70 [5120/49000 (10%)]\tLoss: 0.017487\t Accuracy:97.962%\n",
      "Rate : 70 [5760/49000 (12%)]\tLoss: 0.015835\t Accuracy:98.049%\n",
      "Rate : 70 [6400/49000 (13%)]\tLoss: 0.027883\t Accuracy:98.119%\n",
      "Rate : 70 [7040/49000 (14%)]\tLoss: 0.000468\t Accuracy:98.190%\n",
      "Rate : 70 [7680/49000 (16%)]\tLoss: 0.021971\t Accuracy:98.249%\n",
      "Rate : 70 [8320/49000 (17%)]\tLoss: 0.156131\t Accuracy:98.192%\n",
      "Rate : 70 [8960/49000 (18%)]\tLoss: 0.008158\t Accuracy:98.232%\n",
      "Rate : 70 [9600/49000 (20%)]\tLoss: 0.001478\t Accuracy:98.318%\n",
      "Rate : 70 [10240/49000 (21%)]\tLoss: 0.008445\t Accuracy:98.355%\n",
      "Rate : 70 [10880/49000 (22%)]\tLoss: 0.069245\t Accuracy:98.360%\n",
      "Rate : 70 [11520/49000 (23%)]\tLoss: 0.052899\t Accuracy:98.355%\n",
      "Rate : 70 [12160/49000 (25%)]\tLoss: 0.134614\t Accuracy:98.384%\n",
      "Rate : 70 [12800/49000 (26%)]\tLoss: 0.101203\t Accuracy:98.387%\n",
      "Rate : 70 [13440/49000 (27%)]\tLoss: 0.006618\t Accuracy:98.367%\n",
      "Rate : 70 [14080/49000 (29%)]\tLoss: 0.063100\t Accuracy:98.349%\n",
      "Rate : 70 [14720/49000 (30%)]\tLoss: 0.191805\t Accuracy:98.346%\n",
      "Rate : 70 [15360/49000 (31%)]\tLoss: 0.000469\t Accuracy:98.363%\n",
      "Rate : 70 [16000/49000 (33%)]\tLoss: 0.007405\t Accuracy:98.378%\n",
      "Rate : 70 [16640/49000 (34%)]\tLoss: 0.051785\t Accuracy:98.381%\n",
      "Rate : 70 [17280/49000 (35%)]\tLoss: 0.002867\t Accuracy:98.394%\n",
      "Rate : 70 [17920/49000 (37%)]\tLoss: 0.009408\t Accuracy:98.407%\n",
      "Rate : 70 [18560/49000 (38%)]\tLoss: 0.005952\t Accuracy:98.419%\n",
      "Rate : 70 [19200/49000 (39%)]\tLoss: 0.964427\t Accuracy:98.414%\n",
      "Rate : 70 [19840/49000 (40%)]\tLoss: 0.014456\t Accuracy:98.420%\n",
      "Rate : 70 [20480/49000 (42%)]\tLoss: 0.015339\t Accuracy:98.420%\n",
      "Rate : 70 [21120/49000 (43%)]\tLoss: 0.007672\t Accuracy:98.430%\n",
      "Rate : 70 [21760/49000 (44%)]\tLoss: 0.025178\t Accuracy:98.435%\n",
      "Rate : 70 [22400/49000 (46%)]\tLoss: 0.254579\t Accuracy:98.426%\n",
      "Rate : 70 [23040/49000 (47%)]\tLoss: 0.003148\t Accuracy:98.418%\n",
      "Rate : 70 [23680/49000 (48%)]\tLoss: 0.001349\t Accuracy:98.435%\n",
      "Rate : 70 [24320/49000 (50%)]\tLoss: 0.000371\t Accuracy:98.460%\n",
      "Rate : 70 [24960/49000 (51%)]\tLoss: 0.059956\t Accuracy:98.468%\n",
      "Rate : 70 [25600/49000 (52%)]\tLoss: 0.000736\t Accuracy:98.455%\n",
      "Rate : 70 [26240/49000 (54%)]\tLoss: 0.019910\t Accuracy:98.470%\n",
      "Rate : 70 [26880/49000 (55%)]\tLoss: 0.002659\t Accuracy:98.477%\n",
      "Rate : 70 [27520/49000 (56%)]\tLoss: 0.000459\t Accuracy:98.490%\n",
      "Rate : 70 [28160/49000 (57%)]\tLoss: 0.036436\t Accuracy:98.514%\n",
      "Rate : 70 [28800/49000 (59%)]\tLoss: 0.012623\t Accuracy:98.533%\n",
      "Rate : 70 [29440/49000 (60%)]\tLoss: 0.051984\t Accuracy:98.544%\n",
      "Rate : 70 [30080/49000 (61%)]\tLoss: 0.002094\t Accuracy:98.552%\n",
      "Rate : 70 [30720/49000 (63%)]\tLoss: 0.000449\t Accuracy:98.556%\n",
      "Rate : 70 [31360/49000 (64%)]\tLoss: 0.281565\t Accuracy:98.563%\n",
      "Rate : 70 [32000/49000 (65%)]\tLoss: 0.001159\t Accuracy:98.576%\n",
      "Rate : 70 [32640/49000 (67%)]\tLoss: 0.030906\t Accuracy:98.589%\n",
      "Rate : 70 [33280/49000 (68%)]\tLoss: 0.000126\t Accuracy:98.574%\n",
      "Rate : 70 [33920/49000 (69%)]\tLoss: 0.001537\t Accuracy:98.574%\n",
      "Rate : 70 [34560/49000 (70%)]\tLoss: 0.005324\t Accuracy:98.566%\n",
      "Rate : 70 [35200/49000 (72%)]\tLoss: 0.001405\t Accuracy:98.581%\n",
      "Rate : 70 [35840/49000 (73%)]\tLoss: 0.051456\t Accuracy:98.587%\n",
      "Rate : 70 [36480/49000 (74%)]\tLoss: 0.001610\t Accuracy:98.590%\n",
      "Rate : 70 [37120/49000 (76%)]\tLoss: 0.002870\t Accuracy:98.606%\n",
      "Rate : 70 [37760/49000 (77%)]\tLoss: 0.000101\t Accuracy:98.621%\n",
      "Rate : 70 [38400/49000 (78%)]\tLoss: 0.015402\t Accuracy:98.642%\n",
      "Rate : 70 [39040/49000 (80%)]\tLoss: 0.045228\t Accuracy:98.644%\n",
      "Rate : 70 [39680/49000 (81%)]\tLoss: 0.000059\t Accuracy:98.663%\n",
      "Rate : 70 [40320/49000 (82%)]\tLoss: 0.088490\t Accuracy:98.669%\n",
      "Rate : 70 [40960/49000 (84%)]\tLoss: 0.086058\t Accuracy:98.670%\n",
      "Rate : 70 [41600/49000 (85%)]\tLoss: 0.000705\t Accuracy:98.679%\n",
      "Rate : 70 [42240/49000 (86%)]\tLoss: 0.001309\t Accuracy:98.682%\n",
      "Rate : 70 [42880/49000 (87%)]\tLoss: 0.014730\t Accuracy:98.695%\n",
      "Rate : 70 [43520/49000 (89%)]\tLoss: 0.000393\t Accuracy:98.705%\n",
      "Rate : 70 [44160/49000 (90%)]\tLoss: 0.041179\t Accuracy:98.712%\n",
      "Rate : 70 [44800/49000 (91%)]\tLoss: 0.012261\t Accuracy:98.720%\n",
      "Rate : 70 [45440/49000 (93%)]\tLoss: 0.006397\t Accuracy:98.718%\n",
      "Rate : 70 [46080/49000 (94%)]\tLoss: 0.003696\t Accuracy:98.729%\n",
      "Rate : 70 [46720/49000 (95%)]\tLoss: 0.001465\t Accuracy:98.740%\n",
      "Rate : 70 [47360/49000 (97%)]\tLoss: 0.000283\t Accuracy:98.749%\n",
      "Rate : 70 [48000/49000 (98%)]\tLoss: 0.170945\t Accuracy:98.755%\n",
      "Rate : 70 [48640/49000 (99%)]\tLoss: 0.061882\t Accuracy:98.759%\n",
      "Rate : 80 [0/49000 (0%)]\tLoss: 1.881862\t Accuracy:40.625%\n",
      "Rate : 80 [640/49000 (1%)]\tLoss: 1.180186\t Accuracy:40.625%\n",
      "Rate : 80 [1280/49000 (3%)]\tLoss: 0.793604\t Accuracy:48.399%\n",
      "Rate : 80 [1920/49000 (4%)]\tLoss: 0.445638\t Accuracy:55.840%\n",
      "Rate : 80 [2560/49000 (5%)]\tLoss: 0.456211\t Accuracy:60.224%\n",
      "Rate : 80 [3200/49000 (7%)]\tLoss: 0.484491\t Accuracy:63.830%\n",
      "Rate : 80 [3840/49000 (8%)]\tLoss: 0.391300\t Accuracy:66.684%\n",
      "Rate : 80 [4480/49000 (9%)]\tLoss: 0.545328\t Accuracy:68.551%\n",
      "Rate : 80 [5120/49000 (10%)]\tLoss: 0.450146\t Accuracy:70.575%\n",
      "Rate : 80 [5760/49000 (12%)]\tLoss: 0.377006\t Accuracy:72.393%\n",
      "Rate : 80 [6400/49000 (13%)]\tLoss: 0.443872\t Accuracy:73.228%\n",
      "Rate : 80 [7040/49000 (14%)]\tLoss: 0.308879\t Accuracy:74.152%\n",
      "Rate : 80 [7680/49000 (16%)]\tLoss: 0.839631\t Accuracy:74.987%\n",
      "Rate : 80 [8320/49000 (17%)]\tLoss: 0.439824\t Accuracy:75.742%\n",
      "Rate : 80 [8960/49000 (18%)]\tLoss: 0.356296\t Accuracy:76.234%\n",
      "Rate : 80 [9600/49000 (20%)]\tLoss: 0.108679\t Accuracy:76.640%\n",
      "Rate : 80 [10240/49000 (21%)]\tLoss: 0.211274\t Accuracy:77.220%\n",
      "Rate : 80 [10880/49000 (22%)]\tLoss: 0.369984\t Accuracy:77.969%\n",
      "Rate : 80 [11520/49000 (23%)]\tLoss: 0.359310\t Accuracy:78.497%\n",
      "Rate : 80 [12160/49000 (25%)]\tLoss: 0.280224\t Accuracy:79.060%\n",
      "Rate : 80 [12800/49000 (26%)]\tLoss: 0.396789\t Accuracy:79.551%\n",
      "Rate : 80 [13440/49000 (27%)]\tLoss: 0.148207\t Accuracy:80.114%\n",
      "Rate : 80 [14080/49000 (29%)]\tLoss: 0.798407\t Accuracy:80.584%\n",
      "Rate : 80 [14720/49000 (30%)]\tLoss: 0.328361\t Accuracy:81.101%\n",
      "Rate : 80 [15360/49000 (31%)]\tLoss: 0.075234\t Accuracy:81.607%\n",
      "Rate : 80 [16000/49000 (33%)]\tLoss: 0.113505\t Accuracy:81.999%\n",
      "Rate : 80 [16640/49000 (34%)]\tLoss: 0.356663\t Accuracy:82.438%\n",
      "Rate : 80 [17280/49000 (35%)]\tLoss: 0.089348\t Accuracy:82.792%\n",
      "Rate : 80 [17920/49000 (37%)]\tLoss: 0.188236\t Accuracy:83.127%\n",
      "Rate : 80 [18560/49000 (38%)]\tLoss: 0.130474\t Accuracy:83.444%\n",
      "Rate : 80 [19200/49000 (39%)]\tLoss: 0.691844\t Accuracy:83.756%\n",
      "Rate : 80 [19840/49000 (40%)]\tLoss: 0.574842\t Accuracy:84.013%\n",
      "Rate : 80 [20480/49000 (42%)]\tLoss: 0.177239\t Accuracy:84.302%\n",
      "Rate : 80 [21120/49000 (43%)]\tLoss: 0.161538\t Accuracy:84.559%\n",
      "Rate : 80 [21760/49000 (44%)]\tLoss: 0.155679\t Accuracy:84.852%\n",
      "Rate : 80 [22400/49000 (46%)]\tLoss: 0.156474\t Accuracy:85.119%\n",
      "Rate : 80 [23040/49000 (47%)]\tLoss: 0.261556\t Accuracy:85.372%\n",
      "Rate : 80 [23680/49000 (48%)]\tLoss: 0.042005\t Accuracy:85.628%\n",
      "Rate : 80 [24320/49000 (50%)]\tLoss: 0.176257\t Accuracy:85.849%\n",
      "Rate : 80 [24960/49000 (51%)]\tLoss: 0.375433\t Accuracy:86.068%\n",
      "Rate : 80 [25600/49000 (52%)]\tLoss: 0.086300\t Accuracy:86.248%\n",
      "Rate : 80 [26240/49000 (54%)]\tLoss: 0.136887\t Accuracy:86.438%\n",
      "Rate : 80 [26880/49000 (55%)]\tLoss: 0.679841\t Accuracy:86.642%\n",
      "Rate : 80 [27520/49000 (56%)]\tLoss: 0.018782\t Accuracy:86.839%\n",
      "Rate : 80 [28160/49000 (57%)]\tLoss: 0.505189\t Accuracy:86.986%\n",
      "Rate : 80 [28800/49000 (59%)]\tLoss: 0.057132\t Accuracy:87.146%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate : 80 [29440/49000 (60%)]\tLoss: 0.195398\t Accuracy:87.300%\n",
      "Rate : 80 [30080/49000 (61%)]\tLoss: 0.286479\t Accuracy:87.427%\n",
      "Rate : 80 [30720/49000 (63%)]\tLoss: 0.044704\t Accuracy:87.581%\n",
      "Rate : 80 [31360/49000 (64%)]\tLoss: 0.804738\t Accuracy:87.761%\n",
      "Rate : 80 [32000/49000 (65%)]\tLoss: 0.123163\t Accuracy:87.931%\n",
      "Rate : 80 [32640/49000 (67%)]\tLoss: 0.280447\t Accuracy:88.075%\n",
      "Rate : 80 [33280/49000 (68%)]\tLoss: 0.061631\t Accuracy:88.223%\n",
      "Rate : 80 [33920/49000 (69%)]\tLoss: 0.019231\t Accuracy:88.328%\n",
      "Rate : 80 [34560/49000 (70%)]\tLoss: 0.125393\t Accuracy:88.471%\n",
      "Rate : 80 [35200/49000 (72%)]\tLoss: 0.063853\t Accuracy:88.590%\n",
      "Rate : 80 [35840/49000 (73%)]\tLoss: 0.036134\t Accuracy:88.741%\n",
      "Rate : 80 [36480/49000 (74%)]\tLoss: 0.046233\t Accuracy:88.867%\n",
      "Rate : 80 [37120/49000 (76%)]\tLoss: 0.361173\t Accuracy:88.999%\n",
      "Rate : 80 [37760/49000 (77%)]\tLoss: 0.187592\t Accuracy:89.109%\n",
      "Rate : 80 [38400/49000 (78%)]\tLoss: 0.239773\t Accuracy:89.223%\n",
      "Rate : 80 [39040/49000 (80%)]\tLoss: 0.114466\t Accuracy:89.322%\n",
      "Rate : 80 [39680/49000 (81%)]\tLoss: 0.061251\t Accuracy:89.429%\n",
      "Rate : 80 [40320/49000 (82%)]\tLoss: 0.173766\t Accuracy:89.559%\n",
      "Rate : 80 [40960/49000 (84%)]\tLoss: 0.210516\t Accuracy:89.666%\n",
      "Rate : 80 [41600/49000 (85%)]\tLoss: 0.017250\t Accuracy:89.775%\n",
      "Rate : 80 [42240/49000 (86%)]\tLoss: 0.012624\t Accuracy:89.885%\n",
      "Rate : 80 [42880/49000 (87%)]\tLoss: 0.177047\t Accuracy:89.975%\n",
      "Rate : 80 [43520/49000 (89%)]\tLoss: 0.117789\t Accuracy:90.062%\n",
      "Rate : 80 [44160/49000 (90%)]\tLoss: 0.024979\t Accuracy:90.159%\n",
      "Rate : 80 [44800/49000 (91%)]\tLoss: 0.121832\t Accuracy:90.255%\n",
      "Rate : 80 [45440/49000 (93%)]\tLoss: 0.159425\t Accuracy:90.350%\n",
      "Rate : 80 [46080/49000 (94%)]\tLoss: 0.142641\t Accuracy:90.432%\n",
      "Rate : 80 [46720/49000 (95%)]\tLoss: 0.134484\t Accuracy:90.529%\n",
      "Rate : 80 [47360/49000 (97%)]\tLoss: 0.030743\t Accuracy:90.617%\n",
      "Rate : 80 [48000/49000 (98%)]\tLoss: 0.053810\t Accuracy:90.687%\n",
      "Rate : 80 [48640/49000 (99%)]\tLoss: 0.475366\t Accuracy:90.773%\n",
      "Rate : 90 [0/49000 (0%)]\tLoss: 4.763660\t Accuracy:25.000%\n",
      "Rate : 90 [640/49000 (1%)]\tLoss: 1.509925\t Accuracy:36.012%\n",
      "Rate : 90 [1280/49000 (3%)]\tLoss: 1.725719\t Accuracy:42.226%\n",
      "Rate : 90 [1920/49000 (4%)]\tLoss: 1.333055\t Accuracy:45.389%\n",
      "Rate : 90 [2560/49000 (5%)]\tLoss: 1.324467\t Accuracy:47.531%\n",
      "Rate : 90 [3200/49000 (7%)]\tLoss: 1.039575\t Accuracy:49.845%\n",
      "Rate : 90 [3840/49000 (8%)]\tLoss: 1.037479\t Accuracy:51.679%\n",
      "Rate : 90 [4480/49000 (9%)]\tLoss: 0.956076\t Accuracy:53.036%\n",
      "Rate : 90 [5120/49000 (10%)]\tLoss: 1.309318\t Accuracy:54.387%\n",
      "Rate : 90 [5760/49000 (12%)]\tLoss: 1.117176\t Accuracy:55.749%\n",
      "Rate : 90 [6400/49000 (13%)]\tLoss: 0.849653\t Accuracy:56.374%\n",
      "Rate : 90 [7040/49000 (14%)]\tLoss: 0.652163\t Accuracy:57.551%\n",
      "Rate : 90 [7680/49000 (16%)]\tLoss: 0.883602\t Accuracy:58.830%\n",
      "Rate : 90 [8320/49000 (17%)]\tLoss: 1.346743\t Accuracy:60.297%\n",
      "Rate : 90 [8960/49000 (18%)]\tLoss: 1.013648\t Accuracy:61.399%\n",
      "Rate : 90 [9600/49000 (20%)]\tLoss: 0.455099\t Accuracy:62.604%\n",
      "Rate : 90 [10240/49000 (21%)]\tLoss: 0.618384\t Accuracy:63.639%\n",
      "Rate : 90 [10880/49000 (22%)]\tLoss: 0.602877\t Accuracy:64.470%\n",
      "Rate : 90 [11520/49000 (23%)]\tLoss: 0.699575\t Accuracy:65.305%\n",
      "Rate : 90 [12160/49000 (25%)]\tLoss: 0.392426\t Accuracy:66.199%\n",
      "Rate : 90 [12800/49000 (26%)]\tLoss: 0.490063\t Accuracy:66.942%\n",
      "Rate : 90 [13440/49000 (27%)]\tLoss: 0.615786\t Accuracy:67.614%\n",
      "Rate : 90 [14080/49000 (29%)]\tLoss: 0.949783\t Accuracy:68.197%\n",
      "Rate : 90 [14720/49000 (30%)]\tLoss: 1.054325\t Accuracy:68.852%\n",
      "Rate : 90 [15360/49000 (31%)]\tLoss: 0.622867\t Accuracy:69.504%\n",
      "Rate : 90 [16000/49000 (33%)]\tLoss: 0.648963\t Accuracy:70.047%\n",
      "Rate : 90 [16640/49000 (34%)]\tLoss: 0.728607\t Accuracy:70.501%\n",
      "Rate : 90 [17280/49000 (35%)]\tLoss: 0.437275\t Accuracy:71.072%\n",
      "Rate : 90 [17920/49000 (37%)]\tLoss: 0.474839\t Accuracy:71.635%\n",
      "Rate : 90 [18560/49000 (38%)]\tLoss: 0.156438\t Accuracy:72.192%\n",
      "Rate : 90 [19200/49000 (39%)]\tLoss: 0.657896\t Accuracy:72.639%\n",
      "Rate : 90 [19840/49000 (40%)]\tLoss: 0.452061\t Accuracy:73.073%\n",
      "Rate : 90 [20480/49000 (42%)]\tLoss: 0.539908\t Accuracy:73.489%\n",
      "Rate : 90 [21120/49000 (43%)]\tLoss: 0.191545\t Accuracy:73.936%\n",
      "Rate : 90 [21760/49000 (44%)]\tLoss: 0.933401\t Accuracy:74.257%\n",
      "Rate : 90 [22400/49000 (46%)]\tLoss: 0.370492\t Accuracy:74.626%\n",
      "Rate : 90 [23040/49000 (47%)]\tLoss: 0.589887\t Accuracy:74.948%\n",
      "Rate : 90 [23680/49000 (48%)]\tLoss: 0.380932\t Accuracy:75.266%\n",
      "Rate : 90 [24320/49000 (50%)]\tLoss: 0.310712\t Accuracy:75.600%\n",
      "Rate : 90 [24960/49000 (51%)]\tLoss: 0.382679\t Accuracy:75.932%\n",
      "Rate : 90 [25600/49000 (52%)]\tLoss: 0.267143\t Accuracy:76.198%\n",
      "Rate : 90 [26240/49000 (54%)]\tLoss: 0.379162\t Accuracy:76.488%\n",
      "Rate : 90 [26880/49000 (55%)]\tLoss: 0.367720\t Accuracy:76.802%\n",
      "Rate : 90 [27520/49000 (56%)]\tLoss: 0.223333\t Accuracy:77.076%\n",
      "Rate : 90 [28160/49000 (57%)]\tLoss: 0.868487\t Accuracy:77.355%\n",
      "Rate : 90 [28800/49000 (59%)]\tLoss: 0.309216\t Accuracy:77.615%\n",
      "Rate : 90 [29440/49000 (60%)]\tLoss: 0.355735\t Accuracy:77.830%\n",
      "Rate : 90 [30080/49000 (61%)]\tLoss: 0.462471\t Accuracy:78.055%\n",
      "Rate : 90 [30720/49000 (63%)]\tLoss: 0.250925\t Accuracy:78.271%\n",
      "Rate : 90 [31360/49000 (64%)]\tLoss: 0.794815\t Accuracy:78.479%\n",
      "Rate : 90 [32000/49000 (65%)]\tLoss: 0.361424\t Accuracy:78.715%\n",
      "Rate : 90 [32640/49000 (67%)]\tLoss: 0.455752\t Accuracy:78.964%\n",
      "Rate : 90 [33280/49000 (68%)]\tLoss: 0.347107\t Accuracy:79.197%\n",
      "Rate : 90 [33920/49000 (69%)]\tLoss: 0.641900\t Accuracy:79.386%\n",
      "Rate : 90 [34560/49000 (70%)]\tLoss: 0.287809\t Accuracy:79.646%\n",
      "Rate : 90 [35200/49000 (72%)]\tLoss: 0.226019\t Accuracy:79.825%\n",
      "Rate : 90 [35840/49000 (73%)]\tLoss: 0.247140\t Accuracy:80.057%\n",
      "Rate : 90 [36480/49000 (74%)]\tLoss: 0.199934\t Accuracy:80.248%\n",
      "Rate : 90 [37120/49000 (76%)]\tLoss: 0.361132\t Accuracy:80.432%\n",
      "Rate : 90 [37760/49000 (77%)]\tLoss: 0.556640\t Accuracy:80.588%\n",
      "Rate : 90 [38400/49000 (78%)]\tLoss: 0.356060\t Accuracy:80.769%\n",
      "Rate : 90 [39040/49000 (80%)]\tLoss: 0.216945\t Accuracy:80.912%\n",
      "Rate : 90 [39680/49000 (81%)]\tLoss: 0.296707\t Accuracy:81.079%\n",
      "Rate : 90 [40320/49000 (82%)]\tLoss: 0.199486\t Accuracy:81.267%\n",
      "Rate : 90 [40960/49000 (84%)]\tLoss: 0.282417\t Accuracy:81.428%\n",
      "Rate : 90 [41600/49000 (85%)]\tLoss: 0.185148\t Accuracy:81.613%\n",
      "Rate : 90 [42240/49000 (86%)]\tLoss: 0.095704\t Accuracy:81.770%\n",
      "Rate : 90 [42880/49000 (87%)]\tLoss: 0.287323\t Accuracy:81.912%\n",
      "Rate : 90 [43520/49000 (89%)]\tLoss: 0.243490\t Accuracy:82.058%\n",
      "Rate : 90 [44160/49000 (90%)]\tLoss: 0.342844\t Accuracy:82.200%\n",
      "Rate : 90 [44800/49000 (91%)]\tLoss: 0.280509\t Accuracy:82.321%\n",
      "Rate : 90 [45440/49000 (93%)]\tLoss: 0.265215\t Accuracy:82.471%\n",
      "Rate : 90 [46080/49000 (94%)]\tLoss: 0.167264\t Accuracy:82.623%\n",
      "Rate : 90 [46720/49000 (95%)]\tLoss: 0.144542\t Accuracy:82.792%\n",
      "Rate : 90 [47360/49000 (97%)]\tLoss: 0.080954\t Accuracy:82.911%\n",
      "Rate : 90 [48000/49000 (98%)]\tLoss: 0.288730\t Accuracy:83.028%\n",
      "Rate : 90 [48640/49000 (99%)]\tLoss: 0.259485\t Accuracy:83.171%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "pruned_model_90 = smart_prune(acmlp, train_loader, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.921% \n"
     ]
    }
   ],
   "source": [
    "evaluate(pruned_model_90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22200"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_pruned_weights(pruned_model_90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выкинув большую часть сети, мы все еще имеем относительно неплохое качество, хоть и меньше, чем изначально. \n",
    "\n",
    "Вполне возможно проблема в том, что мы слишком агрессивно удаляем связи, когда их остается совсем мало. Давайте попробуем более аккуратные шаги.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def smart_prune_shed(model, train_loader, schedule):\n",
    "    # Создаем именно новую модель, старую не трогаем\n",
    "    model = copy.deepcopy(model)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    error = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    \n",
    "    for rate, epochs in schedule:  # Идем шагами, согласно тому расписанию, которое передали в функцию\n",
    "        t = calc_threshhold(model, float(rate))  # Считаем очередное пороговое значение\n",
    "        model.prune(t)  # Отключаем слабые связи\n",
    "        for i in range(epochs):\n",
    "            correct = 0\n",
    "            for batch_idx, (X_batch, y_batch) in enumerate(train_loader):  # Далее дообучаем модель как обычно в течение указанного количества эпох\n",
    "                var_X_batch = Variable(X_batch).float()\n",
    "                var_y_batch = Variable(y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(var_X_batch)\n",
    "                loss = error(output, var_y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                predicted = torch.max(output.data, 1)[1] \n",
    "                correct += (predicted == var_y_batch).sum()\n",
    "                if batch_idx % 20 == 0:\n",
    "                    print('Rate : {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Accuracy:{:.3f}%'.format(\n",
    "                        rate, batch_idx*len(X_batch), len(train_loader.dataset), 100.*batch_idx / len(train_loader), loss.data, float(correct*100) / float(BATCH_SIZE*(batch_idx+1))))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate : 0 [0/49000 (0%)]\tLoss: 0.005384\t Accuracy:100.000%\n",
      "Rate : 0 [640/49000 (1%)]\tLoss: 0.000911\t Accuracy:98.065%\n",
      "Rate : 0 [1280/49000 (3%)]\tLoss: 0.202851\t Accuracy:97.485%\n",
      "Rate : 0 [1920/49000 (4%)]\tLoss: 0.000099\t Accuracy:97.387%\n",
      "Rate : 0 [2560/49000 (5%)]\tLoss: 0.094035\t Accuracy:97.106%\n",
      "Rate : 0 [3200/49000 (7%)]\tLoss: 0.001190\t Accuracy:97.308%\n",
      "Rate : 0 [3840/49000 (8%)]\tLoss: 0.108540\t Accuracy:97.521%\n",
      "Rate : 0 [4480/49000 (9%)]\tLoss: 0.001416\t Accuracy:97.429%\n",
      "Rate : 0 [5120/49000 (10%)]\tLoss: 0.439223\t Accuracy:97.380%\n",
      "Rate : 0 [5760/49000 (12%)]\tLoss: 0.401924\t Accuracy:97.341%\n",
      "Rate : 0 [6400/49000 (13%)]\tLoss: 0.012014\t Accuracy:97.373%\n",
      "Rate : 0 [7040/49000 (14%)]\tLoss: 0.074397\t Accuracy:97.243%\n",
      "Rate : 0 [7680/49000 (16%)]\tLoss: 0.016140\t Accuracy:97.264%\n",
      "Rate : 0 [8320/49000 (17%)]\tLoss: 0.229941\t Accuracy:97.246%\n",
      "Rate : 0 [8960/49000 (18%)]\tLoss: 0.735903\t Accuracy:97.198%\n",
      "Rate : 0 [9600/49000 (20%)]\tLoss: 0.063619\t Accuracy:97.124%\n",
      "Rate : 0 [10240/49000 (21%)]\tLoss: 0.115830\t Accuracy:97.099%\n",
      "Rate : 0 [10880/49000 (22%)]\tLoss: 0.021085\t Accuracy:97.141%\n",
      "Rate : 0 [11520/49000 (23%)]\tLoss: 0.159215\t Accuracy:97.143%\n",
      "Rate : 0 [12160/49000 (25%)]\tLoss: 0.069184\t Accuracy:97.211%\n",
      "Rate : 0 [12800/49000 (26%)]\tLoss: 0.052835\t Accuracy:97.210%\n",
      "Rate : 0 [13440/49000 (27%)]\tLoss: 0.017183\t Accuracy:97.179%\n",
      "Rate : 0 [14080/49000 (29%)]\tLoss: 0.085354\t Accuracy:97.215%\n",
      "Rate : 0 [14720/49000 (30%)]\tLoss: 0.299169\t Accuracy:97.255%\n",
      "Rate : 0 [15360/49000 (31%)]\tLoss: 0.124858\t Accuracy:97.245%\n",
      "Rate : 0 [16000/49000 (33%)]\tLoss: 0.083203\t Accuracy:97.206%\n",
      "Rate : 0 [16640/49000 (34%)]\tLoss: 0.091071\t Accuracy:97.175%\n",
      "Rate : 0 [17280/49000 (35%)]\tLoss: 0.039193\t Accuracy:97.181%\n",
      "Rate : 0 [17920/49000 (37%)]\tLoss: 0.000847\t Accuracy:97.176%\n",
      "Rate : 0 [18560/49000 (38%)]\tLoss: 0.019564\t Accuracy:97.160%\n",
      "Rate : 0 [19200/49000 (39%)]\tLoss: 0.357282\t Accuracy:97.140%\n",
      "Rate : 0 [19840/49000 (40%)]\tLoss: 0.050772\t Accuracy:97.152%\n",
      "Rate : 0 [20480/49000 (42%)]\tLoss: 0.126184\t Accuracy:97.158%\n",
      "Rate : 0 [21120/49000 (43%)]\tLoss: 0.505292\t Accuracy:97.168%\n",
      "Rate : 0 [21760/49000 (44%)]\tLoss: 0.658211\t Accuracy:97.127%\n",
      "Rate : 0 [22400/49000 (46%)]\tLoss: 0.225346\t Accuracy:97.116%\n",
      "Rate : 0 [23040/49000 (47%)]\tLoss: 0.189000\t Accuracy:97.122%\n",
      "Rate : 0 [23680/49000 (48%)]\tLoss: 0.019297\t Accuracy:97.107%\n",
      "Rate : 0 [24320/49000 (50%)]\tLoss: 0.019447\t Accuracy:97.125%\n",
      "Rate : 0 [24960/49000 (51%)]\tLoss: 0.183461\t Accuracy:97.091%\n",
      "Rate : 0 [25600/49000 (52%)]\tLoss: 0.047212\t Accuracy:97.047%\n",
      "Rate : 0 [26240/49000 (54%)]\tLoss: 0.038848\t Accuracy:97.058%\n",
      "Rate : 0 [26880/49000 (55%)]\tLoss: 0.020028\t Accuracy:97.050%\n",
      "Rate : 0 [27520/49000 (56%)]\tLoss: 0.009679\t Accuracy:97.071%\n",
      "Rate : 0 [28160/49000 (57%)]\tLoss: 0.133008\t Accuracy:97.088%\n",
      "Rate : 0 [28800/49000 (59%)]\tLoss: 0.090423\t Accuracy:97.073%\n",
      "Rate : 0 [29440/49000 (60%)]\tLoss: 0.233208\t Accuracy:97.075%\n",
      "Rate : 0 [30080/49000 (61%)]\tLoss: 0.080634\t Accuracy:97.051%\n",
      "Rate : 0 [30720/49000 (63%)]\tLoss: 0.004145\t Accuracy:97.028%\n",
      "Rate : 0 [31360/49000 (64%)]\tLoss: 0.144451\t Accuracy:97.025%\n",
      "Rate : 0 [32000/49000 (65%)]\tLoss: 0.010738\t Accuracy:97.050%\n",
      "Rate : 0 [32640/49000 (67%)]\tLoss: 0.074914\t Accuracy:97.059%\n",
      "Rate : 0 [33280/49000 (68%)]\tLoss: 0.006358\t Accuracy:97.049%\n",
      "Rate : 0 [33920/49000 (69%)]\tLoss: 0.137165\t Accuracy:97.040%\n",
      "Rate : 0 [34560/49000 (70%)]\tLoss: 0.357986\t Accuracy:97.063%\n",
      "Rate : 0 [35200/49000 (72%)]\tLoss: 0.000866\t Accuracy:97.068%\n",
      "Rate : 0 [35840/49000 (73%)]\tLoss: 0.069342\t Accuracy:97.059%\n",
      "Rate : 0 [36480/49000 (74%)]\tLoss: 0.192907\t Accuracy:97.069%\n",
      "Rate : 0 [37120/49000 (76%)]\tLoss: 0.176573\t Accuracy:97.074%\n",
      "Rate : 0 [37760/49000 (77%)]\tLoss: 0.067243\t Accuracy:97.071%\n",
      "Rate : 0 [38400/49000 (78%)]\tLoss: 0.107911\t Accuracy:97.094%\n",
      "Rate : 0 [39040/49000 (80%)]\tLoss: 0.178031\t Accuracy:97.062%\n",
      "Rate : 0 [39680/49000 (81%)]\tLoss: 0.024321\t Accuracy:97.076%\n",
      "Rate : 0 [40320/49000 (82%)]\tLoss: 0.004151\t Accuracy:97.096%\n",
      "Rate : 0 [40960/49000 (84%)]\tLoss: 0.061093\t Accuracy:97.080%\n",
      "Rate : 0 [41600/49000 (85%)]\tLoss: 0.110430\t Accuracy:97.094%\n",
      "Rate : 0 [42240/49000 (86%)]\tLoss: 0.176156\t Accuracy:97.093%\n",
      "Rate : 0 [42880/49000 (87%)]\tLoss: 0.014689\t Accuracy:97.094%\n",
      "Rate : 0 [43520/49000 (89%)]\tLoss: 0.051752\t Accuracy:97.093%\n",
      "Rate : 0 [44160/49000 (90%)]\tLoss: 0.072988\t Accuracy:97.085%\n",
      "Rate : 0 [44800/49000 (91%)]\tLoss: 0.339006\t Accuracy:97.087%\n",
      "Rate : 0 [45440/49000 (93%)]\tLoss: 0.122127\t Accuracy:97.093%\n",
      "Rate : 0 [46080/49000 (94%)]\tLoss: 0.102839\t Accuracy:97.085%\n",
      "Rate : 0 [46720/49000 (95%)]\tLoss: 0.075276\t Accuracy:97.089%\n",
      "Rate : 0 [47360/49000 (97%)]\tLoss: 0.032275\t Accuracy:97.092%\n",
      "Rate : 0 [48000/49000 (98%)]\tLoss: 0.061910\t Accuracy:97.087%\n",
      "Rate : 0 [48640/49000 (99%)]\tLoss: 0.001413\t Accuracy:97.107%\n",
      "Rate : 40 [0/49000 (0%)]\tLoss: 0.001932\t Accuracy:100.000%\n",
      "Rate : 40 [640/49000 (1%)]\tLoss: 0.180499\t Accuracy:97.619%\n",
      "Rate : 40 [1280/49000 (3%)]\tLoss: 0.289414\t Accuracy:97.790%\n",
      "Rate : 40 [1920/49000 (4%)]\tLoss: 0.000322\t Accuracy:97.746%\n",
      "Rate : 40 [2560/49000 (5%)]\tLoss: 0.055385\t Accuracy:97.685%\n",
      "Rate : 40 [3200/49000 (7%)]\tLoss: 0.015353\t Accuracy:97.803%\n",
      "Rate : 40 [3840/49000 (8%)]\tLoss: 0.063525\t Accuracy:97.908%\n",
      "Rate : 40 [4480/49000 (9%)]\tLoss: 0.083426\t Accuracy:97.828%\n",
      "Rate : 40 [5120/49000 (10%)]\tLoss: 0.353994\t Accuracy:97.787%\n",
      "Rate : 40 [5760/49000 (12%)]\tLoss: 0.078171\t Accuracy:97.911%\n",
      "Rate : 40 [6400/49000 (13%)]\tLoss: 0.091295\t Accuracy:97.963%\n",
      "Rate : 40 [7040/49000 (14%)]\tLoss: 0.128787\t Accuracy:97.950%\n",
      "Rate : 40 [7680/49000 (16%)]\tLoss: 0.009378\t Accuracy:97.860%\n",
      "Rate : 40 [8320/49000 (17%)]\tLoss: 0.119961\t Accuracy:97.809%\n",
      "Rate : 40 [8960/49000 (18%)]\tLoss: 0.138516\t Accuracy:97.843%\n",
      "Rate : 40 [9600/49000 (20%)]\tLoss: 0.004629\t Accuracy:97.799%\n",
      "Rate : 40 [10240/49000 (21%)]\tLoss: 0.002690\t Accuracy:97.819%\n",
      "Rate : 40 [10880/49000 (22%)]\tLoss: 0.126544\t Accuracy:97.791%\n",
      "Rate : 40 [11520/49000 (23%)]\tLoss: 0.004661\t Accuracy:97.775%\n",
      "Rate : 40 [12160/49000 (25%)]\tLoss: 0.007179\t Accuracy:97.777%\n",
      "Rate : 40 [12800/49000 (26%)]\tLoss: 0.083343\t Accuracy:97.779%\n",
      "Rate : 40 [13440/49000 (27%)]\tLoss: 0.008818\t Accuracy:97.781%\n",
      "Rate : 40 [14080/49000 (29%)]\tLoss: 0.033889\t Accuracy:97.782%\n",
      "Rate : 40 [14720/49000 (30%)]\tLoss: 0.104338\t Accuracy:97.817%\n",
      "Rate : 40 [15360/49000 (31%)]\tLoss: 0.154413\t Accuracy:97.817%\n",
      "Rate : 40 [16000/49000 (33%)]\tLoss: 0.018490\t Accuracy:97.786%\n",
      "Rate : 40 [16640/49000 (34%)]\tLoss: 0.019554\t Accuracy:97.781%\n",
      "Rate : 40 [17280/49000 (35%)]\tLoss: 0.001600\t Accuracy:97.805%\n",
      "Rate : 40 [17920/49000 (37%)]\tLoss: 0.014970\t Accuracy:97.811%\n",
      "Rate : 40 [18560/49000 (38%)]\tLoss: 0.001327\t Accuracy:97.806%\n",
      "Rate : 40 [19200/49000 (39%)]\tLoss: 0.304065\t Accuracy:97.790%\n",
      "Rate : 40 [19840/49000 (40%)]\tLoss: 0.127873\t Accuracy:97.791%\n",
      "Rate : 40 [20480/49000 (42%)]\tLoss: 0.016888\t Accuracy:97.806%\n",
      "Rate : 40 [21120/49000 (43%)]\tLoss: 0.112701\t Accuracy:97.792%\n",
      "Rate : 40 [21760/49000 (44%)]\tLoss: 0.234158\t Accuracy:97.784%\n",
      "Rate : 40 [22400/49000 (46%)]\tLoss: 0.426510\t Accuracy:97.744%\n",
      "Rate : 40 [23040/49000 (47%)]\tLoss: 0.166559\t Accuracy:97.733%\n",
      "Rate : 40 [23680/49000 (48%)]\tLoss: 0.244033\t Accuracy:97.710%\n",
      "Rate : 40 [24320/49000 (50%)]\tLoss: 0.024648\t Accuracy:97.717%\n",
      "Rate : 40 [24960/49000 (51%)]\tLoss: 0.149427\t Accuracy:97.695%\n",
      "Rate : 40 [25600/49000 (52%)]\tLoss: 0.018752\t Accuracy:97.686%\n",
      "Rate : 40 [26240/49000 (54%)]\tLoss: 0.086855\t Accuracy:97.693%\n",
      "Rate : 40 [26880/49000 (55%)]\tLoss: 0.125196\t Accuracy:97.692%\n",
      "Rate : 40 [27520/49000 (56%)]\tLoss: 0.000350\t Accuracy:97.684%\n",
      "Rate : 40 [28160/49000 (57%)]\tLoss: 0.217715\t Accuracy:97.687%\n",
      "Rate : 40 [28800/49000 (59%)]\tLoss: 0.046823\t Accuracy:97.700%\n",
      "Rate : 40 [29440/49000 (60%)]\tLoss: 0.031905\t Accuracy:97.710%\n",
      "Rate : 40 [30080/49000 (61%)]\tLoss: 0.023836\t Accuracy:97.722%\n",
      "Rate : 40 [30720/49000 (63%)]\tLoss: 0.002792\t Accuracy:97.714%\n",
      "Rate : 40 [31360/49000 (64%)]\tLoss: 0.332406\t Accuracy:97.735%\n",
      "Rate : 40 [32000/49000 (65%)]\tLoss: 0.001015\t Accuracy:97.743%\n",
      "Rate : 40 [32640/49000 (67%)]\tLoss: 0.039915\t Accuracy:97.750%\n",
      "Rate : 40 [33280/49000 (68%)]\tLoss: 0.004935\t Accuracy:97.752%\n",
      "Rate : 40 [33920/49000 (69%)]\tLoss: 0.031338\t Accuracy:97.741%\n",
      "Rate : 40 [34560/49000 (70%)]\tLoss: 0.236748\t Accuracy:97.736%\n",
      "Rate : 40 [35200/49000 (72%)]\tLoss: 0.001147\t Accuracy:97.735%\n",
      "Rate : 40 [35840/49000 (73%)]\tLoss: 0.077765\t Accuracy:97.750%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate : 40 [36480/49000 (74%)]\tLoss: 0.004357\t Accuracy:97.771%\n",
      "Rate : 40 [37120/49000 (76%)]\tLoss: 0.001037\t Accuracy:97.787%\n",
      "Rate : 40 [37760/49000 (77%)]\tLoss: 0.034745\t Accuracy:97.806%\n",
      "Rate : 40 [38400/49000 (78%)]\tLoss: 0.030201\t Accuracy:97.814%\n",
      "Rate : 40 [39040/49000 (80%)]\tLoss: 0.054389\t Accuracy:97.801%\n",
      "Rate : 40 [39680/49000 (81%)]\tLoss: 0.028741\t Accuracy:97.804%\n",
      "Rate : 40 [40320/49000 (82%)]\tLoss: 0.000039\t Accuracy:97.817%\n",
      "Rate : 40 [40960/49000 (84%)]\tLoss: 0.003018\t Accuracy:97.824%\n",
      "Rate : 40 [41600/49000 (85%)]\tLoss: 0.000240\t Accuracy:97.836%\n",
      "Rate : 40 [42240/49000 (86%)]\tLoss: 0.000264\t Accuracy:97.845%\n",
      "Rate : 40 [42880/49000 (87%)]\tLoss: 0.270638\t Accuracy:97.847%\n",
      "Rate : 40 [43520/49000 (89%)]\tLoss: 0.178158\t Accuracy:97.853%\n",
      "Rate : 40 [44160/49000 (90%)]\tLoss: 0.007313\t Accuracy:97.859%\n",
      "Rate : 40 [44800/49000 (91%)]\tLoss: 0.064545\t Accuracy:97.863%\n",
      "Rate : 40 [45440/49000 (93%)]\tLoss: 0.035238\t Accuracy:97.867%\n",
      "Rate : 40 [46080/49000 (94%)]\tLoss: 0.001478\t Accuracy:97.881%\n",
      "Rate : 40 [46720/49000 (95%)]\tLoss: 0.007141\t Accuracy:97.889%\n",
      "Rate : 40 [47360/49000 (97%)]\tLoss: 0.002586\t Accuracy:97.898%\n",
      "Rate : 40 [48000/49000 (98%)]\tLoss: 0.066968\t Accuracy:97.901%\n",
      "Rate : 40 [48640/49000 (99%)]\tLoss: 0.052237\t Accuracy:97.913%\n",
      "Rate : 50 [0/49000 (0%)]\tLoss: 0.002307\t Accuracy:100.000%\n",
      "Rate : 50 [640/49000 (1%)]\tLoss: 0.000632\t Accuracy:98.661%\n",
      "Rate : 50 [1280/49000 (3%)]\tLoss: 0.175983\t Accuracy:97.790%\n",
      "Rate : 50 [1920/49000 (4%)]\tLoss: 0.000719\t Accuracy:97.797%\n",
      "Rate : 50 [2560/49000 (5%)]\tLoss: 0.016690\t Accuracy:97.685%\n",
      "Rate : 50 [3200/49000 (7%)]\tLoss: 0.071365\t Accuracy:97.865%\n",
      "Rate : 50 [3840/49000 (8%)]\tLoss: 0.021236\t Accuracy:97.934%\n",
      "Rate : 50 [4480/49000 (9%)]\tLoss: 0.069144\t Accuracy:98.072%\n",
      "Rate : 50 [5120/49000 (10%)]\tLoss: 0.094462\t Accuracy:98.098%\n",
      "Rate : 50 [5760/49000 (12%)]\tLoss: 0.004933\t Accuracy:98.222%\n",
      "Rate : 50 [6400/49000 (13%)]\tLoss: 0.010253\t Accuracy:98.305%\n",
      "Rate : 50 [7040/49000 (14%)]\tLoss: 0.065501\t Accuracy:98.275%\n",
      "Rate : 50 [7680/49000 (16%)]\tLoss: 0.188943\t Accuracy:98.211%\n",
      "Rate : 50 [8320/49000 (17%)]\tLoss: 0.075650\t Accuracy:98.204%\n",
      "Rate : 50 [8960/49000 (18%)]\tLoss: 0.000495\t Accuracy:98.254%\n",
      "Rate : 50 [9600/49000 (20%)]\tLoss: 0.000148\t Accuracy:98.277%\n",
      "Rate : 50 [10240/49000 (21%)]\tLoss: 0.001515\t Accuracy:98.287%\n",
      "Rate : 50 [10880/49000 (22%)]\tLoss: 0.113317\t Accuracy:98.286%\n",
      "Rate : 50 [11520/49000 (23%)]\tLoss: 0.088188\t Accuracy:98.277%\n",
      "Rate : 50 [12160/49000 (25%)]\tLoss: 0.017296\t Accuracy:98.278%\n",
      "Rate : 50 [12800/49000 (26%)]\tLoss: 0.068872\t Accuracy:98.247%\n",
      "Rate : 50 [13440/49000 (27%)]\tLoss: 0.009185\t Accuracy:98.293%\n",
      "Rate : 50 [14080/49000 (29%)]\tLoss: 0.471054\t Accuracy:98.250%\n",
      "Rate : 50 [14720/49000 (30%)]\tLoss: 0.018837\t Accuracy:98.305%\n",
      "Rate : 50 [15360/49000 (31%)]\tLoss: 0.028628\t Accuracy:98.298%\n",
      "Rate : 50 [16000/49000 (33%)]\tLoss: 0.054235\t Accuracy:98.266%\n",
      "Rate : 50 [16640/49000 (34%)]\tLoss: 0.039075\t Accuracy:98.267%\n",
      "Rate : 50 [17280/49000 (35%)]\tLoss: 0.001265\t Accuracy:98.267%\n",
      "Rate : 50 [17920/49000 (37%)]\tLoss: 0.000520\t Accuracy:98.251%\n",
      "Rate : 50 [18560/49000 (38%)]\tLoss: 0.000429\t Accuracy:98.247%\n",
      "Rate : 50 [19200/49000 (39%)]\tLoss: 0.221032\t Accuracy:98.243%\n",
      "Rate : 50 [19840/49000 (40%)]\tLoss: 0.126699\t Accuracy:98.269%\n",
      "Rate : 50 [20480/49000 (42%)]\tLoss: 0.064616\t Accuracy:98.264%\n",
      "Rate : 50 [21120/49000 (43%)]\tLoss: 0.118617\t Accuracy:98.260%\n",
      "Rate : 50 [21760/49000 (44%)]\tLoss: 0.225492\t Accuracy:98.242%\n",
      "Rate : 50 [22400/49000 (46%)]\tLoss: 0.090510\t Accuracy:98.226%\n",
      "Rate : 50 [23040/49000 (47%)]\tLoss: 0.243152\t Accuracy:98.197%\n",
      "Rate : 50 [23680/49000 (48%)]\tLoss: 0.303397\t Accuracy:98.187%\n",
      "Rate : 50 [24320/49000 (50%)]\tLoss: 0.103350\t Accuracy:98.189%\n",
      "Rate : 50 [24960/49000 (51%)]\tLoss: 0.005789\t Accuracy:98.171%\n",
      "Rate : 50 [25600/49000 (52%)]\tLoss: 0.031377\t Accuracy:98.135%\n",
      "Rate : 50 [26240/49000 (54%)]\tLoss: 0.012530\t Accuracy:98.131%\n",
      "Rate : 50 [26880/49000 (55%)]\tLoss: 0.109605\t Accuracy:98.131%\n",
      "Rate : 50 [27520/49000 (56%)]\tLoss: 0.001033\t Accuracy:98.127%\n",
      "Rate : 50 [28160/49000 (57%)]\tLoss: 0.150586\t Accuracy:98.138%\n",
      "Rate : 50 [28800/49000 (59%)]\tLoss: 0.004489\t Accuracy:98.144%\n",
      "Rate : 50 [29440/49000 (60%)]\tLoss: 0.159385\t Accuracy:98.117%\n",
      "Rate : 50 [30080/49000 (61%)]\tLoss: 0.057941\t Accuracy:98.120%\n",
      "Rate : 50 [30720/49000 (63%)]\tLoss: 0.010424\t Accuracy:98.120%\n",
      "Rate : 50 [31360/49000 (64%)]\tLoss: 0.014191\t Accuracy:98.127%\n",
      "Rate : 50 [32000/49000 (65%)]\tLoss: 0.004279\t Accuracy:98.146%\n",
      "Rate : 50 [32640/49000 (67%)]\tLoss: 0.004474\t Accuracy:98.157%\n",
      "Rate : 50 [33280/49000 (68%)]\tLoss: 0.000215\t Accuracy:98.172%\n",
      "Rate : 50 [33920/49000 (69%)]\tLoss: 0.031899\t Accuracy:98.189%\n",
      "Rate : 50 [34560/49000 (70%)]\tLoss: 0.196492\t Accuracy:98.205%\n",
      "Rate : 50 [35200/49000 (72%)]\tLoss: 0.002279\t Accuracy:98.212%\n",
      "Rate : 50 [35840/49000 (73%)]\tLoss: 0.008138\t Accuracy:98.213%\n",
      "Rate : 50 [36480/49000 (74%)]\tLoss: 0.020296\t Accuracy:98.209%\n",
      "Rate : 50 [37120/49000 (76%)]\tLoss: 0.019159\t Accuracy:98.218%\n",
      "Rate : 50 [37760/49000 (77%)]\tLoss: 0.013347\t Accuracy:98.217%\n",
      "Rate : 50 [38400/49000 (78%)]\tLoss: 0.153306\t Accuracy:98.233%\n",
      "Rate : 50 [39040/49000 (80%)]\tLoss: 0.014298\t Accuracy:98.224%\n",
      "Rate : 50 [39680/49000 (81%)]\tLoss: 0.001675\t Accuracy:98.232%\n",
      "Rate : 50 [40320/49000 (82%)]\tLoss: 0.000942\t Accuracy:98.250%\n",
      "Rate : 50 [40960/49000 (84%)]\tLoss: 0.046765\t Accuracy:98.244%\n",
      "Rate : 50 [41600/49000 (85%)]\tLoss: 0.000015\t Accuracy:98.244%\n",
      "Rate : 50 [42240/49000 (86%)]\tLoss: 0.000018\t Accuracy:98.254%\n",
      "Rate : 50 [42880/49000 (87%)]\tLoss: 0.026560\t Accuracy:98.266%\n",
      "Rate : 50 [43520/49000 (89%)]\tLoss: 0.084576\t Accuracy:98.271%\n",
      "Rate : 50 [44160/49000 (90%)]\tLoss: 0.004526\t Accuracy:98.264%\n",
      "Rate : 50 [44800/49000 (91%)]\tLoss: 0.037883\t Accuracy:98.274%\n",
      "Rate : 50 [45440/49000 (93%)]\tLoss: 0.034745\t Accuracy:98.278%\n",
      "Rate : 50 [46080/49000 (94%)]\tLoss: 0.000221\t Accuracy:98.293%\n",
      "Rate : 50 [46720/49000 (95%)]\tLoss: 0.024380\t Accuracy:98.295%\n",
      "Rate : 50 [47360/49000 (97%)]\tLoss: 0.000680\t Accuracy:98.304%\n",
      "Rate : 50 [48000/49000 (98%)]\tLoss: 0.003068\t Accuracy:98.301%\n",
      "Rate : 50 [48640/49000 (99%)]\tLoss: 0.014563\t Accuracy:98.309%\n",
      "Rate : 60 [0/49000 (0%)]\tLoss: 0.044862\t Accuracy:96.875%\n",
      "Rate : 60 [640/49000 (1%)]\tLoss: 0.033235\t Accuracy:98.958%\n",
      "Rate : 60 [1280/49000 (3%)]\tLoss: 0.071962\t Accuracy:97.866%\n",
      "Rate : 60 [1920/49000 (4%)]\tLoss: 0.000531\t Accuracy:98.002%\n",
      "Rate : 60 [2560/49000 (5%)]\tLoss: 0.006013\t Accuracy:98.187%\n",
      "Rate : 60 [3200/49000 (7%)]\tLoss: 0.018427\t Accuracy:98.360%\n",
      "Rate : 60 [3840/49000 (8%)]\tLoss: 0.018789\t Accuracy:98.476%\n",
      "Rate : 60 [4480/49000 (9%)]\tLoss: 0.062938\t Accuracy:98.449%\n",
      "Rate : 60 [5120/49000 (10%)]\tLoss: 0.232549\t Accuracy:98.408%\n",
      "Rate : 60 [5760/49000 (12%)]\tLoss: 0.008321\t Accuracy:98.446%\n",
      "Rate : 60 [6400/49000 (13%)]\tLoss: 0.007504\t Accuracy:98.507%\n",
      "Rate : 60 [7040/49000 (14%)]\tLoss: 0.287688\t Accuracy:98.487%\n",
      "Rate : 60 [7680/49000 (16%)]\tLoss: 0.006984\t Accuracy:98.483%\n",
      "Rate : 60 [8320/49000 (17%)]\tLoss: 0.070516\t Accuracy:98.455%\n",
      "Rate : 60 [8960/49000 (18%)]\tLoss: 0.000594\t Accuracy:98.465%\n",
      "Rate : 60 [9600/49000 (20%)]\tLoss: 0.002053\t Accuracy:98.453%\n",
      "Rate : 60 [10240/49000 (21%)]\tLoss: 0.000405\t Accuracy:98.433%\n",
      "Rate : 60 [10880/49000 (22%)]\tLoss: 0.075121\t Accuracy:98.451%\n",
      "Rate : 60 [11520/49000 (23%)]\tLoss: 0.001052\t Accuracy:98.485%\n",
      "Rate : 60 [12160/49000 (25%)]\tLoss: 0.048210\t Accuracy:98.474%\n",
      "Rate : 60 [12800/49000 (26%)]\tLoss: 0.090808\t Accuracy:98.504%\n",
      "Rate : 60 [13440/49000 (27%)]\tLoss: 0.000238\t Accuracy:98.538%\n",
      "Rate : 60 [14080/49000 (29%)]\tLoss: 0.032569\t Accuracy:98.498%\n",
      "Rate : 60 [14720/49000 (30%)]\tLoss: 0.257356\t Accuracy:98.509%\n",
      "Rate : 60 [15360/49000 (31%)]\tLoss: 0.001166\t Accuracy:98.519%\n",
      "Rate : 60 [16000/49000 (33%)]\tLoss: 0.000885\t Accuracy:98.484%\n",
      "Rate : 60 [16640/49000 (34%)]\tLoss: 0.008694\t Accuracy:98.494%\n",
      "Rate : 60 [17280/49000 (35%)]\tLoss: 0.000197\t Accuracy:98.510%\n",
      "Rate : 60 [17920/49000 (37%)]\tLoss: 0.000497\t Accuracy:98.529%\n",
      "Rate : 60 [18560/49000 (38%)]\tLoss: 0.001047\t Accuracy:98.521%\n",
      "Rate : 60 [19200/49000 (39%)]\tLoss: 0.118787\t Accuracy:98.549%\n",
      "Rate : 60 [19840/49000 (40%)]\tLoss: 0.127705\t Accuracy:98.576%\n",
      "Rate : 60 [20480/49000 (42%)]\tLoss: 0.004189\t Accuracy:98.576%\n",
      "Rate : 60 [21120/49000 (43%)]\tLoss: 0.015477\t Accuracy:98.586%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate : 60 [21760/49000 (44%)]\tLoss: 0.073671\t Accuracy:98.587%\n",
      "Rate : 60 [22400/49000 (46%)]\tLoss: 0.081292\t Accuracy:98.587%\n",
      "Rate : 60 [23040/49000 (47%)]\tLoss: 0.091297\t Accuracy:98.574%\n",
      "Rate : 60 [23680/49000 (48%)]\tLoss: 0.059180\t Accuracy:98.570%\n",
      "Rate : 60 [24320/49000 (50%)]\tLoss: 0.047158\t Accuracy:98.591%\n",
      "Rate : 60 [24960/49000 (51%)]\tLoss: 0.002450\t Accuracy:98.596%\n",
      "Rate : 60 [25600/49000 (52%)]\tLoss: 0.001499\t Accuracy:98.599%\n",
      "Rate : 60 [26240/49000 (54%)]\tLoss: 0.010137\t Accuracy:98.622%\n",
      "Rate : 60 [26880/49000 (55%)]\tLoss: 0.030146\t Accuracy:98.618%\n",
      "Rate : 60 [27520/49000 (56%)]\tLoss: 0.000040\t Accuracy:98.624%\n",
      "Rate : 60 [28160/49000 (57%)]\tLoss: 0.105880\t Accuracy:98.627%\n",
      "Rate : 60 [28800/49000 (59%)]\tLoss: 0.005812\t Accuracy:98.627%\n",
      "Rate : 60 [29440/49000 (60%)]\tLoss: 0.015773\t Accuracy:98.612%\n",
      "Rate : 60 [30080/49000 (61%)]\tLoss: 0.013602\t Accuracy:98.609%\n",
      "Rate : 60 [30720/49000 (63%)]\tLoss: 0.000188\t Accuracy:98.611%\n",
      "Rate : 60 [31360/49000 (64%)]\tLoss: 0.024704\t Accuracy:98.617%\n",
      "Rate : 60 [32000/49000 (65%)]\tLoss: 0.000184\t Accuracy:98.626%\n",
      "Rate : 60 [32640/49000 (67%)]\tLoss: 0.071477\t Accuracy:98.620%\n",
      "Rate : 60 [33280/49000 (68%)]\tLoss: 0.003755\t Accuracy:98.613%\n",
      "Rate : 60 [33920/49000 (69%)]\tLoss: 0.062488\t Accuracy:98.607%\n",
      "Rate : 60 [34560/49000 (70%)]\tLoss: 0.044061\t Accuracy:98.607%\n",
      "Rate : 60 [35200/49000 (72%)]\tLoss: 0.000543\t Accuracy:98.592%\n",
      "Rate : 60 [35840/49000 (73%)]\tLoss: 0.003109\t Accuracy:98.589%\n",
      "Rate : 60 [36480/49000 (74%)]\tLoss: 0.039622\t Accuracy:98.600%\n",
      "Rate : 60 [37120/49000 (76%)]\tLoss: 0.000773\t Accuracy:98.603%\n",
      "Rate : 60 [37760/49000 (77%)]\tLoss: 0.007215\t Accuracy:98.608%\n",
      "Rate : 60 [38400/49000 (78%)]\tLoss: 0.104255\t Accuracy:98.605%\n",
      "Rate : 60 [39040/49000 (80%)]\tLoss: 0.006220\t Accuracy:98.597%\n",
      "Rate : 60 [39680/49000 (81%)]\tLoss: 0.000440\t Accuracy:98.610%\n",
      "Rate : 60 [40320/49000 (82%)]\tLoss: 0.000465\t Accuracy:98.622%\n",
      "Rate : 60 [40960/49000 (84%)]\tLoss: 0.000831\t Accuracy:98.605%\n",
      "Rate : 60 [41600/49000 (85%)]\tLoss: 0.000992\t Accuracy:98.616%\n",
      "Rate : 60 [42240/49000 (86%)]\tLoss: 0.000033\t Accuracy:98.623%\n",
      "Rate : 60 [42880/49000 (87%)]\tLoss: 0.061124\t Accuracy:98.620%\n",
      "Rate : 60 [43520/49000 (89%)]\tLoss: 0.016930\t Accuracy:98.618%\n",
      "Rate : 60 [44160/49000 (90%)]\tLoss: 0.035874\t Accuracy:98.615%\n",
      "Rate : 60 [44800/49000 (91%)]\tLoss: 0.145220\t Accuracy:98.619%\n",
      "Rate : 60 [45440/49000 (93%)]\tLoss: 0.080913\t Accuracy:98.623%\n",
      "Rate : 60 [46080/49000 (94%)]\tLoss: 0.007299\t Accuracy:98.629%\n",
      "Rate : 60 [46720/49000 (95%)]\tLoss: 0.000111\t Accuracy:98.640%\n",
      "Rate : 60 [47360/49000 (97%)]\tLoss: 0.001477\t Accuracy:98.650%\n",
      "Rate : 60 [48000/49000 (98%)]\tLoss: 0.029302\t Accuracy:98.651%\n",
      "Rate : 60 [48640/49000 (99%)]\tLoss: 0.008273\t Accuracy:98.662%\n",
      "Rate : 70 [0/49000 (0%)]\tLoss: 0.008830\t Accuracy:100.000%\n",
      "Rate : 70 [640/49000 (1%)]\tLoss: 0.004277\t Accuracy:96.875%\n",
      "Rate : 70 [1280/49000 (3%)]\tLoss: 0.167461\t Accuracy:96.951%\n",
      "Rate : 70 [1920/49000 (4%)]\tLoss: 0.003491\t Accuracy:97.131%\n",
      "Rate : 70 [2560/49000 (5%)]\tLoss: 0.052997\t Accuracy:97.106%\n",
      "Rate : 70 [3200/49000 (7%)]\tLoss: 0.032716\t Accuracy:97.401%\n",
      "Rate : 70 [3840/49000 (8%)]\tLoss: 0.162885\t Accuracy:97.495%\n",
      "Rate : 70 [4480/49000 (9%)]\tLoss: 0.010659\t Accuracy:97.562%\n",
      "Rate : 70 [5120/49000 (10%)]\tLoss: 0.107325\t Accuracy:97.574%\n",
      "Rate : 70 [5760/49000 (12%)]\tLoss: 0.191381\t Accuracy:97.669%\n",
      "Rate : 70 [6400/49000 (13%)]\tLoss: 0.006733\t Accuracy:97.699%\n",
      "Rate : 70 [7040/49000 (14%)]\tLoss: 0.062848\t Accuracy:97.667%\n",
      "Rate : 70 [7680/49000 (16%)]\tLoss: 0.161445\t Accuracy:97.679%\n",
      "Rate : 70 [8320/49000 (17%)]\tLoss: 0.199316\t Accuracy:97.701%\n",
      "Rate : 70 [8960/49000 (18%)]\tLoss: 0.055608\t Accuracy:97.720%\n",
      "Rate : 70 [9600/49000 (20%)]\tLoss: 0.011165\t Accuracy:97.726%\n",
      "Rate : 70 [10240/49000 (21%)]\tLoss: 0.000725\t Accuracy:97.810%\n",
      "Rate : 70 [10880/49000 (22%)]\tLoss: 0.006756\t Accuracy:97.791%\n",
      "Rate : 70 [11520/49000 (23%)]\tLoss: 0.007817\t Accuracy:97.836%\n",
      "Rate : 70 [12160/49000 (25%)]\tLoss: 0.006452\t Accuracy:97.876%\n",
      "Rate : 70 [12800/49000 (26%)]\tLoss: 0.076615\t Accuracy:97.865%\n",
      "Rate : 70 [13440/49000 (27%)]\tLoss: 0.012989\t Accuracy:97.870%\n",
      "Rate : 70 [14080/49000 (29%)]\tLoss: 0.084014\t Accuracy:97.895%\n",
      "Rate : 70 [14720/49000 (30%)]\tLoss: 0.250854\t Accuracy:97.939%\n",
      "Rate : 70 [15360/49000 (31%)]\tLoss: 0.001798\t Accuracy:97.979%\n",
      "Rate : 70 [16000/49000 (33%)]\tLoss: 0.003969\t Accuracy:97.973%\n",
      "Rate : 70 [16640/49000 (34%)]\tLoss: 0.252827\t Accuracy:97.967%\n",
      "Rate : 70 [17280/49000 (35%)]\tLoss: 0.003707\t Accuracy:98.001%\n",
      "Rate : 70 [17920/49000 (37%)]\tLoss: 0.003472\t Accuracy:98.011%\n",
      "Rate : 70 [18560/49000 (38%)]\tLoss: 0.002454\t Accuracy:98.010%\n",
      "Rate : 70 [19200/49000 (39%)]\tLoss: 0.322101\t Accuracy:98.014%\n",
      "Rate : 70 [19840/49000 (40%)]\tLoss: 0.168881\t Accuracy:98.037%\n",
      "Rate : 70 [20480/49000 (42%)]\tLoss: 0.017529\t Accuracy:98.050%\n",
      "Rate : 70 [21120/49000 (43%)]\tLoss: 0.071028\t Accuracy:98.038%\n",
      "Rate : 70 [21760/49000 (44%)]\tLoss: 0.070119\t Accuracy:98.031%\n",
      "Rate : 70 [22400/49000 (46%)]\tLoss: 0.057800\t Accuracy:98.039%\n",
      "Rate : 70 [23040/49000 (47%)]\tLoss: 0.203558\t Accuracy:98.024%\n",
      "Rate : 70 [23680/49000 (48%)]\tLoss: 0.005242\t Accuracy:98.052%\n",
      "Rate : 70 [24320/49000 (50%)]\tLoss: 0.015551\t Accuracy:98.054%\n",
      "Rate : 70 [24960/49000 (51%)]\tLoss: 0.073136\t Accuracy:98.055%\n",
      "Rate : 70 [25600/49000 (52%)]\tLoss: 0.112439\t Accuracy:98.061%\n",
      "Rate : 70 [26240/49000 (54%)]\tLoss: 0.112165\t Accuracy:98.066%\n",
      "Rate : 70 [26880/49000 (55%)]\tLoss: 0.000360\t Accuracy:98.060%\n",
      "Rate : 70 [27520/49000 (56%)]\tLoss: 0.000027\t Accuracy:98.080%\n",
      "Rate : 70 [28160/49000 (57%)]\tLoss: 0.097421\t Accuracy:98.109%\n",
      "Rate : 70 [28800/49000 (59%)]\tLoss: 0.219587\t Accuracy:98.120%\n",
      "Rate : 70 [29440/49000 (60%)]\tLoss: 0.000143\t Accuracy:98.120%\n",
      "Rate : 70 [30080/49000 (61%)]\tLoss: 0.130751\t Accuracy:98.130%\n",
      "Rate : 70 [30720/49000 (63%)]\tLoss: 0.000079\t Accuracy:98.137%\n",
      "Rate : 70 [31360/49000 (64%)]\tLoss: 0.254810\t Accuracy:98.133%\n",
      "Rate : 70 [32000/49000 (65%)]\tLoss: 0.002106\t Accuracy:98.161%\n",
      "Rate : 70 [32640/49000 (67%)]\tLoss: 0.063255\t Accuracy:98.179%\n",
      "Rate : 70 [33280/49000 (68%)]\tLoss: 0.001852\t Accuracy:98.181%\n",
      "Rate : 70 [33920/49000 (69%)]\tLoss: 0.007735\t Accuracy:98.203%\n",
      "Rate : 70 [34560/49000 (70%)]\tLoss: 0.001492\t Accuracy:98.211%\n",
      "Rate : 70 [35200/49000 (72%)]\tLoss: 0.008790\t Accuracy:98.223%\n",
      "Rate : 70 [35840/49000 (73%)]\tLoss: 0.062719\t Accuracy:98.224%\n",
      "Rate : 70 [36480/49000 (74%)]\tLoss: 0.012371\t Accuracy:98.236%\n",
      "Rate : 70 [37120/49000 (76%)]\tLoss: 0.007074\t Accuracy:98.253%\n",
      "Rate : 70 [37760/49000 (77%)]\tLoss: 0.005205\t Accuracy:98.262%\n",
      "Rate : 70 [38400/49000 (78%)]\tLoss: 0.042649\t Accuracy:98.275%\n",
      "Rate : 70 [39040/49000 (80%)]\tLoss: 0.007663\t Accuracy:98.270%\n",
      "Rate : 70 [39680/49000 (81%)]\tLoss: 0.000049\t Accuracy:98.285%\n",
      "Rate : 70 [40320/49000 (82%)]\tLoss: 0.000290\t Accuracy:98.305%\n",
      "Rate : 70 [40960/49000 (84%)]\tLoss: 0.000805\t Accuracy:98.312%\n",
      "Rate : 70 [41600/49000 (85%)]\tLoss: 0.132047\t Accuracy:98.331%\n",
      "Rate : 70 [42240/49000 (86%)]\tLoss: 0.000915\t Accuracy:98.339%\n",
      "Rate : 70 [42880/49000 (87%)]\tLoss: 0.019568\t Accuracy:98.350%\n",
      "Rate : 70 [43520/49000 (89%)]\tLoss: 0.012225\t Accuracy:98.356%\n",
      "Rate : 70 [44160/49000 (90%)]\tLoss: 0.003881\t Accuracy:98.359%\n",
      "Rate : 70 [44800/49000 (91%)]\tLoss: 0.032108\t Accuracy:98.358%\n",
      "Rate : 70 [45440/49000 (93%)]\tLoss: 0.020885\t Accuracy:98.366%\n",
      "Rate : 70 [46080/49000 (94%)]\tLoss: 0.000500\t Accuracy:98.371%\n",
      "Rate : 70 [46720/49000 (95%)]\tLoss: 0.018094\t Accuracy:98.377%\n",
      "Rate : 70 [47360/49000 (97%)]\tLoss: 0.000357\t Accuracy:98.377%\n",
      "Rate : 70 [48000/49000 (98%)]\tLoss: 0.007741\t Accuracy:98.374%\n",
      "Rate : 70 [48640/49000 (99%)]\tLoss: 0.000233\t Accuracy:98.379%\n",
      "Rate : 80 [0/49000 (0%)]\tLoss: 2.283103\t Accuracy:62.500%\n",
      "Rate : 80 [640/49000 (1%)]\tLoss: 0.529589\t Accuracy:79.464%\n",
      "Rate : 80 [1280/49000 (3%)]\tLoss: 0.866304\t Accuracy:83.232%\n",
      "Rate : 80 [1920/49000 (4%)]\tLoss: 0.183132\t Accuracy:85.400%\n",
      "Rate : 80 [2560/49000 (5%)]\tLoss: 0.304304\t Accuracy:86.535%\n",
      "Rate : 80 [3200/49000 (7%)]\tLoss: 0.419850\t Accuracy:87.686%\n",
      "Rate : 80 [3840/49000 (8%)]\tLoss: 0.139199\t Accuracy:88.301%\n",
      "Rate : 80 [4480/49000 (9%)]\tLoss: 0.159312\t Accuracy:88.763%\n",
      "Rate : 80 [5120/49000 (10%)]\tLoss: 0.395491\t Accuracy:89.480%\n",
      "Rate : 80 [5760/49000 (12%)]\tLoss: 0.566998\t Accuracy:90.142%\n",
      "Rate : 80 [6400/49000 (13%)]\tLoss: 0.072574\t Accuracy:90.672%\n",
      "Rate : 80 [7040/49000 (14%)]\tLoss: 0.049670\t Accuracy:91.035%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate : 80 [7680/49000 (16%)]\tLoss: 0.473037\t Accuracy:91.273%\n",
      "Rate : 80 [8320/49000 (17%)]\tLoss: 0.188372\t Accuracy:91.583%\n",
      "Rate : 80 [8960/49000 (18%)]\tLoss: 0.473066\t Accuracy:91.704%\n",
      "Rate : 80 [9600/49000 (20%)]\tLoss: 0.109149\t Accuracy:91.944%\n",
      "Rate : 80 [10240/49000 (21%)]\tLoss: 0.126885\t Accuracy:92.183%\n",
      "Rate : 80 [10880/49000 (22%)]\tLoss: 0.086109\t Accuracy:92.467%\n",
      "Rate : 80 [11520/49000 (23%)]\tLoss: 0.171338\t Accuracy:92.642%\n",
      "Rate : 80 [12160/49000 (25%)]\tLoss: 0.150549\t Accuracy:92.864%\n",
      "Rate : 80 [12800/49000 (26%)]\tLoss: 0.167667\t Accuracy:92.971%\n",
      "Rate : 80 [13440/49000 (27%)]\tLoss: 0.081468\t Accuracy:93.126%\n",
      "Rate : 80 [14080/49000 (29%)]\tLoss: 0.319829\t Accuracy:93.219%\n",
      "Rate : 80 [14720/49000 (30%)]\tLoss: 0.134296\t Accuracy:93.377%\n",
      "Rate : 80 [15360/49000 (31%)]\tLoss: 0.014747\t Accuracy:93.523%\n",
      "Rate : 80 [16000/49000 (33%)]\tLoss: 0.170293\t Accuracy:93.600%\n",
      "Rate : 80 [16640/49000 (34%)]\tLoss: 0.164924\t Accuracy:93.696%\n",
      "Rate : 80 [17280/49000 (35%)]\tLoss: 0.081706\t Accuracy:93.790%\n",
      "Rate : 80 [17920/49000 (37%)]\tLoss: 0.058962\t Accuracy:93.878%\n",
      "Rate : 80 [18560/49000 (38%)]\tLoss: 0.195577\t Accuracy:93.949%\n",
      "Rate : 80 [19200/49000 (39%)]\tLoss: 0.294442\t Accuracy:94.052%\n",
      "Rate : 80 [19840/49000 (40%)]\tLoss: 0.288969\t Accuracy:94.102%\n",
      "Rate : 80 [20480/49000 (42%)]\tLoss: 0.248065\t Accuracy:94.174%\n",
      "Rate : 80 [21120/49000 (43%)]\tLoss: 0.067179\t Accuracy:94.256%\n",
      "Rate : 80 [21760/49000 (44%)]\tLoss: 0.246761\t Accuracy:94.310%\n",
      "Rate : 80 [22400/49000 (46%)]\tLoss: 0.083529\t Accuracy:94.374%\n",
      "Rate : 80 [23040/49000 (47%)]\tLoss: 0.200343\t Accuracy:94.439%\n",
      "Rate : 80 [23680/49000 (48%)]\tLoss: 0.043531\t Accuracy:94.488%\n",
      "Rate : 80 [24320/49000 (50%)]\tLoss: 0.070945\t Accuracy:94.575%\n",
      "Rate : 80 [24960/49000 (51%)]\tLoss: 0.054011\t Accuracy:94.662%\n",
      "Rate : 80 [25600/49000 (52%)]\tLoss: 0.153368\t Accuracy:94.675%\n",
      "Rate : 80 [26240/49000 (54%)]\tLoss: 0.213915\t Accuracy:94.724%\n",
      "Rate : 80 [26880/49000 (55%)]\tLoss: 0.083710\t Accuracy:94.779%\n",
      "Rate : 80 [27520/49000 (56%)]\tLoss: 0.004037\t Accuracy:94.853%\n",
      "Rate : 80 [28160/49000 (57%)]\tLoss: 0.064346\t Accuracy:94.917%\n",
      "Rate : 80 [28800/49000 (59%)]\tLoss: 0.005161\t Accuracy:94.981%\n",
      "Rate : 80 [29440/49000 (60%)]\tLoss: 0.110221\t Accuracy:95.033%\n",
      "Rate : 80 [30080/49000 (61%)]\tLoss: 0.036140\t Accuracy:95.058%\n",
      "Rate : 80 [30720/49000 (63%)]\tLoss: 0.008477\t Accuracy:95.096%\n",
      "Rate : 80 [31360/49000 (64%)]\tLoss: 0.058691\t Accuracy:95.136%\n",
      "Rate : 80 [32000/49000 (65%)]\tLoss: 0.015572\t Accuracy:95.186%\n",
      "Rate : 80 [32640/49000 (67%)]\tLoss: 0.035307\t Accuracy:95.225%\n",
      "Rate : 80 [33280/49000 (68%)]\tLoss: 0.125419\t Accuracy:95.239%\n",
      "Rate : 80 [33920/49000 (69%)]\tLoss: 0.033146\t Accuracy:95.249%\n",
      "Rate : 80 [34560/49000 (70%)]\tLoss: 0.042356\t Accuracy:95.302%\n",
      "Rate : 80 [35200/49000 (72%)]\tLoss: 0.016218\t Accuracy:95.322%\n",
      "Rate : 80 [35840/49000 (73%)]\tLoss: 0.099413\t Accuracy:95.364%\n",
      "Rate : 80 [36480/49000 (74%)]\tLoss: 0.011341\t Accuracy:95.410%\n",
      "Rate : 80 [37120/49000 (76%)]\tLoss: 0.024164\t Accuracy:95.438%\n",
      "Rate : 80 [37760/49000 (77%)]\tLoss: 0.104475\t Accuracy:95.467%\n",
      "Rate : 80 [38400/49000 (78%)]\tLoss: 0.117640\t Accuracy:95.512%\n",
      "Rate : 80 [39040/49000 (80%)]\tLoss: 0.030905\t Accuracy:95.529%\n",
      "Rate : 80 [39680/49000 (81%)]\tLoss: 0.024613\t Accuracy:95.548%\n",
      "Rate : 80 [40320/49000 (82%)]\tLoss: 0.457513\t Accuracy:95.599%\n",
      "Rate : 80 [40960/49000 (84%)]\tLoss: 0.066722\t Accuracy:95.628%\n",
      "Rate : 80 [41600/49000 (85%)]\tLoss: 0.104039\t Accuracy:95.662%\n",
      "Rate : 80 [42240/49000 (86%)]\tLoss: 0.008434\t Accuracy:95.690%\n",
      "Rate : 80 [42880/49000 (87%)]\tLoss: 0.112831\t Accuracy:95.703%\n",
      "Rate : 80 [43520/49000 (89%)]\tLoss: 0.188300\t Accuracy:95.722%\n",
      "Rate : 80 [44160/49000 (90%)]\tLoss: 0.016019\t Accuracy:95.741%\n",
      "Rate : 80 [44800/49000 (91%)]\tLoss: 0.062674\t Accuracy:95.766%\n",
      "Rate : 80 [45440/49000 (93%)]\tLoss: 0.051944\t Accuracy:95.815%\n",
      "Rate : 80 [46080/49000 (94%)]\tLoss: 0.091152\t Accuracy:95.841%\n",
      "Rate : 80 [46720/49000 (95%)]\tLoss: 0.189457\t Accuracy:95.870%\n",
      "Rate : 80 [47360/49000 (97%)]\tLoss: 0.019513\t Accuracy:95.909%\n",
      "Rate : 80 [48000/49000 (98%)]\tLoss: 0.042991\t Accuracy:95.919%\n",
      "Rate : 80 [48640/49000 (99%)]\tLoss: 0.012703\t Accuracy:95.955%\n",
      "Rate : 80 [0/49000 (0%)]\tLoss: 0.157129\t Accuracy:96.875%\n",
      "Rate : 80 [640/49000 (1%)]\tLoss: 0.034713\t Accuracy:98.512%\n",
      "Rate : 80 [1280/49000 (3%)]\tLoss: 0.093553\t Accuracy:98.247%\n",
      "Rate : 80 [1920/49000 (4%)]\tLoss: 0.001763\t Accuracy:98.053%\n",
      "Rate : 80 [2560/49000 (5%)]\tLoss: 0.104911\t Accuracy:97.955%\n",
      "Rate : 80 [3200/49000 (7%)]\tLoss: 0.232340\t Accuracy:98.020%\n",
      "Rate : 80 [3840/49000 (8%)]\tLoss: 0.053075\t Accuracy:97.960%\n",
      "Rate : 80 [4480/49000 (9%)]\tLoss: 0.126308\t Accuracy:97.983%\n",
      "Rate : 80 [5120/49000 (10%)]\tLoss: 0.157369\t Accuracy:98.059%\n",
      "Rate : 80 [5760/49000 (12%)]\tLoss: 0.026885\t Accuracy:98.015%\n",
      "Rate : 80 [6400/49000 (13%)]\tLoss: 0.003185\t Accuracy:98.025%\n",
      "Rate : 80 [7040/49000 (14%)]\tLoss: 0.003803\t Accuracy:98.035%\n",
      "Rate : 80 [7680/49000 (16%)]\tLoss: 0.064842\t Accuracy:98.016%\n",
      "Rate : 80 [8320/49000 (17%)]\tLoss: 0.232898\t Accuracy:98.024%\n",
      "Rate : 80 [8960/49000 (18%)]\tLoss: 0.104404\t Accuracy:98.032%\n",
      "Rate : 80 [9600/49000 (20%)]\tLoss: 0.004908\t Accuracy:97.986%\n",
      "Rate : 80 [10240/49000 (21%)]\tLoss: 0.003798\t Accuracy:98.014%\n",
      "Rate : 80 [10880/49000 (22%)]\tLoss: 0.073916\t Accuracy:98.002%\n",
      "Rate : 80 [11520/49000 (23%)]\tLoss: 0.254009\t Accuracy:98.009%\n",
      "Rate : 80 [12160/49000 (25%)]\tLoss: 0.037591\t Accuracy:98.040%\n",
      "Rate : 80 [12800/49000 (26%)]\tLoss: 0.045133\t Accuracy:98.005%\n",
      "Rate : 80 [13440/49000 (27%)]\tLoss: 0.001604\t Accuracy:98.040%\n",
      "Rate : 80 [14080/49000 (29%)]\tLoss: 0.100678\t Accuracy:98.058%\n",
      "Rate : 80 [14720/49000 (30%)]\tLoss: 0.110098\t Accuracy:98.095%\n",
      "Rate : 80 [15360/49000 (31%)]\tLoss: 0.002438\t Accuracy:98.116%\n",
      "Rate : 80 [16000/49000 (33%)]\tLoss: 0.019762\t Accuracy:98.116%\n",
      "Rate : 80 [16640/49000 (34%)]\tLoss: 0.115688\t Accuracy:98.117%\n",
      "Rate : 80 [17280/49000 (35%)]\tLoss: 0.022467\t Accuracy:98.128%\n",
      "Rate : 80 [17920/49000 (37%)]\tLoss: 0.005897\t Accuracy:98.145%\n",
      "Rate : 80 [18560/49000 (38%)]\tLoss: 0.011479\t Accuracy:98.155%\n",
      "Rate : 80 [19200/49000 (39%)]\tLoss: 0.063889\t Accuracy:98.175%\n",
      "Rate : 80 [19840/49000 (40%)]\tLoss: 0.145169\t Accuracy:98.183%\n",
      "Rate : 80 [20480/49000 (42%)]\tLoss: 0.260309\t Accuracy:98.186%\n",
      "Rate : 80 [21120/49000 (43%)]\tLoss: 0.075873\t Accuracy:98.185%\n",
      "Rate : 80 [21760/49000 (44%)]\tLoss: 0.286232\t Accuracy:98.178%\n",
      "Rate : 80 [22400/49000 (46%)]\tLoss: 0.042750\t Accuracy:98.181%\n",
      "Rate : 80 [23040/49000 (47%)]\tLoss: 0.260887\t Accuracy:98.158%\n",
      "Rate : 80 [23680/49000 (48%)]\tLoss: 0.015938\t Accuracy:98.161%\n",
      "Rate : 80 [24320/49000 (50%)]\tLoss: 0.051587\t Accuracy:98.185%\n",
      "Rate : 80 [24960/49000 (51%)]\tLoss: 0.024497\t Accuracy:98.195%\n",
      "Rate : 80 [25600/49000 (52%)]\tLoss: 0.012584\t Accuracy:98.170%\n",
      "Rate : 80 [26240/49000 (54%)]\tLoss: 0.101562\t Accuracy:98.184%\n",
      "Rate : 80 [26880/49000 (55%)]\tLoss: 0.004822\t Accuracy:98.176%\n",
      "Rate : 80 [27520/49000 (56%)]\tLoss: 0.000357\t Accuracy:98.189%\n",
      "Rate : 80 [28160/49000 (57%)]\tLoss: 0.045885\t Accuracy:98.209%\n",
      "Rate : 80 [28800/49000 (59%)]\tLoss: 0.003958\t Accuracy:98.203%\n",
      "Rate : 80 [29440/49000 (60%)]\tLoss: 0.024753\t Accuracy:98.208%\n",
      "Rate : 80 [30080/49000 (61%)]\tLoss: 0.033420\t Accuracy:98.227%\n",
      "Rate : 80 [30720/49000 (63%)]\tLoss: 0.001070\t Accuracy:98.231%\n",
      "Rate : 80 [31360/49000 (64%)]\tLoss: 0.103316\t Accuracy:98.229%\n",
      "Rate : 80 [32000/49000 (65%)]\tLoss: 0.005723\t Accuracy:98.242%\n",
      "Rate : 80 [32640/49000 (67%)]\tLoss: 0.007008\t Accuracy:98.240%\n",
      "Rate : 80 [33280/49000 (68%)]\tLoss: 0.079789\t Accuracy:98.235%\n",
      "Rate : 80 [33920/49000 (69%)]\tLoss: 0.015706\t Accuracy:98.218%\n",
      "Rate : 80 [34560/49000 (70%)]\tLoss: 0.033855\t Accuracy:98.222%\n",
      "Rate : 80 [35200/49000 (72%)]\tLoss: 0.052792\t Accuracy:98.232%\n",
      "Rate : 80 [35840/49000 (73%)]\tLoss: 0.123641\t Accuracy:98.233%\n",
      "Rate : 80 [36480/49000 (74%)]\tLoss: 0.000489\t Accuracy:98.239%\n",
      "Rate : 80 [37120/49000 (76%)]\tLoss: 0.016535\t Accuracy:98.229%\n",
      "Rate : 80 [37760/49000 (77%)]\tLoss: 0.089377\t Accuracy:98.230%\n",
      "Rate : 80 [38400/49000 (78%)]\tLoss: 0.104310\t Accuracy:98.231%\n",
      "Rate : 80 [39040/49000 (80%)]\tLoss: 0.017631\t Accuracy:98.221%\n",
      "Rate : 80 [39680/49000 (81%)]\tLoss: 0.000706\t Accuracy:98.227%\n",
      "Rate : 80 [40320/49000 (82%)]\tLoss: 0.082283\t Accuracy:98.228%\n",
      "Rate : 80 [40960/49000 (84%)]\tLoss: 0.077018\t Accuracy:98.229%\n",
      "Rate : 80 [41600/49000 (85%)]\tLoss: 0.026369\t Accuracy:98.235%\n",
      "Rate : 80 [42240/49000 (86%)]\tLoss: 0.010044\t Accuracy:98.233%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate : 80 [42880/49000 (87%)]\tLoss: 0.129469\t Accuracy:98.245%\n",
      "Rate : 80 [43520/49000 (89%)]\tLoss: 0.059273\t Accuracy:98.248%\n",
      "Rate : 80 [44160/49000 (90%)]\tLoss: 0.007660\t Accuracy:98.253%\n",
      "Rate : 80 [44800/49000 (91%)]\tLoss: 0.050535\t Accuracy:98.253%\n",
      "Rate : 80 [45440/49000 (93%)]\tLoss: 0.164180\t Accuracy:98.260%\n",
      "Rate : 80 [46080/49000 (94%)]\tLoss: 0.036062\t Accuracy:98.265%\n",
      "Rate : 80 [46720/49000 (95%)]\tLoss: 0.040771\t Accuracy:98.265%\n",
      "Rate : 80 [47360/49000 (97%)]\tLoss: 0.042444\t Accuracy:98.278%\n",
      "Rate : 80 [48000/49000 (98%)]\tLoss: 0.011916\t Accuracy:98.266%\n",
      "Rate : 80 [48640/49000 (99%)]\tLoss: 0.004438\t Accuracy:98.274%\n",
      "Rate : 83 [0/49000 (0%)]\tLoss: 0.307275\t Accuracy:84.375%\n",
      "Rate : 83 [640/49000 (1%)]\tLoss: 0.398142\t Accuracy:82.887%\n",
      "Rate : 83 [1280/49000 (3%)]\tLoss: 0.458066\t Accuracy:86.433%\n",
      "Rate : 83 [1920/49000 (4%)]\tLoss: 0.127934\t Accuracy:87.705%\n",
      "Rate : 83 [2560/49000 (5%)]\tLoss: 0.254379\t Accuracy:88.465%\n",
      "Rate : 83 [3200/49000 (7%)]\tLoss: 0.085625\t Accuracy:89.697%\n",
      "Rate : 83 [3840/49000 (8%)]\tLoss: 0.044159\t Accuracy:90.341%\n",
      "Rate : 83 [4480/49000 (9%)]\tLoss: 0.112829\t Accuracy:90.824%\n",
      "Rate : 83 [5120/49000 (10%)]\tLoss: 0.433097\t Accuracy:91.382%\n",
      "Rate : 83 [5760/49000 (12%)]\tLoss: 0.143289\t Accuracy:91.730%\n",
      "Rate : 83 [6400/49000 (13%)]\tLoss: 0.182305\t Accuracy:92.024%\n",
      "Rate : 83 [7040/49000 (14%)]\tLoss: 0.053875\t Accuracy:92.294%\n",
      "Rate : 83 [7680/49000 (16%)]\tLoss: 0.313919\t Accuracy:92.557%\n",
      "Rate : 83 [8320/49000 (17%)]\tLoss: 0.514641\t Accuracy:92.768%\n",
      "Rate : 83 [8960/49000 (18%)]\tLoss: 0.086738\t Accuracy:92.960%\n",
      "Rate : 83 [9600/49000 (20%)]\tLoss: 0.145938\t Accuracy:93.096%\n",
      "Rate : 83 [10240/49000 (21%)]\tLoss: 0.090621\t Accuracy:93.254%\n",
      "Rate : 83 [10880/49000 (22%)]\tLoss: 0.250158\t Accuracy:93.365%\n",
      "Rate : 83 [11520/49000 (23%)]\tLoss: 0.032746\t Accuracy:93.508%\n",
      "Rate : 83 [12160/49000 (25%)]\tLoss: 0.247784\t Accuracy:93.594%\n",
      "Rate : 83 [12800/49000 (26%)]\tLoss: 0.185332\t Accuracy:93.711%\n",
      "Rate : 83 [13440/49000 (27%)]\tLoss: 0.187858\t Accuracy:93.861%\n",
      "Rate : 83 [14080/49000 (29%)]\tLoss: 0.194925\t Accuracy:93.977%\n",
      "Rate : 83 [14720/49000 (30%)]\tLoss: 0.097081\t Accuracy:94.075%\n",
      "Rate : 83 [15360/49000 (31%)]\tLoss: 0.098835\t Accuracy:94.192%\n",
      "Rate : 83 [16000/49000 (33%)]\tLoss: 0.099761\t Accuracy:94.305%\n",
      "Rate : 83 [16640/49000 (34%)]\tLoss: 0.188426\t Accuracy:94.422%\n",
      "Rate : 83 [17280/49000 (35%)]\tLoss: 0.308326\t Accuracy:94.432%\n",
      "Rate : 83 [17920/49000 (37%)]\tLoss: 0.018278\t Accuracy:94.508%\n",
      "Rate : 83 [18560/49000 (38%)]\tLoss: 0.020055\t Accuracy:94.578%\n",
      "Rate : 83 [19200/49000 (39%)]\tLoss: 0.529707\t Accuracy:94.650%\n",
      "Rate : 83 [19840/49000 (40%)]\tLoss: 0.163309\t Accuracy:94.731%\n",
      "Rate : 83 [20480/49000 (42%)]\tLoss: 0.122784\t Accuracy:94.784%\n",
      "Rate : 83 [21120/49000 (43%)]\tLoss: 0.057721\t Accuracy:94.818%\n",
      "Rate : 83 [21760/49000 (44%)]\tLoss: 0.074424\t Accuracy:94.879%\n",
      "Rate : 83 [22400/49000 (46%)]\tLoss: 0.101351\t Accuracy:94.945%\n",
      "Rate : 83 [23040/49000 (47%)]\tLoss: 0.145558\t Accuracy:94.990%\n",
      "Rate : 83 [23680/49000 (48%)]\tLoss: 0.020979\t Accuracy:95.040%\n",
      "Rate : 83 [24320/49000 (50%)]\tLoss: 0.022258\t Accuracy:95.093%\n",
      "Rate : 83 [24960/49000 (51%)]\tLoss: 0.053575\t Accuracy:95.122%\n",
      "Rate : 83 [25600/49000 (52%)]\tLoss: 0.021357\t Accuracy:95.127%\n",
      "Rate : 83 [26240/49000 (54%)]\tLoss: 0.054902\t Accuracy:95.166%\n",
      "Rate : 83 [26880/49000 (55%)]\tLoss: 0.005332\t Accuracy:95.184%\n",
      "Rate : 83 [27520/49000 (56%)]\tLoss: 0.001816\t Accuracy:95.227%\n",
      "Rate : 83 [28160/49000 (57%)]\tLoss: 0.192866\t Accuracy:95.272%\n",
      "Rate : 83 [28800/49000 (59%)]\tLoss: 0.145128\t Accuracy:95.328%\n",
      "Rate : 83 [29440/49000 (60%)]\tLoss: 0.071896\t Accuracy:95.338%\n",
      "Rate : 83 [30080/49000 (61%)]\tLoss: 0.095318\t Accuracy:95.364%\n",
      "Rate : 83 [30720/49000 (63%)]\tLoss: 0.001009\t Accuracy:95.412%\n",
      "Rate : 83 [31360/49000 (64%)]\tLoss: 0.172523\t Accuracy:95.457%\n",
      "Rate : 83 [32000/49000 (65%)]\tLoss: 0.020596\t Accuracy:95.504%\n",
      "Rate : 83 [32640/49000 (67%)]\tLoss: 0.052418\t Accuracy:95.534%\n",
      "Rate : 83 [33280/49000 (68%)]\tLoss: 0.001595\t Accuracy:95.557%\n",
      "Rate : 83 [33920/49000 (69%)]\tLoss: 0.057520\t Accuracy:95.588%\n",
      "Rate : 83 [34560/49000 (70%)]\tLoss: 0.078965\t Accuracy:95.612%\n",
      "Rate : 83 [35200/49000 (72%)]\tLoss: 0.036021\t Accuracy:95.646%\n",
      "Rate : 83 [35840/49000 (73%)]\tLoss: 0.042739\t Accuracy:95.651%\n",
      "Rate : 83 [36480/49000 (74%)]\tLoss: 0.043909\t Accuracy:95.670%\n",
      "Rate : 83 [37120/49000 (76%)]\tLoss: 0.018795\t Accuracy:95.696%\n",
      "Rate : 83 [37760/49000 (77%)]\tLoss: 0.156283\t Accuracy:95.708%\n",
      "Rate : 83 [38400/49000 (78%)]\tLoss: 0.230140\t Accuracy:95.735%\n",
      "Rate : 83 [39040/49000 (80%)]\tLoss: 0.189678\t Accuracy:95.759%\n",
      "Rate : 83 [39680/49000 (81%)]\tLoss: 0.021651\t Accuracy:95.790%\n",
      "Rate : 83 [40320/49000 (82%)]\tLoss: 0.280229\t Accuracy:95.807%\n",
      "Rate : 83 [40960/49000 (84%)]\tLoss: 0.071596\t Accuracy:95.836%\n",
      "Rate : 83 [41600/49000 (85%)]\tLoss: 0.171893\t Accuracy:95.857%\n",
      "Rate : 83 [42240/49000 (86%)]\tLoss: 0.009330\t Accuracy:95.872%\n",
      "Rate : 83 [42880/49000 (87%)]\tLoss: 0.049153\t Accuracy:95.896%\n",
      "Rate : 83 [43520/49000 (89%)]\tLoss: 0.630333\t Accuracy:95.915%\n",
      "Rate : 83 [44160/49000 (90%)]\tLoss: 0.017892\t Accuracy:95.936%\n",
      "Rate : 83 [44800/49000 (91%)]\tLoss: 0.100329\t Accuracy:95.945%\n",
      "Rate : 83 [45440/49000 (93%)]\tLoss: 0.117276\t Accuracy:95.978%\n",
      "Rate : 83 [46080/49000 (94%)]\tLoss: 0.103238\t Accuracy:95.999%\n",
      "Rate : 83 [46720/49000 (95%)]\tLoss: 0.087439\t Accuracy:96.017%\n",
      "Rate : 83 [47360/49000 (97%)]\tLoss: 0.010206\t Accuracy:96.039%\n",
      "Rate : 83 [48000/49000 (98%)]\tLoss: 0.014248\t Accuracy:96.046%\n",
      "Rate : 83 [48640/49000 (99%)]\tLoss: 0.060307\t Accuracy:96.063%\n",
      "Rate : 83 [0/49000 (0%)]\tLoss: 0.020879\t Accuracy:100.000%\n",
      "Rate : 83 [640/49000 (1%)]\tLoss: 0.138203\t Accuracy:97.470%\n",
      "Rate : 83 [1280/49000 (3%)]\tLoss: 0.113654\t Accuracy:97.180%\n",
      "Rate : 83 [1920/49000 (4%)]\tLoss: 0.012723\t Accuracy:97.182%\n",
      "Rate : 83 [2560/49000 (5%)]\tLoss: 0.056909\t Accuracy:97.415%\n",
      "Rate : 83 [3200/49000 (7%)]\tLoss: 0.169141\t Accuracy:97.710%\n",
      "Rate : 83 [3840/49000 (8%)]\tLoss: 0.015207\t Accuracy:97.856%\n",
      "Rate : 83 [4480/49000 (9%)]\tLoss: 0.034775\t Accuracy:97.895%\n",
      "Rate : 83 [5120/49000 (10%)]\tLoss: 0.159383\t Accuracy:97.845%\n",
      "Rate : 83 [5760/49000 (12%)]\tLoss: 0.022869\t Accuracy:97.894%\n",
      "Rate : 83 [6400/49000 (13%)]\tLoss: 0.037518\t Accuracy:97.917%\n",
      "Rate : 83 [7040/49000 (14%)]\tLoss: 0.023773\t Accuracy:97.964%\n",
      "Rate : 83 [7680/49000 (16%)]\tLoss: 0.255551\t Accuracy:97.938%\n",
      "Rate : 83 [8320/49000 (17%)]\tLoss: 0.279267\t Accuracy:97.869%\n",
      "Rate : 83 [8960/49000 (18%)]\tLoss: 0.075898\t Accuracy:97.865%\n",
      "Rate : 83 [9600/49000 (20%)]\tLoss: 0.010094\t Accuracy:97.830%\n",
      "Rate : 83 [10240/49000 (21%)]\tLoss: 0.015815\t Accuracy:97.887%\n",
      "Rate : 83 [10880/49000 (22%)]\tLoss: 0.057978\t Accuracy:97.901%\n",
      "Rate : 83 [11520/49000 (23%)]\tLoss: 0.015516\t Accuracy:97.896%\n",
      "Rate : 83 [12160/49000 (25%)]\tLoss: 0.188510\t Accuracy:97.859%\n",
      "Rate : 83 [12800/49000 (26%)]\tLoss: 0.101682\t Accuracy:97.896%\n",
      "Rate : 83 [13440/49000 (27%)]\tLoss: 0.028832\t Accuracy:97.944%\n",
      "Rate : 83 [14080/49000 (29%)]\tLoss: 0.121398\t Accuracy:97.952%\n",
      "Rate : 83 [14720/49000 (30%)]\tLoss: 0.106356\t Accuracy:98.007%\n",
      "Rate : 83 [15360/49000 (31%)]\tLoss: 0.004920\t Accuracy:98.038%\n",
      "Rate : 83 [16000/49000 (33%)]\tLoss: 0.237187\t Accuracy:98.023%\n",
      "Rate : 83 [16640/49000 (34%)]\tLoss: 0.111534\t Accuracy:98.033%\n",
      "Rate : 83 [17280/49000 (35%)]\tLoss: 0.108216\t Accuracy:98.007%\n",
      "Rate : 83 [17920/49000 (37%)]\tLoss: 0.026225\t Accuracy:98.017%\n",
      "Rate : 83 [18560/49000 (38%)]\tLoss: 0.004836\t Accuracy:98.042%\n",
      "Rate : 83 [19200/49000 (39%)]\tLoss: 0.263367\t Accuracy:98.045%\n",
      "Rate : 83 [19840/49000 (40%)]\tLoss: 0.139770\t Accuracy:98.042%\n",
      "Rate : 83 [20480/49000 (42%)]\tLoss: 0.045015\t Accuracy:98.030%\n",
      "Rate : 83 [21120/49000 (43%)]\tLoss: 0.074085\t Accuracy:98.010%\n",
      "Rate : 83 [21760/49000 (44%)]\tLoss: 0.074074\t Accuracy:97.995%\n",
      "Rate : 83 [22400/49000 (46%)]\tLoss: 0.040755\t Accuracy:98.012%\n",
      "Rate : 83 [23040/49000 (47%)]\tLoss: 0.030523\t Accuracy:98.019%\n",
      "Rate : 83 [23680/49000 (48%)]\tLoss: 0.012686\t Accuracy:98.022%\n",
      "Rate : 83 [24320/49000 (50%)]\tLoss: 0.022775\t Accuracy:98.029%\n",
      "Rate : 83 [24960/49000 (51%)]\tLoss: 0.124361\t Accuracy:98.011%\n",
      "Rate : 83 [25600/49000 (52%)]\tLoss: 0.004199\t Accuracy:97.991%\n",
      "Rate : 83 [26240/49000 (54%)]\tLoss: 0.003577\t Accuracy:97.994%\n",
      "Rate : 83 [26880/49000 (55%)]\tLoss: 0.002192\t Accuracy:98.008%\n",
      "Rate : 83 [27520/49000 (56%)]\tLoss: 0.000991\t Accuracy:98.029%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate : 83 [28160/49000 (57%)]\tLoss: 0.069160\t Accuracy:98.042%\n",
      "Rate : 83 [28800/49000 (59%)]\tLoss: 0.045274\t Accuracy:98.051%\n",
      "Rate : 83 [29440/49000 (60%)]\tLoss: 0.133291\t Accuracy:98.039%\n",
      "Rate : 83 [30080/49000 (61%)]\tLoss: 0.008680\t Accuracy:98.031%\n",
      "Rate : 83 [30720/49000 (63%)]\tLoss: 0.000852\t Accuracy:98.026%\n",
      "Rate : 83 [31360/49000 (64%)]\tLoss: 0.172954\t Accuracy:98.031%\n",
      "Rate : 83 [32000/49000 (65%)]\tLoss: 0.002211\t Accuracy:98.052%\n",
      "Rate : 83 [32640/49000 (67%)]\tLoss: 0.027064\t Accuracy:98.044%\n",
      "Rate : 83 [33280/49000 (68%)]\tLoss: 0.001684\t Accuracy:98.055%\n",
      "Rate : 83 [33920/49000 (69%)]\tLoss: 0.097648\t Accuracy:98.056%\n",
      "Rate : 83 [34560/49000 (70%)]\tLoss: 0.060645\t Accuracy:98.046%\n",
      "Rate : 83 [35200/49000 (72%)]\tLoss: 0.028150\t Accuracy:98.050%\n",
      "Rate : 83 [35840/49000 (73%)]\tLoss: 0.021517\t Accuracy:98.051%\n",
      "Rate : 83 [36480/49000 (74%)]\tLoss: 0.019816\t Accuracy:98.061%\n",
      "Rate : 83 [37120/49000 (76%)]\tLoss: 0.003668\t Accuracy:98.067%\n",
      "Rate : 83 [37760/49000 (77%)]\tLoss: 0.329540\t Accuracy:98.068%\n",
      "Rate : 83 [38400/49000 (78%)]\tLoss: 0.031614\t Accuracy:98.072%\n",
      "Rate : 83 [39040/49000 (80%)]\tLoss: 0.005761\t Accuracy:98.073%\n",
      "Rate : 83 [39680/49000 (81%)]\tLoss: 0.039408\t Accuracy:98.069%\n",
      "Rate : 83 [40320/49000 (82%)]\tLoss: 0.109958\t Accuracy:98.087%\n",
      "Rate : 83 [40960/49000 (84%)]\tLoss: 0.039606\t Accuracy:98.090%\n",
      "Rate : 83 [41600/49000 (85%)]\tLoss: 0.120018\t Accuracy:98.098%\n",
      "Rate : 83 [42240/49000 (86%)]\tLoss: 0.001393\t Accuracy:98.098%\n",
      "Rate : 83 [42880/49000 (87%)]\tLoss: 0.016025\t Accuracy:98.108%\n",
      "Rate : 83 [43520/49000 (89%)]\tLoss: 0.027372\t Accuracy:98.119%\n",
      "Rate : 83 [44160/49000 (90%)]\tLoss: 0.009528\t Accuracy:98.126%\n",
      "Rate : 83 [44800/49000 (91%)]\tLoss: 0.043905\t Accuracy:98.129%\n",
      "Rate : 83 [45440/49000 (93%)]\tLoss: 0.072427\t Accuracy:98.144%\n",
      "Rate : 83 [46080/49000 (94%)]\tLoss: 0.100549\t Accuracy:98.144%\n",
      "Rate : 83 [46720/49000 (95%)]\tLoss: 0.134332\t Accuracy:98.146%\n",
      "Rate : 83 [47360/49000 (97%)]\tLoss: 0.009121\t Accuracy:98.154%\n",
      "Rate : 83 [48000/49000 (98%)]\tLoss: 0.048722\t Accuracy:98.155%\n",
      "Rate : 83 [48640/49000 (99%)]\tLoss: 0.013952\t Accuracy:98.163%\n",
      "Rate : 85 [0/49000 (0%)]\tLoss: 0.083365\t Accuracy:96.875%\n",
      "Rate : 85 [640/49000 (1%)]\tLoss: 0.209053\t Accuracy:95.833%\n",
      "Rate : 85 [1280/49000 (3%)]\tLoss: 0.123448\t Accuracy:94.207%\n",
      "Rate : 85 [1920/49000 (4%)]\tLoss: 0.074153\t Accuracy:94.723%\n",
      "Rate : 85 [2560/49000 (5%)]\tLoss: 0.029657\t Accuracy:94.946%\n",
      "Rate : 85 [3200/49000 (7%)]\tLoss: 0.152257\t Accuracy:95.111%\n",
      "Rate : 85 [3840/49000 (8%)]\tLoss: 0.024033\t Accuracy:95.455%\n",
      "Rate : 85 [4480/49000 (9%)]\tLoss: 0.056069\t Accuracy:95.678%\n",
      "Rate : 85 [5120/49000 (10%)]\tLoss: 0.211200\t Accuracy:95.924%\n",
      "Rate : 85 [5760/49000 (12%)]\tLoss: 0.122551\t Accuracy:96.012%\n",
      "Rate : 85 [6400/49000 (13%)]\tLoss: 0.122776\t Accuracy:96.004%\n",
      "Rate : 85 [7040/49000 (14%)]\tLoss: 0.015969\t Accuracy:96.069%\n",
      "Rate : 85 [7680/49000 (16%)]\tLoss: 0.092228\t Accuracy:95.993%\n",
      "Rate : 85 [8320/49000 (17%)]\tLoss: 0.194572\t Accuracy:96.073%\n",
      "Rate : 85 [8960/49000 (18%)]\tLoss: 0.038330\t Accuracy:96.097%\n",
      "Rate : 85 [9600/49000 (20%)]\tLoss: 0.020643\t Accuracy:96.148%\n",
      "Rate : 85 [10240/49000 (21%)]\tLoss: 0.005455\t Accuracy:96.271%\n",
      "Rate : 85 [10880/49000 (22%)]\tLoss: 0.156454\t Accuracy:96.316%\n",
      "Rate : 85 [11520/49000 (23%)]\tLoss: 0.102136\t Accuracy:96.338%\n",
      "Rate : 85 [12160/49000 (25%)]\tLoss: 0.284601\t Accuracy:96.391%\n",
      "Rate : 85 [12800/49000 (26%)]\tLoss: 0.046467\t Accuracy:96.446%\n",
      "Rate : 85 [13440/49000 (27%)]\tLoss: 0.024913\t Accuracy:96.504%\n",
      "Rate : 85 [14080/49000 (29%)]\tLoss: 0.207360\t Accuracy:96.528%\n",
      "Rate : 85 [14720/49000 (30%)]\tLoss: 0.168143\t Accuracy:96.590%\n",
      "Rate : 85 [15360/49000 (31%)]\tLoss: 0.012526\t Accuracy:96.635%\n",
      "Rate : 85 [16000/49000 (33%)]\tLoss: 0.033116\t Accuracy:96.650%\n",
      "Rate : 85 [16640/49000 (34%)]\tLoss: 0.140851\t Accuracy:96.677%\n",
      "Rate : 85 [17280/49000 (35%)]\tLoss: 0.037387\t Accuracy:96.690%\n",
      "Rate : 85 [17920/49000 (37%)]\tLoss: 0.003933\t Accuracy:96.719%\n",
      "Rate : 85 [18560/49000 (38%)]\tLoss: 0.007411\t Accuracy:96.767%\n",
      "Rate : 85 [19200/49000 (39%)]\tLoss: 0.250987\t Accuracy:96.807%\n",
      "Rate : 85 [19840/49000 (40%)]\tLoss: 0.097485\t Accuracy:96.825%\n",
      "Rate : 85 [20480/49000 (42%)]\tLoss: 0.038032\t Accuracy:96.860%\n",
      "Rate : 85 [21120/49000 (43%)]\tLoss: 0.357658\t Accuracy:96.875%\n",
      "Rate : 85 [21760/49000 (44%)]\tLoss: 0.124080\t Accuracy:96.916%\n",
      "Rate : 85 [22400/49000 (46%)]\tLoss: 0.176715\t Accuracy:96.902%\n",
      "Rate : 85 [23040/49000 (47%)]\tLoss: 0.127656\t Accuracy:96.901%\n",
      "Rate : 85 [23680/49000 (48%)]\tLoss: 0.034297\t Accuracy:96.930%\n",
      "Rate : 85 [24320/49000 (50%)]\tLoss: 0.016145\t Accuracy:96.957%\n",
      "Rate : 85 [24960/49000 (51%)]\tLoss: 0.184862\t Accuracy:96.963%\n",
      "Rate : 85 [25600/49000 (52%)]\tLoss: 0.024060\t Accuracy:96.957%\n",
      "Rate : 85 [26240/49000 (54%)]\tLoss: 0.012113\t Accuracy:96.985%\n",
      "Rate : 85 [26880/49000 (55%)]\tLoss: 0.036342\t Accuracy:96.990%\n",
      "Rate : 85 [27520/49000 (56%)]\tLoss: 0.085930\t Accuracy:96.998%\n",
      "Rate : 85 [28160/49000 (57%)]\tLoss: 0.347152\t Accuracy:97.031%\n",
      "Rate : 85 [28800/49000 (59%)]\tLoss: 0.093083\t Accuracy:97.045%\n",
      "Rate : 85 [29440/49000 (60%)]\tLoss: 0.015951\t Accuracy:97.048%\n",
      "Rate : 85 [30080/49000 (61%)]\tLoss: 0.073655\t Accuracy:97.041%\n",
      "Rate : 85 [30720/49000 (63%)]\tLoss: 0.003680\t Accuracy:97.047%\n",
      "Rate : 85 [31360/49000 (64%)]\tLoss: 0.232250\t Accuracy:97.050%\n",
      "Rate : 85 [32000/49000 (65%)]\tLoss: 0.013239\t Accuracy:97.078%\n",
      "Rate : 85 [32640/49000 (67%)]\tLoss: 0.010104\t Accuracy:97.095%\n",
      "Rate : 85 [33280/49000 (68%)]\tLoss: 0.006220\t Accuracy:97.103%\n",
      "Rate : 85 [33920/49000 (69%)]\tLoss: 0.021402\t Accuracy:97.122%\n",
      "Rate : 85 [34560/49000 (70%)]\tLoss: 0.212448\t Accuracy:97.138%\n",
      "Rate : 85 [35200/49000 (72%)]\tLoss: 0.015482\t Accuracy:97.156%\n",
      "Rate : 85 [35840/49000 (73%)]\tLoss: 0.022969\t Accuracy:97.179%\n",
      "Rate : 85 [36480/49000 (74%)]\tLoss: 0.014562\t Accuracy:97.195%\n",
      "Rate : 85 [37120/49000 (76%)]\tLoss: 0.046517\t Accuracy:97.206%\n",
      "Rate : 85 [37760/49000 (77%)]\tLoss: 0.327036\t Accuracy:97.214%\n",
      "Rate : 85 [38400/49000 (78%)]\tLoss: 0.105707\t Accuracy:97.234%\n",
      "Rate : 85 [39040/49000 (80%)]\tLoss: 0.003219\t Accuracy:97.236%\n",
      "Rate : 85 [39680/49000 (81%)]\tLoss: 0.040600\t Accuracy:97.243%\n",
      "Rate : 85 [40320/49000 (82%)]\tLoss: 0.053659\t Accuracy:97.269%\n",
      "Rate : 85 [40960/49000 (84%)]\tLoss: 0.066545\t Accuracy:97.285%\n",
      "Rate : 85 [41600/49000 (85%)]\tLoss: 0.019471\t Accuracy:97.305%\n",
      "Rate : 85 [42240/49000 (86%)]\tLoss: 0.012721\t Accuracy:97.327%\n",
      "Rate : 85 [42880/49000 (87%)]\tLoss: 0.055808\t Accuracy:97.334%\n",
      "Rate : 85 [43520/49000 (89%)]\tLoss: 0.414755\t Accuracy:97.350%\n",
      "Rate : 85 [44160/49000 (90%)]\tLoss: 0.006694\t Accuracy:97.362%\n",
      "Rate : 85 [44800/49000 (91%)]\tLoss: 0.049451\t Accuracy:97.363%\n",
      "Rate : 85 [45440/49000 (93%)]\tLoss: 0.096228\t Accuracy:97.392%\n",
      "Rate : 85 [46080/49000 (94%)]\tLoss: 0.020971\t Accuracy:97.387%\n",
      "Rate : 85 [46720/49000 (95%)]\tLoss: 0.018703\t Accuracy:97.393%\n",
      "Rate : 85 [47360/49000 (97%)]\tLoss: 0.021051\t Accuracy:97.415%\n",
      "Rate : 85 [48000/49000 (98%)]\tLoss: 0.021657\t Accuracy:97.423%\n",
      "Rate : 85 [48640/49000 (99%)]\tLoss: 0.027260\t Accuracy:97.434%\n",
      "Rate : 85 [0/49000 (0%)]\tLoss: 0.005735\t Accuracy:100.000%\n",
      "Rate : 85 [640/49000 (1%)]\tLoss: 0.014302\t Accuracy:98.661%\n",
      "Rate : 85 [1280/49000 (3%)]\tLoss: 0.174433\t Accuracy:97.942%\n",
      "Rate : 85 [1920/49000 (4%)]\tLoss: 0.000878\t Accuracy:97.848%\n",
      "Rate : 85 [2560/49000 (5%)]\tLoss: 0.019834\t Accuracy:97.994%\n",
      "Rate : 85 [3200/49000 (7%)]\tLoss: 0.050143\t Accuracy:98.051%\n",
      "Rate : 85 [3840/49000 (8%)]\tLoss: 0.012645\t Accuracy:98.270%\n",
      "Rate : 85 [4480/49000 (9%)]\tLoss: 0.007228\t Accuracy:98.316%\n",
      "Rate : 85 [5120/49000 (10%)]\tLoss: 0.240451\t Accuracy:98.350%\n",
      "Rate : 85 [5760/49000 (12%)]\tLoss: 0.034876\t Accuracy:98.343%\n",
      "Rate : 85 [6400/49000 (13%)]\tLoss: 0.030305\t Accuracy:98.383%\n",
      "Rate : 85 [7040/49000 (14%)]\tLoss: 0.017391\t Accuracy:98.430%\n",
      "Rate : 85 [7680/49000 (16%)]\tLoss: 0.050855\t Accuracy:98.444%\n",
      "Rate : 85 [8320/49000 (17%)]\tLoss: 0.032271\t Accuracy:98.455%\n",
      "Rate : 85 [8960/49000 (18%)]\tLoss: 0.041849\t Accuracy:98.399%\n",
      "Rate : 85 [9600/49000 (20%)]\tLoss: 0.001047\t Accuracy:98.432%\n",
      "Rate : 85 [10240/49000 (21%)]\tLoss: 0.017319\t Accuracy:98.481%\n",
      "Rate : 85 [10880/49000 (22%)]\tLoss: 0.023151\t Accuracy:98.488%\n",
      "Rate : 85 [11520/49000 (23%)]\tLoss: 0.093228\t Accuracy:98.468%\n",
      "Rate : 85 [12160/49000 (25%)]\tLoss: 0.148537\t Accuracy:98.491%\n",
      "Rate : 85 [12800/49000 (26%)]\tLoss: 0.070717\t Accuracy:98.488%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate : 85 [13440/49000 (27%)]\tLoss: 0.000961\t Accuracy:98.501%\n",
      "Rate : 85 [14080/49000 (29%)]\tLoss: 0.088743\t Accuracy:98.512%\n",
      "Rate : 85 [14720/49000 (30%)]\tLoss: 0.043150\t Accuracy:98.549%\n",
      "Rate : 85 [15360/49000 (31%)]\tLoss: 0.023545\t Accuracy:98.564%\n",
      "Rate : 85 [16000/49000 (33%)]\tLoss: 0.053273\t Accuracy:98.553%\n",
      "Rate : 85 [16640/49000 (34%)]\tLoss: 0.154475\t Accuracy:98.566%\n",
      "Rate : 85 [17280/49000 (35%)]\tLoss: 0.040413\t Accuracy:98.562%\n",
      "Rate : 85 [17920/49000 (37%)]\tLoss: 0.002924\t Accuracy:98.563%\n",
      "Rate : 85 [18560/49000 (38%)]\tLoss: 0.000634\t Accuracy:98.564%\n",
      "Rate : 85 [19200/49000 (39%)]\tLoss: 0.123115\t Accuracy:98.560%\n",
      "Rate : 85 [19840/49000 (40%)]\tLoss: 0.019663\t Accuracy:98.566%\n",
      "Rate : 85 [20480/49000 (42%)]\tLoss: 0.006787\t Accuracy:98.533%\n",
      "Rate : 85 [21120/49000 (43%)]\tLoss: 0.185421\t Accuracy:98.539%\n",
      "Rate : 85 [21760/49000 (44%)]\tLoss: 0.077848\t Accuracy:98.541%\n",
      "Rate : 85 [22400/49000 (46%)]\tLoss: 0.118327\t Accuracy:98.533%\n",
      "Rate : 85 [23040/49000 (47%)]\tLoss: 0.058449\t Accuracy:98.539%\n",
      "Rate : 85 [23680/49000 (48%)]\tLoss: 0.003381\t Accuracy:98.549%\n",
      "Rate : 85 [24320/49000 (50%)]\tLoss: 0.010139\t Accuracy:98.546%\n",
      "Rate : 85 [24960/49000 (51%)]\tLoss: 0.212337\t Accuracy:98.548%\n",
      "Rate : 85 [25600/49000 (52%)]\tLoss: 0.005312\t Accuracy:98.529%\n",
      "Rate : 85 [26240/49000 (54%)]\tLoss: 0.000583\t Accuracy:98.519%\n",
      "Rate : 85 [26880/49000 (55%)]\tLoss: 0.050821\t Accuracy:98.521%\n",
      "Rate : 85 [27520/49000 (56%)]\tLoss: 0.007090\t Accuracy:98.534%\n",
      "Rate : 85 [28160/49000 (57%)]\tLoss: 0.132798\t Accuracy:98.542%\n",
      "Rate : 85 [28800/49000 (59%)]\tLoss: 0.029173\t Accuracy:98.543%\n",
      "Rate : 85 [29440/49000 (60%)]\tLoss: 0.016550\t Accuracy:98.548%\n",
      "Rate : 85 [30080/49000 (61%)]\tLoss: 0.024316\t Accuracy:98.535%\n",
      "Rate : 85 [30720/49000 (63%)]\tLoss: 0.002133\t Accuracy:98.533%\n",
      "Rate : 85 [31360/49000 (64%)]\tLoss: 0.227156\t Accuracy:98.531%\n",
      "Rate : 85 [32000/49000 (65%)]\tLoss: 0.033094\t Accuracy:98.533%\n",
      "Rate : 85 [32640/49000 (67%)]\tLoss: 0.009078\t Accuracy:98.543%\n",
      "Rate : 85 [33280/49000 (68%)]\tLoss: 0.004058\t Accuracy:98.538%\n",
      "Rate : 85 [33920/49000 (69%)]\tLoss: 0.005807\t Accuracy:98.536%\n",
      "Rate : 85 [34560/49000 (70%)]\tLoss: 0.086220\t Accuracy:98.534%\n",
      "Rate : 85 [35200/49000 (72%)]\tLoss: 0.056226\t Accuracy:98.541%\n",
      "Rate : 85 [35840/49000 (73%)]\tLoss: 0.007744\t Accuracy:98.542%\n",
      "Rate : 85 [36480/49000 (74%)]\tLoss: 0.006877\t Accuracy:98.535%\n",
      "Rate : 85 [37120/49000 (76%)]\tLoss: 0.006452\t Accuracy:98.530%\n",
      "Rate : 85 [37760/49000 (77%)]\tLoss: 0.481404\t Accuracy:98.510%\n",
      "Rate : 85 [38400/49000 (78%)]\tLoss: 0.114135\t Accuracy:98.509%\n",
      "Rate : 85 [39040/49000 (80%)]\tLoss: 0.004392\t Accuracy:98.490%\n",
      "Rate : 85 [39680/49000 (81%)]\tLoss: 0.010388\t Accuracy:98.494%\n",
      "Rate : 85 [40320/49000 (82%)]\tLoss: 0.000213\t Accuracy:98.508%\n",
      "Rate : 85 [40960/49000 (84%)]\tLoss: 0.134930\t Accuracy:98.514%\n",
      "Rate : 85 [41600/49000 (85%)]\tLoss: 0.015643\t Accuracy:98.523%\n",
      "Rate : 85 [42240/49000 (86%)]\tLoss: 0.005094\t Accuracy:98.529%\n",
      "Rate : 85 [42880/49000 (87%)]\tLoss: 0.028923\t Accuracy:98.532%\n",
      "Rate : 85 [43520/49000 (89%)]\tLoss: 0.054493\t Accuracy:98.530%\n",
      "Rate : 85 [44160/49000 (90%)]\tLoss: 0.011823\t Accuracy:98.520%\n",
      "Rate : 85 [44800/49000 (91%)]\tLoss: 0.084282\t Accuracy:98.512%\n",
      "Rate : 85 [45440/49000 (93%)]\tLoss: 0.049612\t Accuracy:98.527%\n",
      "Rate : 85 [46080/49000 (94%)]\tLoss: 0.029822\t Accuracy:98.530%\n",
      "Rate : 85 [46720/49000 (95%)]\tLoss: 0.005957\t Accuracy:98.535%\n",
      "Rate : 85 [47360/49000 (97%)]\tLoss: 0.020137\t Accuracy:98.544%\n",
      "Rate : 85 [48000/49000 (98%)]\tLoss: 0.045795\t Accuracy:98.538%\n",
      "Rate : 85 [48640/49000 (99%)]\tLoss: 0.015368\t Accuracy:98.543%\n",
      "Rate : 87 [0/49000 (0%)]\tLoss: 0.293041\t Accuracy:90.625%\n",
      "Rate : 87 [640/49000 (1%)]\tLoss: 0.161630\t Accuracy:88.690%\n",
      "Rate : 87 [1280/49000 (3%)]\tLoss: 0.160994\t Accuracy:89.482%\n",
      "Rate : 87 [1920/49000 (4%)]\tLoss: 0.184268\t Accuracy:90.625%\n",
      "Rate : 87 [2560/49000 (5%)]\tLoss: 0.074728\t Accuracy:91.358%\n",
      "Rate : 87 [3200/49000 (7%)]\tLoss: 0.103691\t Accuracy:92.234%\n",
      "Rate : 87 [3840/49000 (8%)]\tLoss: 0.051181\t Accuracy:92.743%\n",
      "Rate : 87 [4480/49000 (9%)]\tLoss: 0.018573\t Accuracy:93.152%\n",
      "Rate : 87 [5120/49000 (10%)]\tLoss: 0.459789\t Accuracy:93.750%\n",
      "Rate : 87 [5760/49000 (12%)]\tLoss: 0.105762\t Accuracy:94.061%\n",
      "Rate : 87 [6400/49000 (13%)]\tLoss: 0.165752\t Accuracy:94.325%\n",
      "Rate : 87 [7040/49000 (14%)]\tLoss: 0.069656\t Accuracy:94.429%\n",
      "Rate : 87 [7680/49000 (16%)]\tLoss: 0.078690\t Accuracy:94.528%\n",
      "Rate : 87 [8320/49000 (17%)]\tLoss: 0.101409\t Accuracy:94.684%\n",
      "Rate : 87 [8960/49000 (18%)]\tLoss: 0.071507\t Accuracy:94.795%\n",
      "Rate : 87 [9600/49000 (20%)]\tLoss: 0.023009\t Accuracy:94.985%\n",
      "Rate : 87 [10240/49000 (21%)]\tLoss: 0.005833\t Accuracy:95.084%\n",
      "Rate : 87 [10880/49000 (22%)]\tLoss: 0.121973\t Accuracy:95.170%\n",
      "Rate : 87 [11520/49000 (23%)]\tLoss: 0.161419\t Accuracy:95.178%\n",
      "Rate : 87 [12160/49000 (25%)]\tLoss: 0.260970\t Accuracy:95.259%\n",
      "Rate : 87 [12800/49000 (26%)]\tLoss: 0.264567\t Accuracy:95.348%\n",
      "Rate : 87 [13440/49000 (27%)]\tLoss: 0.017361\t Accuracy:95.442%\n",
      "Rate : 87 [14080/49000 (29%)]\tLoss: 0.195759\t Accuracy:95.507%\n",
      "Rate : 87 [14720/49000 (30%)]\tLoss: 0.310865\t Accuracy:95.580%\n",
      "Rate : 87 [15360/49000 (31%)]\tLoss: 0.026268\t Accuracy:95.602%\n",
      "Rate : 87 [16000/49000 (33%)]\tLoss: 0.051251\t Accuracy:95.677%\n",
      "Rate : 87 [16640/49000 (34%)]\tLoss: 0.051828\t Accuracy:95.735%\n",
      "Rate : 87 [17280/49000 (35%)]\tLoss: 0.050192\t Accuracy:95.783%\n",
      "Rate : 87 [17920/49000 (37%)]\tLoss: 0.017046\t Accuracy:95.822%\n",
      "Rate : 87 [18560/49000 (38%)]\tLoss: 0.466519\t Accuracy:95.891%\n",
      "Rate : 87 [19200/49000 (39%)]\tLoss: 0.275285\t Accuracy:95.934%\n",
      "Rate : 87 [19840/49000 (40%)]\tLoss: 0.207443\t Accuracy:95.999%\n",
      "Rate : 87 [20480/49000 (42%)]\tLoss: 0.016661\t Accuracy:96.027%\n",
      "Rate : 87 [21120/49000 (43%)]\tLoss: 0.097764\t Accuracy:96.067%\n",
      "Rate : 87 [21760/49000 (44%)]\tLoss: 0.150192\t Accuracy:96.104%\n",
      "Rate : 87 [22400/49000 (46%)]\tLoss: 0.323098\t Accuracy:96.108%\n",
      "Rate : 87 [23040/49000 (47%)]\tLoss: 0.301633\t Accuracy:96.151%\n",
      "Rate : 87 [23680/49000 (48%)]\tLoss: 0.099675\t Accuracy:96.209%\n",
      "Rate : 87 [24320/49000 (50%)]\tLoss: 0.009822\t Accuracy:96.255%\n",
      "Rate : 87 [24960/49000 (51%)]\tLoss: 0.148471\t Accuracy:96.299%\n",
      "Rate : 87 [25600/49000 (52%)]\tLoss: 0.014257\t Accuracy:96.321%\n",
      "Rate : 87 [26240/49000 (54%)]\tLoss: 0.001939\t Accuracy:96.338%\n",
      "Rate : 87 [26880/49000 (55%)]\tLoss: 0.017527\t Accuracy:96.362%\n",
      "Rate : 87 [27520/49000 (56%)]\tLoss: 0.004497\t Accuracy:96.392%\n",
      "Rate : 87 [28160/49000 (57%)]\tLoss: 0.194650\t Accuracy:96.435%\n",
      "Rate : 87 [28800/49000 (59%)]\tLoss: 0.026380\t Accuracy:96.466%\n",
      "Rate : 87 [29440/49000 (60%)]\tLoss: 0.059178\t Accuracy:96.475%\n",
      "Rate : 87 [30080/49000 (61%)]\tLoss: 0.071301\t Accuracy:96.490%\n",
      "Rate : 87 [30720/49000 (63%)]\tLoss: 0.006365\t Accuracy:96.524%\n",
      "Rate : 87 [31360/49000 (64%)]\tLoss: 0.110068\t Accuracy:96.556%\n",
      "Rate : 87 [32000/49000 (65%)]\tLoss: 0.013629\t Accuracy:96.603%\n",
      "Rate : 87 [32640/49000 (67%)]\tLoss: 0.028163\t Accuracy:96.639%\n",
      "Rate : 87 [33280/49000 (68%)]\tLoss: 0.176025\t Accuracy:96.662%\n",
      "Rate : 87 [33920/49000 (69%)]\tLoss: 0.011627\t Accuracy:96.672%\n",
      "Rate : 87 [34560/49000 (70%)]\tLoss: 0.039344\t Accuracy:96.693%\n",
      "Rate : 87 [35200/49000 (72%)]\tLoss: 0.020226\t Accuracy:96.708%\n",
      "Rate : 87 [35840/49000 (73%)]\tLoss: 0.022193\t Accuracy:96.738%\n",
      "Rate : 87 [36480/49000 (74%)]\tLoss: 0.022805\t Accuracy:96.746%\n",
      "Rate : 87 [37120/49000 (76%)]\tLoss: 0.036235\t Accuracy:96.767%\n",
      "Rate : 87 [37760/49000 (77%)]\tLoss: 0.082247\t Accuracy:96.777%\n",
      "Rate : 87 [38400/49000 (78%)]\tLoss: 0.072360\t Accuracy:96.807%\n",
      "Rate : 87 [39040/49000 (80%)]\tLoss: 0.003437\t Accuracy:96.834%\n",
      "Rate : 87 [39680/49000 (81%)]\tLoss: 0.052551\t Accuracy:96.865%\n",
      "Rate : 87 [40320/49000 (82%)]\tLoss: 0.005159\t Accuracy:96.868%\n",
      "Rate : 87 [40960/49000 (84%)]\tLoss: 0.052730\t Accuracy:96.877%\n",
      "Rate : 87 [41600/49000 (85%)]\tLoss: 0.047792\t Accuracy:96.899%\n",
      "Rate : 87 [42240/49000 (86%)]\tLoss: 0.009362\t Accuracy:96.922%\n",
      "Rate : 87 [42880/49000 (87%)]\tLoss: 0.206332\t Accuracy:96.931%\n",
      "Rate : 87 [43520/49000 (89%)]\tLoss: 0.079080\t Accuracy:96.946%\n",
      "Rate : 87 [44160/49000 (90%)]\tLoss: 0.022579\t Accuracy:96.959%\n",
      "Rate : 87 [44800/49000 (91%)]\tLoss: 0.055100\t Accuracy:96.973%\n",
      "Rate : 87 [45440/49000 (93%)]\tLoss: 0.030514\t Accuracy:96.987%\n",
      "Rate : 87 [46080/49000 (94%)]\tLoss: 0.095846\t Accuracy:96.999%\n",
      "Rate : 87 [46720/49000 (95%)]\tLoss: 0.077621\t Accuracy:97.010%\n",
      "Rate : 87 [47360/49000 (97%)]\tLoss: 0.011216\t Accuracy:97.027%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate : 87 [48000/49000 (98%)]\tLoss: 0.059969\t Accuracy:97.039%\n",
      "Rate : 87 [48640/49000 (99%)]\tLoss: 0.025394\t Accuracy:97.060%\n",
      "Rate : 87 [0/49000 (0%)]\tLoss: 0.068750\t Accuracy:96.875%\n",
      "Rate : 87 [640/49000 (1%)]\tLoss: 0.027252\t Accuracy:98.214%\n",
      "Rate : 87 [1280/49000 (3%)]\tLoss: 0.387721\t Accuracy:97.713%\n",
      "Rate : 87 [1920/49000 (4%)]\tLoss: 0.007388\t Accuracy:97.797%\n",
      "Rate : 87 [2560/49000 (5%)]\tLoss: 0.006001\t Accuracy:97.685%\n",
      "Rate : 87 [3200/49000 (7%)]\tLoss: 0.070003\t Accuracy:97.710%\n",
      "Rate : 87 [3840/49000 (8%)]\tLoss: 0.011492\t Accuracy:97.986%\n",
      "Rate : 87 [4480/49000 (9%)]\tLoss: 0.092617\t Accuracy:98.094%\n",
      "Rate : 87 [5120/49000 (10%)]\tLoss: 0.096720\t Accuracy:98.195%\n",
      "Rate : 87 [5760/49000 (12%)]\tLoss: 0.026915\t Accuracy:98.222%\n",
      "Rate : 87 [6400/49000 (13%)]\tLoss: 0.023685\t Accuracy:98.259%\n",
      "Rate : 87 [7040/49000 (14%)]\tLoss: 0.039611\t Accuracy:98.232%\n",
      "Rate : 87 [7680/49000 (16%)]\tLoss: 0.071087\t Accuracy:98.224%\n",
      "Rate : 87 [8320/49000 (17%)]\tLoss: 0.085091\t Accuracy:98.276%\n",
      "Rate : 87 [8960/49000 (18%)]\tLoss: 0.059724\t Accuracy:98.232%\n",
      "Rate : 87 [9600/49000 (20%)]\tLoss: 0.004543\t Accuracy:98.245%\n",
      "Rate : 87 [10240/49000 (21%)]\tLoss: 0.014753\t Accuracy:98.267%\n",
      "Rate : 87 [10880/49000 (22%)]\tLoss: 0.023950\t Accuracy:98.277%\n",
      "Rate : 87 [11520/49000 (23%)]\tLoss: 0.014132\t Accuracy:98.251%\n",
      "Rate : 87 [12160/49000 (25%)]\tLoss: 0.142760\t Accuracy:98.220%\n",
      "Rate : 87 [12800/49000 (26%)]\tLoss: 0.231716\t Accuracy:98.176%\n",
      "Rate : 87 [13440/49000 (27%)]\tLoss: 0.002036\t Accuracy:98.174%\n",
      "Rate : 87 [14080/49000 (29%)]\tLoss: 0.122255\t Accuracy:98.172%\n",
      "Rate : 87 [14720/49000 (30%)]\tLoss: 0.135822\t Accuracy:98.170%\n",
      "Rate : 87 [15360/49000 (31%)]\tLoss: 0.046711\t Accuracy:98.194%\n",
      "Rate : 87 [16000/49000 (33%)]\tLoss: 0.044060\t Accuracy:98.204%\n",
      "Rate : 87 [16640/49000 (34%)]\tLoss: 0.050066\t Accuracy:98.243%\n",
      "Rate : 87 [17280/49000 (35%)]\tLoss: 0.060622\t Accuracy:98.261%\n",
      "Rate : 87 [17920/49000 (37%)]\tLoss: 0.007939\t Accuracy:98.268%\n",
      "Rate : 87 [18560/49000 (38%)]\tLoss: 0.022027\t Accuracy:98.273%\n",
      "Rate : 87 [19200/49000 (39%)]\tLoss: 0.227291\t Accuracy:98.269%\n",
      "Rate : 87 [19840/49000 (40%)]\tLoss: 0.140940\t Accuracy:98.284%\n",
      "Rate : 87 [20480/49000 (42%)]\tLoss: 0.009995\t Accuracy:98.284%\n",
      "Rate : 87 [21120/49000 (43%)]\tLoss: 0.200082\t Accuracy:98.289%\n",
      "Rate : 87 [21760/49000 (44%)]\tLoss: 0.182841\t Accuracy:98.284%\n",
      "Rate : 87 [22400/49000 (46%)]\tLoss: 0.168896\t Accuracy:98.275%\n",
      "Rate : 87 [23040/49000 (47%)]\tLoss: 0.052728\t Accuracy:98.266%\n",
      "Rate : 87 [23680/49000 (48%)]\tLoss: 0.019524\t Accuracy:98.288%\n",
      "Rate : 87 [24320/49000 (50%)]\tLoss: 0.004692\t Accuracy:98.304%\n",
      "Rate : 87 [24960/49000 (51%)]\tLoss: 0.099933\t Accuracy:98.319%\n",
      "Rate : 87 [25600/49000 (52%)]\tLoss: 0.000596\t Accuracy:98.299%\n",
      "Rate : 87 [26240/49000 (54%)]\tLoss: 0.000687\t Accuracy:98.310%\n",
      "Rate : 87 [26880/49000 (55%)]\tLoss: 0.008470\t Accuracy:98.306%\n",
      "Rate : 87 [27520/49000 (56%)]\tLoss: 0.001413\t Accuracy:98.312%\n",
      "Rate : 87 [28160/49000 (57%)]\tLoss: 0.037638\t Accuracy:98.333%\n",
      "Rate : 87 [28800/49000 (59%)]\tLoss: 0.012459\t Accuracy:98.346%\n",
      "Rate : 87 [29440/49000 (60%)]\tLoss: 0.007097\t Accuracy:98.337%\n",
      "Rate : 87 [30080/49000 (61%)]\tLoss: 0.031535\t Accuracy:98.340%\n",
      "Rate : 87 [30720/49000 (63%)]\tLoss: 0.001439\t Accuracy:98.338%\n",
      "Rate : 87 [31360/49000 (64%)]\tLoss: 0.145062\t Accuracy:98.334%\n",
      "Rate : 87 [32000/49000 (65%)]\tLoss: 0.034787\t Accuracy:98.349%\n",
      "Rate : 87 [32640/49000 (67%)]\tLoss: 0.004651\t Accuracy:98.347%\n",
      "Rate : 87 [33280/49000 (68%)]\tLoss: 0.086516\t Accuracy:98.340%\n",
      "Rate : 87 [33920/49000 (69%)]\tLoss: 0.006334\t Accuracy:98.327%\n",
      "Rate : 87 [34560/49000 (70%)]\tLoss: 0.048956\t Accuracy:98.329%\n",
      "Rate : 87 [35200/49000 (72%)]\tLoss: 0.009065\t Accuracy:98.320%\n",
      "Rate : 87 [35840/49000 (73%)]\tLoss: 0.002525\t Accuracy:98.327%\n",
      "Rate : 87 [36480/49000 (74%)]\tLoss: 0.006637\t Accuracy:98.318%\n",
      "Rate : 87 [37120/49000 (76%)]\tLoss: 0.001568\t Accuracy:98.339%\n",
      "Rate : 87 [37760/49000 (77%)]\tLoss: 0.299583\t Accuracy:98.333%\n",
      "Rate : 87 [38400/49000 (78%)]\tLoss: 0.050653\t Accuracy:98.343%\n",
      "Rate : 87 [39040/49000 (80%)]\tLoss: 0.000774\t Accuracy:98.342%\n",
      "Rate : 87 [39680/49000 (81%)]\tLoss: 0.037348\t Accuracy:98.353%\n",
      "Rate : 87 [40320/49000 (82%)]\tLoss: 0.003527\t Accuracy:98.350%\n",
      "Rate : 87 [40960/49000 (84%)]\tLoss: 0.027625\t Accuracy:98.348%\n",
      "Rate : 87 [41600/49000 (85%)]\tLoss: 0.045938\t Accuracy:98.362%\n",
      "Rate : 87 [42240/49000 (86%)]\tLoss: 0.002205\t Accuracy:98.370%\n",
      "Rate : 87 [42880/49000 (87%)]\tLoss: 0.100782\t Accuracy:98.371%\n",
      "Rate : 87 [43520/49000 (89%)]\tLoss: 0.071001\t Accuracy:98.377%\n",
      "Rate : 87 [44160/49000 (90%)]\tLoss: 0.004902\t Accuracy:98.373%\n",
      "Rate : 87 [44800/49000 (91%)]\tLoss: 0.054342\t Accuracy:98.372%\n",
      "Rate : 87 [45440/49000 (93%)]\tLoss: 0.017262\t Accuracy:98.377%\n",
      "Rate : 87 [46080/49000 (94%)]\tLoss: 0.113335\t Accuracy:98.382%\n",
      "Rate : 87 [46720/49000 (95%)]\tLoss: 0.074053\t Accuracy:98.387%\n",
      "Rate : 87 [47360/49000 (97%)]\tLoss: 0.010355\t Accuracy:98.386%\n",
      "Rate : 87 [48000/49000 (98%)]\tLoss: 0.017825\t Accuracy:98.384%\n",
      "Rate : 87 [48640/49000 (99%)]\tLoss: 0.065229\t Accuracy:98.389%\n",
      "Rate : 89 [0/49000 (0%)]\tLoss: 0.764123\t Accuracy:81.250%\n",
      "Rate : 89 [640/49000 (1%)]\tLoss: 0.340618\t Accuracy:80.060%\n",
      "Rate : 89 [1280/49000 (3%)]\tLoss: 0.223539\t Accuracy:83.155%\n",
      "Rate : 89 [1920/49000 (4%)]\tLoss: 0.218237\t Accuracy:86.066%\n",
      "Rate : 89 [2560/49000 (5%)]\tLoss: 0.235029\t Accuracy:87.809%\n",
      "Rate : 89 [3200/49000 (7%)]\tLoss: 0.289826\t Accuracy:88.830%\n",
      "Rate : 89 [3840/49000 (8%)]\tLoss: 0.491408\t Accuracy:89.695%\n",
      "Rate : 89 [4480/49000 (9%)]\tLoss: 0.276729\t Accuracy:90.426%\n",
      "Rate : 89 [5120/49000 (10%)]\tLoss: 0.200328\t Accuracy:90.955%\n",
      "Rate : 89 [5760/49000 (12%)]\tLoss: 0.079516\t Accuracy:91.436%\n",
      "Rate : 89 [6400/49000 (13%)]\tLoss: 0.112262\t Accuracy:91.884%\n",
      "Rate : 89 [7040/49000 (14%)]\tLoss: 0.058952\t Accuracy:92.265%\n",
      "Rate : 89 [7680/49000 (16%)]\tLoss: 0.116825\t Accuracy:92.596%\n",
      "Rate : 89 [8320/49000 (17%)]\tLoss: 0.286631\t Accuracy:92.876%\n",
      "Rate : 89 [8960/49000 (18%)]\tLoss: 0.075573\t Accuracy:93.094%\n",
      "Rate : 89 [9600/49000 (20%)]\tLoss: 0.048155\t Accuracy:93.283%\n",
      "Rate : 89 [10240/49000 (21%)]\tLoss: 0.201284\t Accuracy:93.468%\n",
      "Rate : 89 [10880/49000 (22%)]\tLoss: 0.178741\t Accuracy:93.658%\n",
      "Rate : 89 [11520/49000 (23%)]\tLoss: 0.026739\t Accuracy:93.759%\n",
      "Rate : 89 [12160/49000 (25%)]\tLoss: 0.128526\t Accuracy:93.939%\n",
      "Rate : 89 [12800/49000 (26%)]\tLoss: 0.105593\t Accuracy:94.038%\n",
      "Rate : 89 [13440/49000 (27%)]\tLoss: 0.088174\t Accuracy:94.188%\n",
      "Rate : 89 [14080/49000 (29%)]\tLoss: 0.187645\t Accuracy:94.232%\n",
      "Rate : 89 [14720/49000 (30%)]\tLoss: 0.057173\t Accuracy:94.333%\n",
      "Rate : 89 [15360/49000 (31%)]\tLoss: 0.012361\t Accuracy:94.484%\n",
      "Rate : 89 [16000/49000 (33%)]\tLoss: 0.051742\t Accuracy:94.605%\n",
      "Rate : 89 [16640/49000 (34%)]\tLoss: 0.032397\t Accuracy:94.758%\n",
      "Rate : 89 [17280/49000 (35%)]\tLoss: 0.051209\t Accuracy:94.859%\n",
      "Rate : 89 [17920/49000 (37%)]\tLoss: 0.013132\t Accuracy:94.925%\n",
      "Rate : 89 [18560/49000 (38%)]\tLoss: 0.008283\t Accuracy:95.009%\n",
      "Rate : 89 [19200/49000 (39%)]\tLoss: 0.290839\t Accuracy:95.081%\n",
      "Rate : 89 [19840/49000 (40%)]\tLoss: 0.244098\t Accuracy:95.169%\n",
      "Rate : 89 [20480/49000 (42%)]\tLoss: 0.041591\t Accuracy:95.217%\n",
      "Rate : 89 [21120/49000 (43%)]\tLoss: 0.163164\t Accuracy:95.272%\n",
      "Rate : 89 [21760/49000 (44%)]\tLoss: 0.209022\t Accuracy:95.310%\n",
      "Rate : 89 [22400/49000 (46%)]\tLoss: 0.064172\t Accuracy:95.377%\n",
      "Rate : 89 [23040/49000 (47%)]\tLoss: 0.057638\t Accuracy:95.436%\n",
      "Rate : 89 [23680/49000 (48%)]\tLoss: 0.029516\t Accuracy:95.496%\n",
      "Rate : 89 [24320/49000 (50%)]\tLoss: 0.038685\t Accuracy:95.577%\n",
      "Rate : 89 [24960/49000 (51%)]\tLoss: 0.106068\t Accuracy:95.639%\n",
      "Rate : 89 [25600/49000 (52%)]\tLoss: 0.035838\t Accuracy:95.666%\n",
      "Rate : 89 [26240/49000 (54%)]\tLoss: 0.056487\t Accuracy:95.706%\n",
      "Rate : 89 [26880/49000 (55%)]\tLoss: 0.005206\t Accuracy:95.753%\n",
      "Rate : 89 [27520/49000 (56%)]\tLoss: 0.043658\t Accuracy:95.808%\n",
      "Rate : 89 [28160/49000 (57%)]\tLoss: 0.022923\t Accuracy:95.861%\n",
      "Rate : 89 [28800/49000 (59%)]\tLoss: 0.024003\t Accuracy:95.918%\n",
      "Rate : 89 [29440/49000 (60%)]\tLoss: 0.210977\t Accuracy:95.911%\n",
      "Rate : 89 [30080/49000 (61%)]\tLoss: 0.038888\t Accuracy:95.945%\n",
      "Rate : 89 [30720/49000 (63%)]\tLoss: 0.012697\t Accuracy:95.977%\n",
      "Rate : 89 [31360/49000 (64%)]\tLoss: 0.360713\t Accuracy:96.015%\n",
      "Rate : 89 [32000/49000 (65%)]\tLoss: 0.012949\t Accuracy:96.057%\n",
      "Rate : 89 [32640/49000 (67%)]\tLoss: 0.036994\t Accuracy:96.101%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate : 89 [33280/49000 (68%)]\tLoss: 0.124340\t Accuracy:96.125%\n",
      "Rate : 89 [33920/49000 (69%)]\tLoss: 0.078177\t Accuracy:96.148%\n",
      "Rate : 89 [34560/49000 (70%)]\tLoss: 0.065722\t Accuracy:96.181%\n",
      "Rate : 89 [35200/49000 (72%)]\tLoss: 0.071129\t Accuracy:96.182%\n",
      "Rate : 89 [35840/49000 (73%)]\tLoss: 0.018471\t Accuracy:96.220%\n",
      "Rate : 89 [36480/49000 (74%)]\tLoss: 0.018648\t Accuracy:96.253%\n",
      "Rate : 89 [37120/49000 (76%)]\tLoss: 0.004522\t Accuracy:96.277%\n",
      "Rate : 89 [37760/49000 (77%)]\tLoss: 0.061294\t Accuracy:96.285%\n",
      "Rate : 89 [38400/49000 (78%)]\tLoss: 0.054402\t Accuracy:96.323%\n",
      "Rate : 89 [39040/49000 (80%)]\tLoss: 0.003971\t Accuracy:96.330%\n",
      "Rate : 89 [39680/49000 (81%)]\tLoss: 0.021897\t Accuracy:96.366%\n",
      "Rate : 89 [40320/49000 (82%)]\tLoss: 0.009034\t Accuracy:96.394%\n",
      "Rate : 89 [40960/49000 (84%)]\tLoss: 0.037528\t Accuracy:96.438%\n",
      "Rate : 89 [41600/49000 (85%)]\tLoss: 0.018116\t Accuracy:96.471%\n",
      "Rate : 89 [42240/49000 (86%)]\tLoss: 0.018899\t Accuracy:96.496%\n",
      "Rate : 89 [42880/49000 (87%)]\tLoss: 0.043080\t Accuracy:96.518%\n",
      "Rate : 89 [43520/49000 (89%)]\tLoss: 0.237706\t Accuracy:96.547%\n",
      "Rate : 89 [44160/49000 (90%)]\tLoss: 0.038219\t Accuracy:96.560%\n",
      "Rate : 89 [44800/49000 (91%)]\tLoss: 0.052001\t Accuracy:96.558%\n",
      "Rate : 89 [45440/49000 (93%)]\tLoss: 0.070256\t Accuracy:96.574%\n",
      "Rate : 89 [46080/49000 (94%)]\tLoss: 0.084373\t Accuracy:96.584%\n",
      "Rate : 89 [46720/49000 (95%)]\tLoss: 0.032518\t Accuracy:96.595%\n",
      "Rate : 89 [47360/49000 (97%)]\tLoss: 0.002361\t Accuracy:96.613%\n",
      "Rate : 89 [48000/49000 (98%)]\tLoss: 0.016280\t Accuracy:96.625%\n",
      "Rate : 89 [48640/49000 (99%)]\tLoss: 0.053058\t Accuracy:96.631%\n",
      "Rate : 89 [0/49000 (0%)]\tLoss: 0.016532\t Accuracy:100.000%\n",
      "Rate : 89 [640/49000 (1%)]\tLoss: 0.074040\t Accuracy:98.214%\n",
      "Rate : 89 [1280/49000 (3%)]\tLoss: 0.156252\t Accuracy:98.018%\n",
      "Rate : 89 [1920/49000 (4%)]\tLoss: 0.027906\t Accuracy:97.951%\n",
      "Rate : 89 [2560/49000 (5%)]\tLoss: 0.078551\t Accuracy:97.840%\n",
      "Rate : 89 [3200/49000 (7%)]\tLoss: 0.024746\t Accuracy:98.020%\n",
      "Rate : 89 [3840/49000 (8%)]\tLoss: 0.016900\t Accuracy:98.037%\n",
      "Rate : 89 [4480/49000 (9%)]\tLoss: 0.029828\t Accuracy:98.160%\n",
      "Rate : 89 [5120/49000 (10%)]\tLoss: 0.070154\t Accuracy:98.234%\n",
      "Rate : 89 [5760/49000 (12%)]\tLoss: 0.013048\t Accuracy:98.222%\n",
      "Rate : 89 [6400/49000 (13%)]\tLoss: 0.004695\t Accuracy:98.274%\n",
      "Rate : 89 [7040/49000 (14%)]\tLoss: 0.023246\t Accuracy:98.162%\n",
      "Rate : 89 [7680/49000 (16%)]\tLoss: 0.096318\t Accuracy:98.159%\n",
      "Rate : 89 [8320/49000 (17%)]\tLoss: 0.023832\t Accuracy:98.156%\n",
      "Rate : 89 [8960/49000 (18%)]\tLoss: 0.051765\t Accuracy:98.109%\n",
      "Rate : 89 [9600/49000 (20%)]\tLoss: 0.012472\t Accuracy:98.069%\n",
      "Rate : 89 [10240/49000 (21%)]\tLoss: 0.093852\t Accuracy:98.063%\n",
      "Rate : 89 [10880/49000 (22%)]\tLoss: 0.062847\t Accuracy:98.094%\n",
      "Rate : 89 [11520/49000 (23%)]\tLoss: 0.007905\t Accuracy:98.113%\n",
      "Rate : 89 [12160/49000 (25%)]\tLoss: 0.421914\t Accuracy:98.114%\n",
      "Rate : 89 [12800/49000 (26%)]\tLoss: 0.202123\t Accuracy:98.091%\n",
      "Rate : 89 [13440/49000 (27%)]\tLoss: 0.007694\t Accuracy:98.092%\n",
      "Rate : 89 [14080/49000 (29%)]\tLoss: 0.160915\t Accuracy:98.016%\n",
      "Rate : 89 [14720/49000 (30%)]\tLoss: 0.027041\t Accuracy:97.993%\n",
      "Rate : 89 [15360/49000 (31%)]\tLoss: 0.004199\t Accuracy:97.992%\n",
      "Rate : 89 [16000/49000 (33%)]\tLoss: 0.042910\t Accuracy:98.010%\n",
      "Rate : 89 [16640/49000 (34%)]\tLoss: 0.039577\t Accuracy:98.051%\n",
      "Rate : 89 [17280/49000 (35%)]\tLoss: 0.032077\t Accuracy:98.048%\n",
      "Rate : 89 [17920/49000 (37%)]\tLoss: 0.011049\t Accuracy:98.061%\n",
      "Rate : 89 [18560/49000 (38%)]\tLoss: 0.003534\t Accuracy:98.074%\n",
      "Rate : 89 [19200/49000 (39%)]\tLoss: 0.213104\t Accuracy:98.071%\n",
      "Rate : 89 [19840/49000 (40%)]\tLoss: 0.084974\t Accuracy:98.083%\n",
      "Rate : 89 [20480/49000 (42%)]\tLoss: 0.006440\t Accuracy:98.065%\n",
      "Rate : 89 [21120/49000 (43%)]\tLoss: 0.092886\t Accuracy:98.066%\n",
      "Rate : 89 [21760/49000 (44%)]\tLoss: 0.159107\t Accuracy:98.064%\n",
      "Rate : 89 [22400/49000 (46%)]\tLoss: 0.022257\t Accuracy:98.074%\n",
      "Rate : 89 [23040/49000 (47%)]\tLoss: 0.173866\t Accuracy:98.071%\n",
      "Rate : 89 [23680/49000 (48%)]\tLoss: 0.057282\t Accuracy:98.094%\n",
      "Rate : 89 [24320/49000 (50%)]\tLoss: 0.051496\t Accuracy:98.111%\n",
      "Rate : 89 [24960/49000 (51%)]\tLoss: 0.116823\t Accuracy:98.111%\n",
      "Rate : 89 [25600/49000 (52%)]\tLoss: 0.015800\t Accuracy:98.100%\n",
      "Rate : 89 [26240/49000 (54%)]\tLoss: 0.023663\t Accuracy:98.108%\n",
      "Rate : 89 [26880/49000 (55%)]\tLoss: 0.002490\t Accuracy:98.116%\n",
      "Rate : 89 [27520/49000 (56%)]\tLoss: 0.005665\t Accuracy:98.138%\n",
      "Rate : 89 [28160/49000 (57%)]\tLoss: 0.016852\t Accuracy:98.145%\n",
      "Rate : 89 [28800/49000 (59%)]\tLoss: 0.038695\t Accuracy:98.137%\n",
      "Rate : 89 [29440/49000 (60%)]\tLoss: 0.118649\t Accuracy:98.130%\n",
      "Rate : 89 [30080/49000 (61%)]\tLoss: 0.020222\t Accuracy:98.130%\n",
      "Rate : 89 [30720/49000 (63%)]\tLoss: 0.007135\t Accuracy:98.127%\n",
      "Rate : 89 [31360/49000 (64%)]\tLoss: 0.313718\t Accuracy:98.133%\n",
      "Rate : 89 [32000/49000 (65%)]\tLoss: 0.007012\t Accuracy:98.149%\n",
      "Rate : 89 [32640/49000 (67%)]\tLoss: 0.030533\t Accuracy:98.167%\n",
      "Rate : 89 [33280/49000 (68%)]\tLoss: 0.183518\t Accuracy:98.172%\n",
      "Rate : 89 [33920/49000 (69%)]\tLoss: 0.054197\t Accuracy:98.174%\n",
      "Rate : 89 [34560/49000 (70%)]\tLoss: 0.029797\t Accuracy:98.182%\n",
      "Rate : 89 [35200/49000 (72%)]\tLoss: 0.030980\t Accuracy:98.183%\n",
      "Rate : 89 [35840/49000 (73%)]\tLoss: 0.006645\t Accuracy:98.199%\n",
      "Rate : 89 [36480/49000 (74%)]\tLoss: 0.023508\t Accuracy:98.209%\n",
      "Rate : 89 [37120/49000 (76%)]\tLoss: 0.003355\t Accuracy:98.215%\n",
      "Rate : 89 [37760/49000 (77%)]\tLoss: 0.030466\t Accuracy:98.219%\n",
      "Rate : 89 [38400/49000 (78%)]\tLoss: 0.039947\t Accuracy:98.225%\n",
      "Rate : 89 [39040/49000 (80%)]\tLoss: 0.002173\t Accuracy:98.219%\n",
      "Rate : 89 [39680/49000 (81%)]\tLoss: 0.013319\t Accuracy:98.235%\n",
      "Rate : 89 [40320/49000 (82%)]\tLoss: 0.004594\t Accuracy:98.236%\n",
      "Rate : 89 [40960/49000 (84%)]\tLoss: 0.022970\t Accuracy:98.246%\n",
      "Rate : 89 [41600/49000 (85%)]\tLoss: 0.010842\t Accuracy:98.256%\n",
      "Rate : 89 [42240/49000 (86%)]\tLoss: 0.017477\t Accuracy:98.261%\n",
      "Rate : 89 [42880/49000 (87%)]\tLoss: 0.069039\t Accuracy:98.257%\n",
      "Rate : 89 [43520/49000 (89%)]\tLoss: 0.006210\t Accuracy:98.266%\n",
      "Rate : 89 [44160/49000 (90%)]\tLoss: 0.019752\t Accuracy:98.264%\n",
      "Rate : 89 [44800/49000 (91%)]\tLoss: 0.043742\t Accuracy:98.262%\n",
      "Rate : 89 [45440/49000 (93%)]\tLoss: 0.033324\t Accuracy:98.269%\n",
      "Rate : 89 [46080/49000 (94%)]\tLoss: 0.023408\t Accuracy:98.278%\n",
      "Rate : 89 [46720/49000 (95%)]\tLoss: 0.014461\t Accuracy:98.274%\n",
      "Rate : 89 [47360/49000 (97%)]\tLoss: 0.000611\t Accuracy:98.276%\n",
      "Rate : 89 [48000/49000 (98%)]\tLoss: 0.008232\t Accuracy:98.262%\n",
      "Rate : 89 [48640/49000 (99%)]\tLoss: 0.091289\t Accuracy:98.268%\n",
      "Rate : 90 [0/49000 (0%)]\tLoss: 0.490438\t Accuracy:78.125%\n",
      "Rate : 90 [640/49000 (1%)]\tLoss: 0.702440\t Accuracy:80.357%\n",
      "Rate : 90 [1280/49000 (3%)]\tLoss: 0.372979\t Accuracy:83.079%\n",
      "Rate : 90 [1920/49000 (4%)]\tLoss: 0.344035\t Accuracy:84.477%\n",
      "Rate : 90 [2560/49000 (5%)]\tLoss: 0.447023\t Accuracy:85.494%\n",
      "Rate : 90 [3200/49000 (7%)]\tLoss: 0.497324\t Accuracy:86.479%\n",
      "Rate : 90 [3840/49000 (8%)]\tLoss: 0.166527\t Accuracy:87.345%\n",
      "Rate : 90 [4480/49000 (9%)]\tLoss: 0.236811\t Accuracy:87.544%\n",
      "Rate : 90 [5120/49000 (10%)]\tLoss: 0.430643\t Accuracy:88.179%\n",
      "Rate : 90 [5760/49000 (12%)]\tLoss: 0.066936\t Accuracy:88.691%\n",
      "Rate : 90 [6400/49000 (13%)]\tLoss: 0.194961\t Accuracy:89.210%\n",
      "Rate : 90 [7040/49000 (14%)]\tLoss: 0.102057\t Accuracy:89.649%\n",
      "Rate : 90 [7680/49000 (16%)]\tLoss: 0.360922\t Accuracy:89.912%\n",
      "Rate : 90 [8320/49000 (17%)]\tLoss: 0.246138\t Accuracy:90.278%\n",
      "Rate : 90 [8960/49000 (18%)]\tLoss: 0.228074\t Accuracy:90.425%\n",
      "Rate : 90 [9600/49000 (20%)]\tLoss: 0.075966\t Accuracy:90.573%\n",
      "Rate : 90 [10240/49000 (21%)]\tLoss: 0.215684\t Accuracy:90.722%\n",
      "Rate : 90 [10880/49000 (22%)]\tLoss: 0.289367\t Accuracy:90.973%\n",
      "Rate : 90 [11520/49000 (23%)]\tLoss: 0.191252\t Accuracy:91.144%\n",
      "Rate : 90 [12160/49000 (25%)]\tLoss: 0.177427\t Accuracy:91.306%\n",
      "Rate : 90 [12800/49000 (26%)]\tLoss: 0.150244\t Accuracy:91.467%\n",
      "Rate : 90 [13440/49000 (27%)]\tLoss: 0.072642\t Accuracy:91.597%\n",
      "Rate : 90 [14080/49000 (29%)]\tLoss: 0.150577\t Accuracy:91.766%\n",
      "Rate : 90 [14720/49000 (30%)]\tLoss: 0.054745\t Accuracy:91.845%\n",
      "Rate : 90 [15360/49000 (31%)]\tLoss: 0.214436\t Accuracy:91.976%\n",
      "Rate : 90 [16000/49000 (33%)]\tLoss: 0.167725\t Accuracy:92.097%\n",
      "Rate : 90 [16640/49000 (34%)]\tLoss: 0.106751\t Accuracy:92.232%\n",
      "Rate : 90 [17280/49000 (35%)]\tLoss: 0.233752\t Accuracy:92.369%\n",
      "Rate : 90 [17920/49000 (37%)]\tLoss: 0.154684\t Accuracy:92.435%\n",
      "Rate : 90 [18560/49000 (38%)]\tLoss: 0.054444\t Accuracy:92.491%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate : 90 [19200/49000 (39%)]\tLoss: 0.575244\t Accuracy:92.585%\n",
      "Rate : 90 [19840/49000 (40%)]\tLoss: 0.281078\t Accuracy:92.628%\n",
      "Rate : 90 [20480/49000 (42%)]\tLoss: 0.092745\t Accuracy:92.682%\n",
      "Rate : 90 [21120/49000 (43%)]\tLoss: 0.147277\t Accuracy:92.767%\n",
      "Rate : 90 [21760/49000 (44%)]\tLoss: 0.225129\t Accuracy:92.832%\n",
      "Rate : 90 [22400/49000 (46%)]\tLoss: 0.496192\t Accuracy:92.885%\n",
      "Rate : 90 [23040/49000 (47%)]\tLoss: 0.317466\t Accuracy:92.983%\n",
      "Rate : 90 [23680/49000 (48%)]\tLoss: 0.065247\t Accuracy:93.071%\n",
      "Rate : 90 [24320/49000 (50%)]\tLoss: 0.102704\t Accuracy:93.146%\n",
      "Rate : 90 [24960/49000 (51%)]\tLoss: 0.147467\t Accuracy:93.194%\n",
      "Rate : 90 [25600/49000 (52%)]\tLoss: 0.084376\t Accuracy:93.223%\n",
      "Rate : 90 [26240/49000 (54%)]\tLoss: 0.097825\t Accuracy:93.270%\n",
      "Rate : 90 [26880/49000 (55%)]\tLoss: 0.234092\t Accuracy:93.323%\n",
      "Rate : 90 [27520/49000 (56%)]\tLoss: 0.116771\t Accuracy:93.358%\n",
      "Rate : 90 [28160/49000 (57%)]\tLoss: 0.075731\t Accuracy:93.427%\n",
      "Rate : 90 [28800/49000 (59%)]\tLoss: 0.173496\t Accuracy:93.483%\n",
      "Rate : 90 [29440/49000 (60%)]\tLoss: 0.154083\t Accuracy:93.533%\n",
      "Rate : 90 [30080/49000 (61%)]\tLoss: 0.121552\t Accuracy:93.584%\n",
      "Rate : 90 [30720/49000 (63%)]\tLoss: 0.059732\t Accuracy:93.623%\n",
      "Rate : 90 [31360/49000 (64%)]\tLoss: 0.356527\t Accuracy:93.667%\n",
      "Rate : 90 [32000/49000 (65%)]\tLoss: 0.363638\t Accuracy:93.700%\n",
      "Rate : 90 [32640/49000 (67%)]\tLoss: 0.200144\t Accuracy:93.744%\n",
      "Rate : 90 [33280/49000 (68%)]\tLoss: 0.104226\t Accuracy:93.795%\n",
      "Rate : 90 [33920/49000 (69%)]\tLoss: 0.084860\t Accuracy:93.844%\n",
      "Rate : 90 [34560/49000 (70%)]\tLoss: 0.195873\t Accuracy:93.883%\n",
      "Rate : 90 [35200/49000 (72%)]\tLoss: 0.118016\t Accuracy:93.912%\n",
      "Rate : 90 [35840/49000 (73%)]\tLoss: 0.060215\t Accuracy:93.951%\n",
      "Rate : 90 [36480/49000 (74%)]\tLoss: 0.124392\t Accuracy:93.980%\n",
      "Rate : 90 [37120/49000 (76%)]\tLoss: 0.079697\t Accuracy:94.003%\n",
      "Rate : 90 [37760/49000 (77%)]\tLoss: 0.238778\t Accuracy:94.012%\n",
      "Rate : 90 [38400/49000 (78%)]\tLoss: 0.111156\t Accuracy:94.054%\n",
      "Rate : 90 [39040/49000 (80%)]\tLoss: 0.046586\t Accuracy:94.067%\n",
      "Rate : 90 [39680/49000 (81%)]\tLoss: 0.070615\t Accuracy:94.123%\n",
      "Rate : 90 [40320/49000 (82%)]\tLoss: 0.035736\t Accuracy:94.159%\n",
      "Rate : 90 [40960/49000 (84%)]\tLoss: 0.080187\t Accuracy:94.209%\n",
      "Rate : 90 [41600/49000 (85%)]\tLoss: 0.076556\t Accuracy:94.250%\n",
      "Rate : 90 [42240/49000 (86%)]\tLoss: 0.248546\t Accuracy:94.273%\n",
      "Rate : 90 [42880/49000 (87%)]\tLoss: 0.168272\t Accuracy:94.281%\n",
      "Rate : 90 [43520/49000 (89%)]\tLoss: 0.175289\t Accuracy:94.306%\n",
      "Rate : 90 [44160/49000 (90%)]\tLoss: 0.157704\t Accuracy:94.338%\n",
      "Rate : 90 [44800/49000 (91%)]\tLoss: 0.131948\t Accuracy:94.361%\n",
      "Rate : 90 [45440/49000 (93%)]\tLoss: 0.042448\t Accuracy:94.394%\n",
      "Rate : 90 [46080/49000 (94%)]\tLoss: 0.217016\t Accuracy:94.427%\n",
      "Rate : 90 [46720/49000 (95%)]\tLoss: 0.069149\t Accuracy:94.456%\n",
      "Rate : 90 [47360/49000 (97%)]\tLoss: 0.029659\t Accuracy:94.497%\n",
      "Rate : 90 [48000/49000 (98%)]\tLoss: 0.036367\t Accuracy:94.518%\n",
      "Rate : 90 [48640/49000 (99%)]\tLoss: 0.057063\t Accuracy:94.564%\n",
      "Rate : 90 [0/49000 (0%)]\tLoss: 0.041540\t Accuracy:100.000%\n",
      "Rate : 90 [640/49000 (1%)]\tLoss: 0.089149\t Accuracy:96.429%\n",
      "Rate : 90 [1280/49000 (3%)]\tLoss: 0.054306\t Accuracy:96.265%\n",
      "Rate : 90 [1920/49000 (4%)]\tLoss: 0.056671\t Accuracy:95.902%\n",
      "Rate : 90 [2560/49000 (5%)]\tLoss: 0.255853\t Accuracy:96.065%\n",
      "Rate : 90 [3200/49000 (7%)]\tLoss: 0.101572\t Accuracy:96.163%\n",
      "Rate : 90 [3840/49000 (8%)]\tLoss: 0.042743\t Accuracy:96.307%\n",
      "Rate : 90 [4480/49000 (9%)]\tLoss: 0.184101\t Accuracy:96.254%\n",
      "Rate : 90 [5120/49000 (10%)]\tLoss: 0.126435\t Accuracy:96.273%\n",
      "Rate : 90 [5760/49000 (12%)]\tLoss: 0.087762\t Accuracy:96.443%\n",
      "Rate : 90 [6400/49000 (13%)]\tLoss: 0.107337\t Accuracy:96.611%\n",
      "Rate : 90 [7040/49000 (14%)]\tLoss: 0.066098\t Accuracy:96.606%\n",
      "Rate : 90 [7680/49000 (16%)]\tLoss: 0.268828\t Accuracy:96.551%\n",
      "Rate : 90 [8320/49000 (17%)]\tLoss: 0.184007\t Accuracy:96.552%\n",
      "Rate : 90 [8960/49000 (18%)]\tLoss: 0.093871\t Accuracy:96.452%\n",
      "Rate : 90 [9600/49000 (20%)]\tLoss: 0.043816\t Accuracy:96.501%\n",
      "Rate : 90 [10240/49000 (21%)]\tLoss: 0.049935\t Accuracy:96.486%\n",
      "Rate : 90 [10880/49000 (22%)]\tLoss: 0.120762\t Accuracy:96.554%\n",
      "Rate : 90 [11520/49000 (23%)]\tLoss: 0.043991\t Accuracy:96.555%\n",
      "Rate : 90 [12160/49000 (25%)]\tLoss: 0.047605\t Accuracy:96.572%\n",
      "Rate : 90 [12800/49000 (26%)]\tLoss: 0.066300\t Accuracy:96.532%\n",
      "Rate : 90 [13440/49000 (27%)]\tLoss: 0.035610\t Accuracy:96.556%\n",
      "Rate : 90 [14080/49000 (29%)]\tLoss: 0.046487\t Accuracy:96.577%\n",
      "Rate : 90 [14720/49000 (30%)]\tLoss: 0.012460\t Accuracy:96.604%\n",
      "Rate : 90 [15360/49000 (31%)]\tLoss: 0.099184\t Accuracy:96.570%\n",
      "Rate : 90 [16000/49000 (33%)]\tLoss: 0.086199\t Accuracy:96.563%\n",
      "Rate : 90 [16640/49000 (34%)]\tLoss: 0.053054\t Accuracy:96.581%\n",
      "Rate : 90 [17280/49000 (35%)]\tLoss: 0.195492\t Accuracy:96.632%\n",
      "Rate : 90 [17920/49000 (37%)]\tLoss: 0.035930\t Accuracy:96.641%\n",
      "Rate : 90 [18560/49000 (38%)]\tLoss: 0.010213\t Accuracy:96.633%\n",
      "Rate : 90 [19200/49000 (39%)]\tLoss: 0.226082\t Accuracy:96.657%\n",
      "Rate : 90 [19840/49000 (40%)]\tLoss: 0.152830\t Accuracy:96.623%\n",
      "Rate : 90 [20480/49000 (42%)]\tLoss: 0.057426\t Accuracy:96.607%\n",
      "Rate : 90 [21120/49000 (43%)]\tLoss: 0.169718\t Accuracy:96.634%\n",
      "Rate : 90 [21760/49000 (44%)]\tLoss: 0.244665\t Accuracy:96.646%\n",
      "Rate : 90 [22400/49000 (46%)]\tLoss: 0.252199\t Accuracy:96.634%\n",
      "Rate : 90 [23040/49000 (47%)]\tLoss: 0.068904\t Accuracy:96.641%\n",
      "Rate : 90 [23680/49000 (48%)]\tLoss: 0.031381\t Accuracy:96.673%\n",
      "Rate : 90 [24320/49000 (50%)]\tLoss: 0.034629\t Accuracy:96.686%\n",
      "Rate : 90 [24960/49000 (51%)]\tLoss: 0.190718\t Accuracy:96.667%\n",
      "Rate : 90 [25600/49000 (52%)]\tLoss: 0.039018\t Accuracy:96.668%\n",
      "Rate : 90 [26240/49000 (54%)]\tLoss: 0.037527\t Accuracy:96.666%\n",
      "Rate : 90 [26880/49000 (55%)]\tLoss: 0.077080\t Accuracy:96.678%\n",
      "Rate : 90 [27520/49000 (56%)]\tLoss: 0.058724\t Accuracy:96.664%\n",
      "Rate : 90 [28160/49000 (57%)]\tLoss: 0.042633\t Accuracy:96.687%\n",
      "Rate : 90 [28800/49000 (59%)]\tLoss: 0.108123\t Accuracy:96.698%\n",
      "Rate : 90 [29440/49000 (60%)]\tLoss: 0.078337\t Accuracy:96.695%\n",
      "Rate : 90 [30080/49000 (61%)]\tLoss: 0.066657\t Accuracy:96.689%\n",
      "Rate : 90 [30720/49000 (63%)]\tLoss: 0.005929\t Accuracy:96.693%\n",
      "Rate : 90 [31360/49000 (64%)]\tLoss: 0.284472\t Accuracy:96.716%\n",
      "Rate : 90 [32000/49000 (65%)]\tLoss: 0.063596\t Accuracy:96.725%\n",
      "Rate : 90 [32640/49000 (67%)]\tLoss: 0.089586\t Accuracy:96.753%\n",
      "Rate : 90 [33280/49000 (68%)]\tLoss: 0.107884\t Accuracy:96.770%\n",
      "Rate : 90 [33920/49000 (69%)]\tLoss: 0.032351\t Accuracy:96.787%\n",
      "Rate : 90 [34560/49000 (70%)]\tLoss: 0.197217\t Accuracy:96.785%\n",
      "Rate : 90 [35200/49000 (72%)]\tLoss: 0.080065\t Accuracy:96.790%\n",
      "Rate : 90 [35840/49000 (73%)]\tLoss: 0.031903\t Accuracy:96.803%\n",
      "Rate : 90 [36480/49000 (74%)]\tLoss: 0.093009\t Accuracy:96.809%\n",
      "Rate : 90 [37120/49000 (76%)]\tLoss: 0.048379\t Accuracy:96.805%\n",
      "Rate : 90 [37760/49000 (77%)]\tLoss: 0.102152\t Accuracy:96.809%\n",
      "Rate : 90 [38400/49000 (78%)]\tLoss: 0.110838\t Accuracy:96.823%\n",
      "Rate : 90 [39040/49000 (80%)]\tLoss: 0.014466\t Accuracy:96.816%\n",
      "Rate : 90 [39680/49000 (81%)]\tLoss: 0.034542\t Accuracy:96.835%\n",
      "Rate : 90 [40320/49000 (82%)]\tLoss: 0.015900\t Accuracy:96.843%\n",
      "Rate : 90 [40960/49000 (84%)]\tLoss: 0.075113\t Accuracy:96.860%\n",
      "Rate : 90 [41600/49000 (85%)]\tLoss: 0.040671\t Accuracy:96.875%\n",
      "Rate : 90 [42240/49000 (86%)]\tLoss: 0.147629\t Accuracy:96.875%\n",
      "Rate : 90 [42880/49000 (87%)]\tLoss: 0.147277\t Accuracy:96.866%\n",
      "Rate : 90 [43520/49000 (89%)]\tLoss: 0.141510\t Accuracy:96.864%\n",
      "Rate : 90 [44160/49000 (90%)]\tLoss: 0.043214\t Accuracy:96.866%\n",
      "Rate : 90 [44800/49000 (91%)]\tLoss: 0.083299\t Accuracy:96.862%\n",
      "Rate : 90 [45440/49000 (93%)]\tLoss: 0.050720\t Accuracy:96.879%\n",
      "Rate : 90 [46080/49000 (94%)]\tLoss: 0.156736\t Accuracy:96.884%\n",
      "Rate : 90 [46720/49000 (95%)]\tLoss: 0.042402\t Accuracy:96.884%\n",
      "Rate : 90 [47360/49000 (97%)]\tLoss: 0.030258\t Accuracy:96.898%\n",
      "Rate : 90 [48000/49000 (98%)]\tLoss: 0.019955\t Accuracy:96.892%\n",
      "Rate : 90 [48640/49000 (99%)]\tLoss: 0.126174\t Accuracy:96.898%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "pruned_model_90 = smart_prune_shed(acmlp, train_loader, [\n",
    "    (0, 1), \n",
    "    (40, 1), \n",
    "    (50, 1), \n",
    "    (60, 1), \n",
    "    (70, 1), \n",
    "    (80, 2), \n",
    "    (83, 2), \n",
    "    (85, 2), \n",
    "    (87, 2), \n",
    "    (89, 2), \n",
    "    (90, 2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.953% \n"
     ]
    }
   ],
   "source": [
    "evaluate(pruned_model_90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22200"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_pruned_weights(pruned_model_90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтож, это оригинальное качество за всего 10% сети.\n",
    "\n",
    "Ради интереса попробуем \"дожать до победы\" и удалим 99%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate : 90 [0/49000 (0%)]\tLoss: 0.031259\t Accuracy:100.000%\n",
      "Rate : 90 [640/49000 (1%)]\tLoss: 0.149086\t Accuracy:97.321%\n",
      "Rate : 90 [1280/49000 (3%)]\tLoss: 0.032991\t Accuracy:96.951%\n",
      "Rate : 90 [1920/49000 (4%)]\tLoss: 0.040974\t Accuracy:96.465%\n",
      "Rate : 90 [2560/49000 (5%)]\tLoss: 0.176223\t Accuracy:96.721%\n",
      "Rate : 90 [3200/49000 (7%)]\tLoss: 0.103680\t Accuracy:96.937%\n",
      "Rate : 90 [3840/49000 (8%)]\tLoss: 0.026324\t Accuracy:97.030%\n",
      "Rate : 90 [4480/49000 (9%)]\tLoss: 0.124739\t Accuracy:97.141%\n",
      "Rate : 90 [5120/49000 (10%)]\tLoss: 0.105597\t Accuracy:97.224%\n",
      "Rate : 90 [5760/49000 (12%)]\tLoss: 0.096774\t Accuracy:97.272%\n",
      "Rate : 90 [6400/49000 (13%)]\tLoss: 0.234538\t Accuracy:97.326%\n",
      "Rate : 90 [7040/49000 (14%)]\tLoss: 0.069501\t Accuracy:97.285%\n",
      "Rate : 90 [7680/49000 (16%)]\tLoss: 0.224392\t Accuracy:97.225%\n",
      "Rate : 90 [8320/49000 (17%)]\tLoss: 0.207697\t Accuracy:97.282%\n",
      "Rate : 90 [8960/49000 (18%)]\tLoss: 0.075373\t Accuracy:97.220%\n",
      "Rate : 90 [9600/49000 (20%)]\tLoss: 0.006230\t Accuracy:97.238%\n",
      "Rate : 90 [10240/49000 (21%)]\tLoss: 0.021234\t Accuracy:97.225%\n",
      "Rate : 90 [10880/49000 (22%)]\tLoss: 0.078380\t Accuracy:97.251%\n",
      "Rate : 90 [11520/49000 (23%)]\tLoss: 0.027266\t Accuracy:97.213%\n",
      "Rate : 90 [12160/49000 (25%)]\tLoss: 0.076693\t Accuracy:97.228%\n",
      "Rate : 90 [12800/49000 (26%)]\tLoss: 0.034698\t Accuracy:97.226%\n",
      "Rate : 90 [13440/49000 (27%)]\tLoss: 0.208746\t Accuracy:97.209%\n",
      "Rate : 90 [14080/49000 (29%)]\tLoss: 0.056395\t Accuracy:97.194%\n",
      "Rate : 90 [14720/49000 (30%)]\tLoss: 0.029850\t Accuracy:97.221%\n",
      "Rate : 90 [15360/49000 (31%)]\tLoss: 0.060843\t Accuracy:97.219%\n",
      "Rate : 90 [16000/49000 (33%)]\tLoss: 0.042024\t Accuracy:97.212%\n",
      "Rate : 90 [16640/49000 (34%)]\tLoss: 0.046436\t Accuracy:97.229%\n",
      "Rate : 90 [17280/49000 (35%)]\tLoss: 0.126874\t Accuracy:97.274%\n",
      "Rate : 90 [17920/49000 (37%)]\tLoss: 0.016202\t Accuracy:97.276%\n",
      "Rate : 90 [18560/49000 (38%)]\tLoss: 0.228758\t Accuracy:97.262%\n",
      "Rate : 90 [19200/49000 (39%)]\tLoss: 0.219221\t Accuracy:97.296%\n",
      "Rate : 90 [19840/49000 (40%)]\tLoss: 0.124727\t Accuracy:97.268%\n",
      "Rate : 90 [20480/49000 (42%)]\tLoss: 0.058643\t Accuracy:97.236%\n",
      "Rate : 90 [21120/49000 (43%)]\tLoss: 0.159502\t Accuracy:97.225%\n",
      "Rate : 90 [21760/49000 (44%)]\tLoss: 0.208464\t Accuracy:97.215%\n",
      "Rate : 90 [22400/49000 (46%)]\tLoss: 0.106901\t Accuracy:97.227%\n",
      "Rate : 90 [23040/49000 (47%)]\tLoss: 0.040196\t Accuracy:97.252%\n",
      "Rate : 90 [23680/49000 (48%)]\tLoss: 0.025416\t Accuracy:97.267%\n",
      "Rate : 90 [24320/49000 (50%)]\tLoss: 0.036732\t Accuracy:97.294%\n",
      "Rate : 90 [24960/49000 (51%)]\tLoss: 0.189355\t Accuracy:97.295%\n",
      "Rate : 90 [25600/49000 (52%)]\tLoss: 0.023285\t Accuracy:97.300%\n",
      "Rate : 90 [26240/49000 (54%)]\tLoss: 0.013894\t Accuracy:97.298%\n",
      "Rate : 90 [26880/49000 (55%)]\tLoss: 0.037486\t Accuracy:97.321%\n",
      "Rate : 90 [27520/49000 (56%)]\tLoss: 0.064154\t Accuracy:97.318%\n",
      "Rate : 90 [28160/49000 (57%)]\tLoss: 0.025872\t Accuracy:97.343%\n",
      "Rate : 90 [28800/49000 (59%)]\tLoss: 0.187319\t Accuracy:97.336%\n",
      "Rate : 90 [29440/49000 (60%)]\tLoss: 0.058389\t Accuracy:97.333%\n",
      "Rate : 90 [30080/49000 (61%)]\tLoss: 0.120708\t Accuracy:97.320%\n",
      "Rate : 90 [30720/49000 (63%)]\tLoss: 0.004385\t Accuracy:97.311%\n",
      "Rate : 90 [31360/49000 (64%)]\tLoss: 0.229847\t Accuracy:97.324%\n",
      "Rate : 90 [32000/49000 (65%)]\tLoss: 0.110759\t Accuracy:97.321%\n",
      "Rate : 90 [32640/49000 (67%)]\tLoss: 0.200848\t Accuracy:97.337%\n",
      "Rate : 90 [33280/49000 (68%)]\tLoss: 0.118921\t Accuracy:97.340%\n",
      "Rate : 90 [33920/49000 (69%)]\tLoss: 0.011333\t Accuracy:97.355%\n",
      "Rate : 90 [34560/49000 (70%)]\tLoss: 0.157028\t Accuracy:97.349%\n",
      "Rate : 90 [35200/49000 (72%)]\tLoss: 0.067912\t Accuracy:97.340%\n",
      "Rate : 90 [35840/49000 (73%)]\tLoss: 0.032003\t Accuracy:97.341%\n",
      "Rate : 90 [36480/49000 (74%)]\tLoss: 0.071969\t Accuracy:97.341%\n",
      "Rate : 90 [37120/49000 (76%)]\tLoss: 0.038270\t Accuracy:97.333%\n",
      "Rate : 90 [37760/49000 (77%)]\tLoss: 0.087905\t Accuracy:97.333%\n",
      "Rate : 90 [38400/49000 (78%)]\tLoss: 0.135701\t Accuracy:97.330%\n",
      "Rate : 90 [39040/49000 (80%)]\tLoss: 0.007914\t Accuracy:97.318%\n",
      "Rate : 90 [39680/49000 (81%)]\tLoss: 0.018758\t Accuracy:97.338%\n",
      "Rate : 90 [40320/49000 (82%)]\tLoss: 0.029239\t Accuracy:97.336%\n",
      "Rate : 90 [40960/49000 (84%)]\tLoss: 0.075850\t Accuracy:97.356%\n",
      "Rate : 90 [41600/49000 (85%)]\tLoss: 0.022339\t Accuracy:97.370%\n",
      "Rate : 90 [42240/49000 (86%)]\tLoss: 0.099360\t Accuracy:97.374%\n",
      "Rate : 90 [42880/49000 (87%)]\tLoss: 0.043237\t Accuracy:97.369%\n",
      "Rate : 90 [43520/49000 (89%)]\tLoss: 0.123668\t Accuracy:97.353%\n",
      "Rate : 90 [44160/49000 (90%)]\tLoss: 0.015442\t Accuracy:97.359%\n",
      "Rate : 90 [44800/49000 (91%)]\tLoss: 0.089436\t Accuracy:97.352%\n",
      "Rate : 90 [45440/49000 (93%)]\tLoss: 0.062471\t Accuracy:97.361%\n",
      "Rate : 90 [46080/49000 (94%)]\tLoss: 0.254642\t Accuracy:97.354%\n",
      "Rate : 90 [46720/49000 (95%)]\tLoss: 0.027102\t Accuracy:97.361%\n",
      "Rate : 90 [47360/49000 (97%)]\tLoss: 0.018788\t Accuracy:97.375%\n",
      "Rate : 90 [48000/49000 (98%)]\tLoss: 0.017222\t Accuracy:97.381%\n",
      "Rate : 90 [48640/49000 (99%)]\tLoss: 0.124707\t Accuracy:97.382%\n",
      "Rate : 90 [0/49000 (0%)]\tLoss: 0.036688\t Accuracy:100.000%\n",
      "Rate : 90 [640/49000 (1%)]\tLoss: 0.087338\t Accuracy:97.917%\n",
      "Rate : 90 [1280/49000 (3%)]\tLoss: 0.065040\t Accuracy:97.256%\n",
      "Rate : 90 [1920/49000 (4%)]\tLoss: 0.037207\t Accuracy:97.131%\n",
      "Rate : 90 [2560/49000 (5%)]\tLoss: 0.074261\t Accuracy:97.299%\n",
      "Rate : 90 [3200/49000 (7%)]\tLoss: 0.058348\t Accuracy:97.370%\n",
      "Rate : 90 [3840/49000 (8%)]\tLoss: 0.029779\t Accuracy:97.417%\n",
      "Rate : 90 [4480/49000 (9%)]\tLoss: 0.067525\t Accuracy:97.496%\n",
      "Rate : 90 [5120/49000 (10%)]\tLoss: 0.029299\t Accuracy:97.574%\n",
      "Rate : 90 [5760/49000 (12%)]\tLoss: 0.066879\t Accuracy:97.738%\n",
      "Rate : 90 [6400/49000 (13%)]\tLoss: 0.050993\t Accuracy:97.839%\n",
      "Rate : 90 [7040/49000 (14%)]\tLoss: 0.047557\t Accuracy:97.837%\n",
      "Rate : 90 [7680/49000 (16%)]\tLoss: 0.198212\t Accuracy:97.770%\n",
      "Rate : 90 [8320/49000 (17%)]\tLoss: 0.201392\t Accuracy:97.821%\n",
      "Rate : 90 [8960/49000 (18%)]\tLoss: 0.057568\t Accuracy:97.754%\n",
      "Rate : 90 [9600/49000 (20%)]\tLoss: 0.004773\t Accuracy:97.799%\n",
      "Rate : 90 [10240/49000 (21%)]\tLoss: 0.014034\t Accuracy:97.741%\n",
      "Rate : 90 [10880/49000 (22%)]\tLoss: 0.089325\t Accuracy:97.755%\n",
      "Rate : 90 [11520/49000 (23%)]\tLoss: 0.010281\t Accuracy:97.741%\n",
      "Rate : 90 [12160/49000 (25%)]\tLoss: 0.023913\t Accuracy:97.777%\n",
      "Rate : 90 [12800/49000 (26%)]\tLoss: 0.034640\t Accuracy:97.771%\n",
      "Rate : 90 [13440/49000 (27%)]\tLoss: 0.007966\t Accuracy:97.766%\n",
      "Rate : 90 [14080/49000 (29%)]\tLoss: 0.036206\t Accuracy:97.768%\n",
      "Rate : 90 [14720/49000 (30%)]\tLoss: 0.022691\t Accuracy:97.804%\n",
      "Rate : 90 [15360/49000 (31%)]\tLoss: 0.067216\t Accuracy:97.791%\n",
      "Rate : 90 [16000/49000 (33%)]\tLoss: 0.027967\t Accuracy:97.786%\n",
      "Rate : 90 [16640/49000 (34%)]\tLoss: 0.037303\t Accuracy:97.829%\n",
      "Rate : 90 [17280/49000 (35%)]\tLoss: 0.091247\t Accuracy:97.863%\n",
      "Rate : 90 [17920/49000 (37%)]\tLoss: 0.018259\t Accuracy:97.850%\n",
      "Rate : 90 [18560/49000 (38%)]\tLoss: 0.003018\t Accuracy:97.854%\n",
      "Rate : 90 [19200/49000 (39%)]\tLoss: 0.178883\t Accuracy:97.858%\n",
      "Rate : 90 [19840/49000 (40%)]\tLoss: 0.044070\t Accuracy:97.846%\n",
      "Rate : 90 [20480/49000 (42%)]\tLoss: 0.053080\t Accuracy:97.796%\n",
      "Rate : 90 [21120/49000 (43%)]\tLoss: 0.006636\t Accuracy:97.806%\n",
      "Rate : 90 [21760/49000 (44%)]\tLoss: 0.207721\t Accuracy:97.793%\n",
      "Rate : 90 [22400/49000 (46%)]\tLoss: 0.038745\t Accuracy:97.784%\n",
      "Rate : 90 [23040/49000 (47%)]\tLoss: 0.041988\t Accuracy:97.798%\n",
      "Rate : 90 [23680/49000 (48%)]\tLoss: 0.013224\t Accuracy:97.790%\n",
      "Rate : 90 [24320/49000 (50%)]\tLoss: 0.020692\t Accuracy:97.799%\n",
      "Rate : 90 [24960/49000 (51%)]\tLoss: 0.193649\t Accuracy:97.811%\n",
      "Rate : 90 [25600/49000 (52%)]\tLoss: 0.013417\t Accuracy:97.800%\n",
      "Rate : 90 [26240/49000 (54%)]\tLoss: 0.014248\t Accuracy:97.792%\n",
      "Rate : 90 [26880/49000 (55%)]\tLoss: 0.038012\t Accuracy:97.797%\n",
      "Rate : 90 [27520/49000 (56%)]\tLoss: 0.043428\t Accuracy:97.786%\n",
      "Rate : 90 [28160/49000 (57%)]\tLoss: 0.121989\t Accuracy:97.801%\n",
      "Rate : 90 [28800/49000 (59%)]\tLoss: 0.079226\t Accuracy:97.794%\n",
      "Rate : 90 [29440/49000 (60%)]\tLoss: 0.087898\t Accuracy:97.784%\n",
      "Rate : 90 [30080/49000 (61%)]\tLoss: 0.071704\t Accuracy:97.758%\n",
      "Rate : 90 [30720/49000 (63%)]\tLoss: 0.005728\t Accuracy:97.750%\n",
      "Rate : 90 [31360/49000 (64%)]\tLoss: 0.217967\t Accuracy:97.757%\n",
      "Rate : 90 [32000/49000 (65%)]\tLoss: 0.092192\t Accuracy:97.758%\n",
      "Rate : 90 [32640/49000 (67%)]\tLoss: 0.114674\t Accuracy:97.763%\n",
      "Rate : 90 [33280/49000 (68%)]\tLoss: 0.127345\t Accuracy:97.764%\n",
      "Rate : 90 [33920/49000 (69%)]\tLoss: 0.006743\t Accuracy:97.767%\n",
      "Rate : 90 [34560/49000 (70%)]\tLoss: 0.071646\t Accuracy:97.754%\n",
      "Rate : 90 [35200/49000 (72%)]\tLoss: 0.033450\t Accuracy:97.755%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate : 90 [35840/49000 (73%)]\tLoss: 0.034664\t Accuracy:97.761%\n",
      "Rate : 90 [36480/49000 (74%)]\tLoss: 0.066103\t Accuracy:97.765%\n",
      "Rate : 90 [37120/49000 (76%)]\tLoss: 0.032462\t Accuracy:97.763%\n",
      "Rate : 90 [37760/49000 (77%)]\tLoss: 0.102585\t Accuracy:97.753%\n",
      "Rate : 90 [38400/49000 (78%)]\tLoss: 0.114253\t Accuracy:97.757%\n",
      "Rate : 90 [39040/49000 (80%)]\tLoss: 0.005880\t Accuracy:97.743%\n",
      "Rate : 90 [39680/49000 (81%)]\tLoss: 0.013531\t Accuracy:97.761%\n",
      "Rate : 90 [40320/49000 (82%)]\tLoss: 0.018758\t Accuracy:97.762%\n",
      "Rate : 90 [40960/49000 (84%)]\tLoss: 0.062535\t Accuracy:97.778%\n",
      "Rate : 90 [41600/49000 (85%)]\tLoss: 0.021072\t Accuracy:97.788%\n",
      "Rate : 90 [42240/49000 (86%)]\tLoss: 0.060866\t Accuracy:97.790%\n",
      "Rate : 90 [42880/49000 (87%)]\tLoss: 0.049780\t Accuracy:97.788%\n",
      "Rate : 90 [43520/49000 (89%)]\tLoss: 0.193850\t Accuracy:97.773%\n",
      "Rate : 90 [44160/49000 (90%)]\tLoss: 0.026006\t Accuracy:97.778%\n",
      "Rate : 90 [44800/49000 (91%)]\tLoss: 0.055933\t Accuracy:97.767%\n",
      "Rate : 90 [45440/49000 (93%)]\tLoss: 0.041167\t Accuracy:97.777%\n",
      "Rate : 90 [46080/49000 (94%)]\tLoss: 0.085752\t Accuracy:97.775%\n",
      "Rate : 90 [46720/49000 (95%)]\tLoss: 0.028261\t Accuracy:97.775%\n",
      "Rate : 90 [47360/49000 (97%)]\tLoss: 0.012241\t Accuracy:97.789%\n",
      "Rate : 90 [48000/49000 (98%)]\tLoss: 0.012954\t Accuracy:97.789%\n",
      "Rate : 90 [48640/49000 (99%)]\tLoss: 0.173157\t Accuracy:97.791%\n",
      "Rate : 92 [0/49000 (0%)]\tLoss: 1.215147\t Accuracy:65.625%\n",
      "Rate : 92 [640/49000 (1%)]\tLoss: 1.374920\t Accuracy:67.113%\n",
      "Rate : 92 [1280/49000 (3%)]\tLoss: 1.131140\t Accuracy:69.665%\n",
      "Rate : 92 [1920/49000 (4%)]\tLoss: 0.700245\t Accuracy:71.107%\n",
      "Rate : 92 [2560/49000 (5%)]\tLoss: 1.324581\t Accuracy:71.836%\n",
      "Rate : 92 [3200/49000 (7%)]\tLoss: 1.085314\t Accuracy:72.246%\n",
      "Rate : 92 [3840/49000 (8%)]\tLoss: 0.521168\t Accuracy:72.986%\n",
      "Rate : 92 [4480/49000 (9%)]\tLoss: 0.714656\t Accuracy:73.027%\n",
      "Rate : 92 [5120/49000 (10%)]\tLoss: 0.719532\t Accuracy:73.273%\n",
      "Rate : 92 [5760/49000 (12%)]\tLoss: 0.506323\t Accuracy:73.671%\n",
      "Rate : 92 [6400/49000 (13%)]\tLoss: 0.875157\t Accuracy:73.927%\n",
      "Rate : 92 [7040/49000 (14%)]\tLoss: 0.588630\t Accuracy:74.024%\n",
      "Rate : 92 [7680/49000 (16%)]\tLoss: 0.795682\t Accuracy:73.859%\n",
      "Rate : 92 [8320/49000 (17%)]\tLoss: 0.715801\t Accuracy:73.898%\n",
      "Rate : 92 [8960/49000 (18%)]\tLoss: 0.666100\t Accuracy:73.932%\n",
      "Rate : 92 [9600/49000 (20%)]\tLoss: 0.630883\t Accuracy:73.806%\n",
      "Rate : 92 [10240/49000 (21%)]\tLoss: 0.603363\t Accuracy:73.851%\n",
      "Rate : 92 [10880/49000 (22%)]\tLoss: 0.475122\t Accuracy:73.992%\n",
      "Rate : 92 [11520/49000 (23%)]\tLoss: 0.559922\t Accuracy:73.918%\n",
      "Rate : 92 [12160/49000 (25%)]\tLoss: 0.762984\t Accuracy:74.098%\n",
      "Rate : 92 [12800/49000 (26%)]\tLoss: 0.693430\t Accuracy:74.096%\n",
      "Rate : 92 [13440/49000 (27%)]\tLoss: 0.479168\t Accuracy:74.339%\n",
      "Rate : 92 [14080/49000 (29%)]\tLoss: 0.821205\t Accuracy:74.476%\n",
      "Rate : 92 [14720/49000 (30%)]\tLoss: 0.727509\t Accuracy:74.688%\n",
      "Rate : 92 [15360/49000 (31%)]\tLoss: 0.448231\t Accuracy:74.799%\n",
      "Rate : 92 [16000/49000 (33%)]\tLoss: 0.820557\t Accuracy:74.963%\n",
      "Rate : 92 [16640/49000 (34%)]\tLoss: 0.650529\t Accuracy:75.090%\n",
      "Rate : 92 [17280/49000 (35%)]\tLoss: 0.753053\t Accuracy:75.040%\n",
      "Rate : 92 [17920/49000 (37%)]\tLoss: 0.436392\t Accuracy:75.156%\n",
      "Rate : 92 [18560/49000 (38%)]\tLoss: 0.664439\t Accuracy:75.172%\n",
      "Rate : 92 [19200/49000 (39%)]\tLoss: 0.797584\t Accuracy:75.099%\n",
      "Rate : 92 [19840/49000 (40%)]\tLoss: 0.550895\t Accuracy:75.151%\n",
      "Rate : 92 [20480/49000 (42%)]\tLoss: 0.868037\t Accuracy:75.127%\n",
      "Rate : 92 [21120/49000 (43%)]\tLoss: 0.246523\t Accuracy:75.052%\n",
      "Rate : 92 [21760/49000 (44%)]\tLoss: 0.508942\t Accuracy:75.078%\n",
      "Rate : 92 [22400/49000 (46%)]\tLoss: 0.490360\t Accuracy:75.125%\n",
      "Rate : 92 [23040/49000 (47%)]\tLoss: 0.481748\t Accuracy:75.260%\n",
      "Rate : 92 [23680/49000 (48%)]\tLoss: 0.420130\t Accuracy:75.472%\n",
      "Rate : 92 [24320/49000 (50%)]\tLoss: 0.737751\t Accuracy:75.608%\n",
      "Rate : 92 [24960/49000 (51%)]\tLoss: 0.604874\t Accuracy:75.720%\n",
      "Rate : 92 [25600/49000 (52%)]\tLoss: 0.697393\t Accuracy:75.866%\n",
      "Rate : 92 [26240/49000 (54%)]\tLoss: 0.515266\t Accuracy:76.039%\n",
      "Rate : 92 [26880/49000 (55%)]\tLoss: 0.534977\t Accuracy:76.104%\n",
      "Rate : 92 [27520/49000 (56%)]\tLoss: 0.352623\t Accuracy:76.256%\n",
      "Rate : 92 [28160/49000 (57%)]\tLoss: 0.640281\t Accuracy:76.412%\n",
      "Rate : 92 [28800/49000 (59%)]\tLoss: 0.452346\t Accuracy:76.505%\n",
      "Rate : 92 [29440/49000 (60%)]\tLoss: 0.289186\t Accuracy:76.581%\n",
      "Rate : 92 [30080/49000 (61%)]\tLoss: 0.605994\t Accuracy:76.664%\n",
      "Rate : 92 [30720/49000 (63%)]\tLoss: 0.282856\t Accuracy:76.821%\n",
      "Rate : 92 [31360/49000 (64%)]\tLoss: 0.452377\t Accuracy:76.972%\n",
      "Rate : 92 [32000/49000 (65%)]\tLoss: 0.539162\t Accuracy:77.104%\n",
      "Rate : 92 [32640/49000 (67%)]\tLoss: 0.478119\t Accuracy:77.247%\n",
      "Rate : 92 [33280/49000 (68%)]\tLoss: 0.390478\t Accuracy:77.363%\n",
      "Rate : 92 [33920/49000 (69%)]\tLoss: 0.261326\t Accuracy:77.471%\n",
      "Rate : 92 [34560/49000 (70%)]\tLoss: 0.360401\t Accuracy:77.582%\n",
      "Rate : 92 [35200/49000 (72%)]\tLoss: 0.293176\t Accuracy:77.657%\n",
      "Rate : 92 [35840/49000 (73%)]\tLoss: 0.277352\t Accuracy:77.771%\n",
      "Rate : 92 [36480/49000 (74%)]\tLoss: 0.424446\t Accuracy:77.868%\n",
      "Rate : 92 [37120/49000 (76%)]\tLoss: 0.264110\t Accuracy:77.961%\n",
      "Rate : 92 [37760/49000 (77%)]\tLoss: 0.847751\t Accuracy:78.048%\n",
      "Rate : 92 [38400/49000 (78%)]\tLoss: 0.502765\t Accuracy:78.130%\n",
      "Rate : 92 [39040/49000 (80%)]\tLoss: 0.130778\t Accuracy:78.217%\n",
      "Rate : 92 [39680/49000 (81%)]\tLoss: 0.399788\t Accuracy:78.321%\n",
      "Rate : 92 [40320/49000 (82%)]\tLoss: 0.163925\t Accuracy:78.417%\n",
      "Rate : 92 [40960/49000 (84%)]\tLoss: 0.255896\t Accuracy:78.498%\n",
      "Rate : 92 [41600/49000 (85%)]\tLoss: 0.403038\t Accuracy:78.596%\n",
      "Rate : 92 [42240/49000 (86%)]\tLoss: 0.538181\t Accuracy:78.719%\n",
      "Rate : 92 [42880/49000 (87%)]\tLoss: 0.648135\t Accuracy:78.815%\n",
      "Rate : 92 [43520/49000 (89%)]\tLoss: 0.535572\t Accuracy:78.867%\n",
      "Rate : 92 [44160/49000 (90%)]\tLoss: 1.123339\t Accuracy:78.962%\n",
      "Rate : 92 [44800/49000 (91%)]\tLoss: 0.390915\t Accuracy:79.002%\n",
      "Rate : 92 [45440/49000 (93%)]\tLoss: 0.421636\t Accuracy:79.115%\n",
      "Rate : 92 [46080/49000 (94%)]\tLoss: 0.329890\t Accuracy:79.190%\n",
      "Rate : 92 [46720/49000 (95%)]\tLoss: 0.413777\t Accuracy:79.244%\n",
      "Rate : 92 [47360/49000 (97%)]\tLoss: 0.352640\t Accuracy:79.343%\n",
      "Rate : 92 [48000/49000 (98%)]\tLoss: 0.218738\t Accuracy:79.428%\n",
      "Rate : 92 [48640/49000 (99%)]\tLoss: 0.439229\t Accuracy:79.465%\n",
      "Rate : 92 [0/49000 (0%)]\tLoss: 0.318646\t Accuracy:87.500%\n",
      "Rate : 92 [640/49000 (1%)]\tLoss: 0.441004\t Accuracy:85.417%\n",
      "Rate : 92 [1280/49000 (3%)]\tLoss: 0.334157\t Accuracy:85.137%\n",
      "Rate : 92 [1920/49000 (4%)]\tLoss: 0.195558\t Accuracy:85.092%\n",
      "Rate : 92 [2560/49000 (5%)]\tLoss: 0.499668\t Accuracy:84.799%\n",
      "Rate : 92 [3200/49000 (7%)]\tLoss: 0.429220\t Accuracy:84.561%\n",
      "Rate : 92 [3840/49000 (8%)]\tLoss: 0.228233\t Accuracy:85.227%\n",
      "Rate : 92 [4480/49000 (9%)]\tLoss: 0.255733\t Accuracy:85.084%\n",
      "Rate : 92 [5120/49000 (10%)]\tLoss: 0.459670\t Accuracy:85.093%\n",
      "Rate : 92 [5760/49000 (12%)]\tLoss: 0.255358\t Accuracy:85.066%\n",
      "Rate : 92 [6400/49000 (13%)]\tLoss: 0.608471\t Accuracy:85.044%\n",
      "Rate : 92 [7040/49000 (14%)]\tLoss: 0.356062\t Accuracy:84.997%\n",
      "Rate : 92 [7680/49000 (16%)]\tLoss: 0.557006\t Accuracy:84.764%\n",
      "Rate : 92 [8320/49000 (17%)]\tLoss: 0.597090\t Accuracy:84.746%\n",
      "Rate : 92 [8960/49000 (18%)]\tLoss: 0.428970\t Accuracy:84.586%\n",
      "Rate : 92 [9600/49000 (20%)]\tLoss: 0.246707\t Accuracy:84.531%\n",
      "Rate : 92 [10240/49000 (21%)]\tLoss: 0.326054\t Accuracy:84.336%\n",
      "Rate : 92 [10880/49000 (22%)]\tLoss: 0.278171\t Accuracy:84.448%\n",
      "Rate : 92 [11520/49000 (23%)]\tLoss: 0.349713\t Accuracy:84.254%\n",
      "Rate : 92 [12160/49000 (25%)]\tLoss: 0.401623\t Accuracy:84.268%\n",
      "Rate : 92 [12800/49000 (26%)]\tLoss: 0.485830\t Accuracy:84.204%\n",
      "Rate : 92 [13440/49000 (27%)]\tLoss: 0.283765\t Accuracy:84.308%\n",
      "Rate : 92 [14080/49000 (29%)]\tLoss: 0.535526\t Accuracy:84.396%\n",
      "Rate : 92 [14720/49000 (30%)]\tLoss: 0.249918\t Accuracy:84.524%\n",
      "Rate : 92 [15360/49000 (31%)]\tLoss: 0.303292\t Accuracy:84.570%\n",
      "Rate : 92 [16000/49000 (33%)]\tLoss: 0.319971\t Accuracy:84.656%\n",
      "Rate : 92 [16640/49000 (34%)]\tLoss: 0.487233\t Accuracy:84.693%\n",
      "Rate : 92 [17280/49000 (35%)]\tLoss: 0.446580\t Accuracy:84.595%\n",
      "Rate : 92 [17920/49000 (37%)]\tLoss: 0.230452\t Accuracy:84.665%\n",
      "Rate : 92 [18560/49000 (38%)]\tLoss: 0.301852\t Accuracy:84.698%\n",
      "Rate : 92 [19200/49000 (39%)]\tLoss: 0.694748\t Accuracy:84.630%\n",
      "Rate : 92 [19840/49000 (40%)]\tLoss: 0.537767\t Accuracy:84.647%\n",
      "Rate : 92 [20480/49000 (42%)]\tLoss: 0.550275\t Accuracy:84.663%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate : 92 [21120/49000 (43%)]\tLoss: 0.165195\t Accuracy:84.564%\n",
      "Rate : 92 [21760/49000 (44%)]\tLoss: 0.368105\t Accuracy:84.577%\n",
      "Rate : 92 [22400/49000 (46%)]\tLoss: 0.355233\t Accuracy:84.598%\n",
      "Rate : 92 [23040/49000 (47%)]\tLoss: 0.298237\t Accuracy:84.639%\n",
      "Rate : 92 [23680/49000 (48%)]\tLoss: 0.187305\t Accuracy:84.696%\n",
      "Rate : 92 [24320/49000 (50%)]\tLoss: 0.541945\t Accuracy:84.699%\n",
      "Rate : 92 [24960/49000 (51%)]\tLoss: 0.509774\t Accuracy:84.695%\n",
      "Rate : 92 [25600/49000 (52%)]\tLoss: 0.424078\t Accuracy:84.668%\n",
      "Rate : 92 [26240/49000 (54%)]\tLoss: 0.294184\t Accuracy:84.725%\n",
      "Rate : 92 [26880/49000 (55%)]\tLoss: 0.440680\t Accuracy:84.676%\n",
      "Rate : 92 [27520/49000 (56%)]\tLoss: 0.219140\t Accuracy:84.716%\n",
      "Rate : 92 [28160/49000 (57%)]\tLoss: 0.383162\t Accuracy:84.779%\n",
      "Rate : 92 [28800/49000 (59%)]\tLoss: 0.323354\t Accuracy:84.791%\n",
      "Rate : 92 [29440/49000 (60%)]\tLoss: 0.292318\t Accuracy:84.779%\n",
      "Rate : 92 [30080/49000 (61%)]\tLoss: 0.488400\t Accuracy:84.727%\n",
      "Rate : 92 [30720/49000 (63%)]\tLoss: 0.184732\t Accuracy:84.762%\n",
      "Rate : 92 [31360/49000 (64%)]\tLoss: 0.311239\t Accuracy:84.818%\n",
      "Rate : 92 [32000/49000 (65%)]\tLoss: 0.382928\t Accuracy:84.853%\n",
      "Rate : 92 [32640/49000 (67%)]\tLoss: 0.405468\t Accuracy:84.880%\n",
      "Rate : 92 [33280/49000 (68%)]\tLoss: 0.333301\t Accuracy:84.897%\n",
      "Rate : 92 [33920/49000 (69%)]\tLoss: 0.177900\t Accuracy:84.905%\n",
      "Rate : 92 [34560/49000 (70%)]\tLoss: 0.355837\t Accuracy:84.927%\n",
      "Rate : 92 [35200/49000 (72%)]\tLoss: 0.265844\t Accuracy:84.894%\n",
      "Rate : 92 [35840/49000 (73%)]\tLoss: 0.225074\t Accuracy:84.899%\n",
      "Rate : 92 [36480/49000 (74%)]\tLoss: 0.308275\t Accuracy:84.898%\n",
      "Rate : 92 [37120/49000 (76%)]\tLoss: 0.198667\t Accuracy:84.903%\n",
      "Rate : 92 [37760/49000 (77%)]\tLoss: 0.477205\t Accuracy:84.910%\n",
      "Rate : 92 [38400/49000 (78%)]\tLoss: 0.370853\t Accuracy:84.903%\n",
      "Rate : 92 [39040/49000 (80%)]\tLoss: 0.099978\t Accuracy:84.920%\n",
      "Rate : 92 [39680/49000 (81%)]\tLoss: 0.311648\t Accuracy:84.944%\n",
      "Rate : 92 [40320/49000 (82%)]\tLoss: 0.147641\t Accuracy:84.957%\n",
      "Rate : 92 [40960/49000 (84%)]\tLoss: 0.196335\t Accuracy:84.956%\n",
      "Rate : 92 [41600/49000 (85%)]\tLoss: 0.200131\t Accuracy:85.000%\n",
      "Rate : 92 [42240/49000 (86%)]\tLoss: 0.481402\t Accuracy:85.047%\n",
      "Rate : 92 [42880/49000 (87%)]\tLoss: 0.412319\t Accuracy:85.055%\n",
      "Rate : 92 [43520/49000 (89%)]\tLoss: 0.423220\t Accuracy:85.032%\n",
      "Rate : 92 [44160/49000 (90%)]\tLoss: 0.975244\t Accuracy:85.074%\n",
      "Rate : 92 [44800/49000 (91%)]\tLoss: 0.354182\t Accuracy:85.062%\n",
      "Rate : 92 [45440/49000 (93%)]\tLoss: 0.329116\t Accuracy:85.114%\n",
      "Rate : 92 [46080/49000 (94%)]\tLoss: 0.324515\t Accuracy:85.121%\n",
      "Rate : 92 [46720/49000 (95%)]\tLoss: 0.324422\t Accuracy:85.115%\n",
      "Rate : 92 [47360/49000 (97%)]\tLoss: 0.286136\t Accuracy:85.149%\n",
      "Rate : 92 [48000/49000 (98%)]\tLoss: 0.161923\t Accuracy:85.172%\n",
      "Rate : 92 [48640/49000 (99%)]\tLoss: 0.460883\t Accuracy:85.164%\n",
      "Rate : 94 [0/49000 (0%)]\tLoss: 2.872695\t Accuracy:15.625%\n",
      "Rate : 94 [640/49000 (1%)]\tLoss: 2.773003\t Accuracy:27.381%\n",
      "Rate : 94 [1280/49000 (3%)]\tLoss: 2.715264\t Accuracy:27.668%\n",
      "Rate : 94 [1920/49000 (4%)]\tLoss: 1.830300\t Accuracy:27.613%\n",
      "Rate : 94 [2560/49000 (5%)]\tLoss: 2.392591\t Accuracy:27.353%\n",
      "Rate : 94 [3200/49000 (7%)]\tLoss: 2.662784\t Accuracy:26.578%\n",
      "Rate : 94 [3840/49000 (8%)]\tLoss: 2.155172\t Accuracy:26.524%\n",
      "Rate : 94 [4480/49000 (9%)]\tLoss: 2.361311\t Accuracy:26.507%\n",
      "Rate : 94 [5120/49000 (10%)]\tLoss: 2.175892\t Accuracy:26.514%\n",
      "Rate : 94 [5760/49000 (12%)]\tLoss: 2.286490\t Accuracy:26.485%\n",
      "Rate : 94 [6400/49000 (13%)]\tLoss: 2.056239\t Accuracy:26.835%\n",
      "Rate : 94 [7040/49000 (14%)]\tLoss: 2.329578\t Accuracy:27.050%\n",
      "Rate : 94 [7680/49000 (16%)]\tLoss: 2.517844\t Accuracy:26.828%\n",
      "Rate : 94 [8320/49000 (17%)]\tLoss: 1.832607\t Accuracy:26.832%\n",
      "Rate : 94 [8960/49000 (18%)]\tLoss: 2.092988\t Accuracy:26.913%\n",
      "Rate : 94 [9600/49000 (20%)]\tLoss: 2.284254\t Accuracy:26.931%\n",
      "Rate : 94 [10240/49000 (21%)]\tLoss: 2.070269\t Accuracy:27.093%\n",
      "Rate : 94 [10880/49000 (22%)]\tLoss: 1.905072\t Accuracy:27.190%\n",
      "Rate : 94 [11520/49000 (23%)]\tLoss: 1.968146\t Accuracy:27.164%\n",
      "Rate : 94 [12160/49000 (25%)]\tLoss: 1.896145\t Accuracy:27.477%\n",
      "Rate : 94 [12800/49000 (26%)]\tLoss: 1.738914\t Accuracy:27.650%\n",
      "Rate : 94 [13440/49000 (27%)]\tLoss: 1.715117\t Accuracy:27.739%\n",
      "Rate : 94 [14080/49000 (29%)]\tLoss: 2.055570\t Accuracy:27.749%\n",
      "Rate : 94 [14720/49000 (30%)]\tLoss: 1.804963\t Accuracy:27.834%\n",
      "Rate : 94 [15360/49000 (31%)]\tLoss: 2.259751\t Accuracy:27.846%\n",
      "Rate : 94 [16000/49000 (33%)]\tLoss: 1.883491\t Accuracy:27.932%\n",
      "Rate : 94 [16640/49000 (34%)]\tLoss: 1.799122\t Accuracy:27.945%\n",
      "Rate : 94 [17280/49000 (35%)]\tLoss: 2.043624\t Accuracy:27.981%\n",
      "Rate : 94 [17920/49000 (37%)]\tLoss: 1.678657\t Accuracy:28.047%\n",
      "Rate : 94 [18560/49000 (38%)]\tLoss: 1.322120\t Accuracy:28.351%\n",
      "Rate : 94 [19200/49000 (39%)]\tLoss: 2.032061\t Accuracy:28.593%\n",
      "Rate : 94 [19840/49000 (40%)]\tLoss: 1.798011\t Accuracy:28.774%\n",
      "Rate : 94 [20480/49000 (42%)]\tLoss: 1.863533\t Accuracy:28.988%\n",
      "Rate : 94 [21120/49000 (43%)]\tLoss: 1.458246\t Accuracy:29.283%\n",
      "Rate : 94 [21760/49000 (44%)]\tLoss: 2.043618\t Accuracy:29.433%\n",
      "Rate : 94 [22400/49000 (46%)]\tLoss: 1.959667\t Accuracy:29.703%\n",
      "Rate : 94 [23040/49000 (47%)]\tLoss: 1.798870\t Accuracy:29.885%\n",
      "Rate : 94 [23680/49000 (48%)]\tLoss: 1.485068\t Accuracy:30.048%\n",
      "Rate : 94 [24320/49000 (50%)]\tLoss: 1.534954\t Accuracy:30.215%\n",
      "Rate : 94 [24960/49000 (51%)]\tLoss: 1.525670\t Accuracy:30.390%\n",
      "Rate : 94 [25600/49000 (52%)]\tLoss: 1.374487\t Accuracy:30.489%\n",
      "Rate : 94 [26240/49000 (54%)]\tLoss: 1.732019\t Accuracy:30.614%\n",
      "Rate : 94 [26880/49000 (55%)]\tLoss: 1.363339\t Accuracy:30.763%\n",
      "Rate : 94 [27520/49000 (56%)]\tLoss: 1.571938\t Accuracy:30.934%\n",
      "Rate : 94 [28160/49000 (57%)]\tLoss: 1.390521\t Accuracy:31.115%\n",
      "Rate : 94 [28800/49000 (59%)]\tLoss: 1.605439\t Accuracy:31.215%\n",
      "Rate : 94 [29440/49000 (60%)]\tLoss: 1.718682\t Accuracy:31.352%\n",
      "Rate : 94 [30080/49000 (61%)]\tLoss: 1.647152\t Accuracy:31.453%\n",
      "Rate : 94 [30720/49000 (63%)]\tLoss: 1.495221\t Accuracy:31.546%\n",
      "Rate : 94 [31360/49000 (64%)]\tLoss: 1.513606\t Accuracy:31.655%\n",
      "Rate : 94 [32000/49000 (65%)]\tLoss: 1.412307\t Accuracy:31.793%\n",
      "Rate : 94 [32640/49000 (67%)]\tLoss: 1.306424\t Accuracy:31.850%\n",
      "Rate : 94 [33280/49000 (68%)]\tLoss: 1.630002\t Accuracy:31.937%\n",
      "Rate : 94 [33920/49000 (69%)]\tLoss: 1.720905\t Accuracy:32.013%\n",
      "Rate : 94 [34560/49000 (70%)]\tLoss: 1.509075\t Accuracy:32.158%\n",
      "Rate : 94 [35200/49000 (72%)]\tLoss: 1.682581\t Accuracy:32.246%\n",
      "Rate : 94 [35840/49000 (73%)]\tLoss: 1.221713\t Accuracy:32.318%\n",
      "Rate : 94 [36480/49000 (74%)]\tLoss: 1.432221\t Accuracy:32.600%\n",
      "Rate : 94 [37120/49000 (76%)]\tLoss: 1.567952\t Accuracy:32.817%\n",
      "Rate : 94 [37760/49000 (77%)]\tLoss: 1.528759\t Accuracy:33.049%\n",
      "Rate : 94 [38400/49000 (78%)]\tLoss: 1.519871\t Accuracy:33.295%\n",
      "Rate : 94 [39040/49000 (80%)]\tLoss: 1.540031\t Accuracy:33.479%\n",
      "Rate : 94 [39680/49000 (81%)]\tLoss: 1.548278\t Accuracy:33.768%\n",
      "Rate : 94 [40320/49000 (82%)]\tLoss: 1.644847\t Accuracy:34.011%\n",
      "Rate : 94 [40960/49000 (84%)]\tLoss: 1.293897\t Accuracy:34.270%\n",
      "Rate : 94 [41600/49000 (85%)]\tLoss: 1.294722\t Accuracy:34.469%\n",
      "Rate : 94 [42240/49000 (86%)]\tLoss: 1.294809\t Accuracy:34.699%\n",
      "Rate : 94 [42880/49000 (87%)]\tLoss: 1.677997\t Accuracy:34.878%\n",
      "Rate : 94 [43520/49000 (89%)]\tLoss: 1.521624\t Accuracy:35.023%\n",
      "Rate : 94 [44160/49000 (90%)]\tLoss: 1.426129\t Accuracy:35.138%\n",
      "Rate : 94 [44800/49000 (91%)]\tLoss: 1.511205\t Accuracy:35.265%\n",
      "Rate : 94 [45440/49000 (93%)]\tLoss: 1.496160\t Accuracy:35.457%\n",
      "Rate : 94 [46080/49000 (94%)]\tLoss: 1.322311\t Accuracy:35.624%\n",
      "Rate : 94 [46720/49000 (95%)]\tLoss: 1.502588\t Accuracy:35.776%\n",
      "Rate : 94 [47360/49000 (97%)]\tLoss: 1.500350\t Accuracy:35.941%\n",
      "Rate : 94 [48000/49000 (98%)]\tLoss: 1.306490\t Accuracy:36.076%\n",
      "Rate : 94 [48640/49000 (99%)]\tLoss: 1.353046\t Accuracy:36.195%\n",
      "Rate : 94 [0/49000 (0%)]\tLoss: 1.301813\t Accuracy:59.375%\n",
      "Rate : 94 [640/49000 (1%)]\tLoss: 1.362494\t Accuracy:49.107%\n",
      "Rate : 94 [1280/49000 (3%)]\tLoss: 1.168595\t Accuracy:49.619%\n",
      "Rate : 94 [1920/49000 (4%)]\tLoss: 1.163285\t Accuracy:48.514%\n",
      "Rate : 94 [2560/49000 (5%)]\tLoss: 1.416302\t Accuracy:47.840%\n",
      "Rate : 94 [3200/49000 (7%)]\tLoss: 1.398196\t Accuracy:47.092%\n",
      "Rate : 94 [3840/49000 (8%)]\tLoss: 1.310728\t Accuracy:47.262%\n",
      "Rate : 94 [4480/49000 (9%)]\tLoss: 1.455583\t Accuracy:46.454%\n",
      "Rate : 94 [5120/49000 (10%)]\tLoss: 1.215954\t Accuracy:46.040%\n",
      "Rate : 94 [5760/49000 (12%)]\tLoss: 1.186431\t Accuracy:45.528%\n",
      "Rate : 94 [6400/49000 (13%)]\tLoss: 1.523486\t Accuracy:45.243%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate : 94 [7040/49000 (14%)]\tLoss: 1.269408\t Accuracy:45.404%\n",
      "Rate : 94 [7680/49000 (16%)]\tLoss: 1.743771\t Accuracy:45.176%\n",
      "Rate : 94 [8320/49000 (17%)]\tLoss: 1.242566\t Accuracy:45.007%\n",
      "Rate : 94 [8960/49000 (18%)]\tLoss: 1.446526\t Accuracy:44.862%\n",
      "Rate : 94 [9600/49000 (20%)]\tLoss: 1.397754\t Accuracy:44.892%\n",
      "Rate : 94 [10240/49000 (21%)]\tLoss: 1.364166\t Accuracy:44.714%\n",
      "Rate : 94 [10880/49000 (22%)]\tLoss: 1.353372\t Accuracy:44.776%\n",
      "Rate : 94 [11520/49000 (23%)]\tLoss: 1.276020\t Accuracy:44.642%\n",
      "Rate : 94 [12160/49000 (25%)]\tLoss: 1.572047\t Accuracy:44.636%\n",
      "Rate : 94 [12800/49000 (26%)]\tLoss: 1.251310\t Accuracy:44.631%\n",
      "Rate : 94 [13440/49000 (27%)]\tLoss: 1.355556\t Accuracy:44.537%\n",
      "Rate : 94 [14080/49000 (29%)]\tLoss: 1.529529\t Accuracy:44.572%\n",
      "Rate : 94 [14720/49000 (30%)]\tLoss: 1.070532\t Accuracy:44.591%\n",
      "Rate : 94 [15360/49000 (31%)]\tLoss: 1.412135\t Accuracy:44.575%\n",
      "Rate : 94 [16000/49000 (33%)]\tLoss: 1.240197\t Accuracy:44.654%\n",
      "Rate : 94 [16640/49000 (34%)]\tLoss: 1.372947\t Accuracy:44.524%\n",
      "Rate : 94 [17280/49000 (35%)]\tLoss: 1.608836\t Accuracy:44.466%\n",
      "Rate : 94 [17920/49000 (37%)]\tLoss: 1.214737\t Accuracy:44.508%\n",
      "Rate : 94 [18560/49000 (38%)]\tLoss: 1.134532\t Accuracy:44.508%\n",
      "Rate : 94 [19200/49000 (39%)]\tLoss: 1.533457\t Accuracy:44.494%\n",
      "Rate : 94 [19840/49000 (40%)]\tLoss: 1.425085\t Accuracy:44.374%\n",
      "Rate : 94 [20480/49000 (42%)]\tLoss: 1.461036\t Accuracy:44.340%\n",
      "Rate : 94 [21120/49000 (43%)]\tLoss: 1.108094\t Accuracy:44.332%\n",
      "Rate : 94 [21760/49000 (44%)]\tLoss: 1.501474\t Accuracy:44.250%\n",
      "Rate : 94 [22400/49000 (46%)]\tLoss: 1.569755\t Accuracy:44.365%\n",
      "Rate : 94 [23040/49000 (47%)]\tLoss: 1.451152\t Accuracy:44.318%\n",
      "Rate : 94 [23680/49000 (48%)]\tLoss: 1.347040\t Accuracy:44.302%\n",
      "Rate : 94 [24320/49000 (50%)]\tLoss: 1.307000\t Accuracy:44.313%\n",
      "Rate : 94 [24960/49000 (51%)]\tLoss: 1.275164\t Accuracy:44.390%\n",
      "Rate : 94 [25600/49000 (52%)]\tLoss: 1.111376\t Accuracy:44.343%\n",
      "Rate : 94 [26240/49000 (54%)]\tLoss: 1.320183\t Accuracy:44.317%\n",
      "Rate : 94 [26880/49000 (55%)]\tLoss: 1.125741\t Accuracy:44.289%\n",
      "Rate : 94 [27520/49000 (56%)]\tLoss: 1.165596\t Accuracy:44.316%\n",
      "Rate : 94 [28160/49000 (57%)]\tLoss: 1.228932\t Accuracy:44.364%\n",
      "Rate : 94 [28800/49000 (59%)]\tLoss: 1.131503\t Accuracy:44.388%\n",
      "Rate : 94 [29440/49000 (60%)]\tLoss: 1.331182\t Accuracy:44.429%\n",
      "Rate : 94 [30080/49000 (61%)]\tLoss: 1.369102\t Accuracy:44.374%\n",
      "Rate : 94 [30720/49000 (63%)]\tLoss: 1.088109\t Accuracy:44.391%\n",
      "Rate : 94 [31360/49000 (64%)]\tLoss: 1.347845\t Accuracy:44.390%\n",
      "Rate : 94 [32000/49000 (65%)]\tLoss: 1.149237\t Accuracy:44.365%\n",
      "Rate : 94 [32640/49000 (67%)]\tLoss: 1.200430\t Accuracy:44.377%\n",
      "Rate : 94 [33280/49000 (68%)]\tLoss: 1.291431\t Accuracy:44.401%\n",
      "Rate : 94 [33920/49000 (69%)]\tLoss: 1.400366\t Accuracy:44.398%\n",
      "Rate : 94 [34560/49000 (70%)]\tLoss: 1.203730\t Accuracy:44.441%\n",
      "Rate : 94 [35200/49000 (72%)]\tLoss: 1.518441\t Accuracy:44.386%\n",
      "Rate : 94 [35840/49000 (73%)]\tLoss: 1.056400\t Accuracy:44.372%\n",
      "Rate : 94 [36480/49000 (74%)]\tLoss: 1.171286\t Accuracy:44.374%\n",
      "Rate : 94 [37120/49000 (76%)]\tLoss: 1.356480\t Accuracy:44.353%\n",
      "Rate : 94 [37760/49000 (77%)]\tLoss: 1.402481\t Accuracy:44.337%\n",
      "Rate : 94 [38400/49000 (78%)]\tLoss: 1.377410\t Accuracy:44.351%\n",
      "Rate : 94 [39040/49000 (80%)]\tLoss: 1.270321\t Accuracy:44.328%\n",
      "Rate : 94 [39680/49000 (81%)]\tLoss: 1.495267\t Accuracy:44.407%\n",
      "Rate : 94 [40320/49000 (82%)]\tLoss: 1.255007\t Accuracy:44.451%\n",
      "Rate : 94 [40960/49000 (84%)]\tLoss: 1.123389\t Accuracy:44.514%\n",
      "Rate : 94 [41600/49000 (85%)]\tLoss: 1.118912\t Accuracy:44.507%\n",
      "Rate : 94 [42240/49000 (86%)]\tLoss: 1.203115\t Accuracy:44.516%\n",
      "Rate : 94 [42880/49000 (87%)]\tLoss: 1.487697\t Accuracy:44.556%\n",
      "Rate : 94 [43520/49000 (89%)]\tLoss: 1.385134\t Accuracy:44.508%\n",
      "Rate : 94 [44160/49000 (90%)]\tLoss: 1.270297\t Accuracy:44.474%\n",
      "Rate : 94 [44800/49000 (91%)]\tLoss: 1.325907\t Accuracy:44.470%\n",
      "Rate : 94 [45440/49000 (93%)]\tLoss: 1.349386\t Accuracy:44.568%\n",
      "Rate : 94 [46080/49000 (94%)]\tLoss: 1.156030\t Accuracy:44.609%\n",
      "Rate : 94 [46720/49000 (95%)]\tLoss: 1.330845\t Accuracy:44.618%\n",
      "Rate : 94 [47360/49000 (97%)]\tLoss: 1.315592\t Accuracy:44.668%\n",
      "Rate : 94 [48000/49000 (98%)]\tLoss: 1.171934\t Accuracy:44.724%\n",
      "Rate : 94 [48640/49000 (99%)]\tLoss: 1.124284\t Accuracy:44.732%\n",
      "Rate : 95 [0/49000 (0%)]\tLoss: 2.509866\t Accuracy:37.500%\n",
      "Rate : 95 [640/49000 (1%)]\tLoss: 3.074748\t Accuracy:22.470%\n",
      "Rate : 95 [1280/49000 (3%)]\tLoss: 3.335933\t Accuracy:22.104%\n",
      "Rate : 95 [1920/49000 (4%)]\tLoss: 2.478629\t Accuracy:21.414%\n",
      "Rate : 95 [2560/49000 (5%)]\tLoss: 2.700298\t Accuracy:20.756%\n",
      "Rate : 95 [3200/49000 (7%)]\tLoss: 2.527397\t Accuracy:20.514%\n",
      "Rate : 95 [3840/49000 (8%)]\tLoss: 2.458685\t Accuracy:20.455%\n",
      "Rate : 95 [4480/49000 (9%)]\tLoss: 2.748929\t Accuracy:20.567%\n",
      "Rate : 95 [5120/49000 (10%)]\tLoss: 2.262941\t Accuracy:20.672%\n",
      "Rate : 95 [5760/49000 (12%)]\tLoss: 2.342169\t Accuracy:20.994%\n",
      "Rate : 95 [6400/49000 (13%)]\tLoss: 2.154845\t Accuracy:20.662%\n",
      "Rate : 95 [7040/49000 (14%)]\tLoss: 1.916626\t Accuracy:20.772%\n",
      "Rate : 95 [7680/49000 (16%)]\tLoss: 2.189841\t Accuracy:20.708%\n",
      "Rate : 95 [8320/49000 (17%)]\tLoss: 1.996150\t Accuracy:20.761%\n",
      "Rate : 95 [8960/49000 (18%)]\tLoss: 2.005192\t Accuracy:20.707%\n",
      "Rate : 95 [9600/49000 (20%)]\tLoss: 2.043783\t Accuracy:20.764%\n",
      "Rate : 95 [10240/49000 (21%)]\tLoss: 1.953159\t Accuracy:20.824%\n",
      "Rate : 95 [10880/49000 (22%)]\tLoss: 2.319869\t Accuracy:20.885%\n",
      "Rate : 95 [11520/49000 (23%)]\tLoss: 2.252557\t Accuracy:20.862%\n",
      "Rate : 95 [12160/49000 (25%)]\tLoss: 2.073010\t Accuracy:20.891%\n",
      "Rate : 95 [12800/49000 (26%)]\tLoss: 2.225829\t Accuracy:20.854%\n",
      "Rate : 95 [13440/49000 (27%)]\tLoss: 2.198756\t Accuracy:20.776%\n",
      "Rate : 95 [14080/49000 (29%)]\tLoss: 2.359337\t Accuracy:20.904%\n",
      "Rate : 95 [14720/49000 (30%)]\tLoss: 2.179482\t Accuracy:20.960%\n",
      "Rate : 95 [15360/49000 (31%)]\tLoss: 1.984558\t Accuracy:21.011%\n",
      "Rate : 95 [16000/49000 (33%)]\tLoss: 2.262345\t Accuracy:21.070%\n",
      "Rate : 95 [16640/49000 (34%)]\tLoss: 2.217017\t Accuracy:21.083%\n",
      "Rate : 95 [17280/49000 (35%)]\tLoss: 2.344888\t Accuracy:21.037%\n",
      "Rate : 95 [17920/49000 (37%)]\tLoss: 2.058333\t Accuracy:20.995%\n",
      "Rate : 95 [18560/49000 (38%)]\tLoss: 2.311204\t Accuracy:20.971%\n",
      "Rate : 95 [19200/49000 (39%)]\tLoss: 2.157706\t Accuracy:20.918%\n",
      "Rate : 95 [19840/49000 (40%)]\tLoss: 2.131741\t Accuracy:20.853%\n",
      "Rate : 95 [20480/49000 (42%)]\tLoss: 2.208828\t Accuracy:20.793%\n",
      "Rate : 95 [21120/49000 (43%)]\tLoss: 1.874577\t Accuracy:20.854%\n",
      "Rate : 95 [21760/49000 (44%)]\tLoss: 2.414343\t Accuracy:20.783%\n",
      "Rate : 95 [22400/49000 (46%)]\tLoss: 2.253028\t Accuracy:20.769%\n",
      "Rate : 95 [23040/49000 (47%)]\tLoss: 2.296651\t Accuracy:20.679%\n",
      "Rate : 95 [23680/49000 (48%)]\tLoss: 2.266923\t Accuracy:20.694%\n",
      "Rate : 95 [24320/49000 (50%)]\tLoss: 2.187780\t Accuracy:20.676%\n",
      "Rate : 95 [24960/49000 (51%)]\tLoss: 2.114538\t Accuracy:20.703%\n",
      "Rate : 95 [25600/49000 (52%)]\tLoss: 2.004695\t Accuracy:20.701%\n",
      "Rate : 95 [26240/49000 (54%)]\tLoss: 2.086093\t Accuracy:20.665%\n",
      "Rate : 95 [26880/49000 (55%)]\tLoss: 2.091099\t Accuracy:20.656%\n",
      "Rate : 95 [27520/49000 (56%)]\tLoss: 1.983086\t Accuracy:20.717%\n",
      "Rate : 95 [28160/49000 (57%)]\tLoss: 2.187383\t Accuracy:20.772%\n",
      "Rate : 95 [28800/49000 (59%)]\tLoss: 1.957017\t Accuracy:20.824%\n",
      "Rate : 95 [29440/49000 (60%)]\tLoss: 1.985707\t Accuracy:20.881%\n",
      "Rate : 95 [30080/49000 (61%)]\tLoss: 2.127970\t Accuracy:20.839%\n",
      "Rate : 95 [30720/49000 (63%)]\tLoss: 2.038718\t Accuracy:20.844%\n",
      "Rate : 95 [31360/49000 (64%)]\tLoss: 2.076885\t Accuracy:20.840%\n",
      "Rate : 95 [32000/49000 (65%)]\tLoss: 2.021174\t Accuracy:20.829%\n",
      "Rate : 95 [32640/49000 (67%)]\tLoss: 2.151030\t Accuracy:20.862%\n",
      "Rate : 95 [33280/49000 (68%)]\tLoss: 2.222921\t Accuracy:20.863%\n",
      "Rate : 95 [33920/49000 (69%)]\tLoss: 1.869865\t Accuracy:20.856%\n",
      "Rate : 95 [34560/49000 (70%)]\tLoss: 2.203016\t Accuracy:20.866%\n",
      "Rate : 95 [35200/49000 (72%)]\tLoss: 2.167552\t Accuracy:20.856%\n",
      "Rate : 95 [35840/49000 (73%)]\tLoss: 2.003715\t Accuracy:20.883%\n",
      "Rate : 95 [36480/49000 (74%)]\tLoss: 2.152137\t Accuracy:20.881%\n",
      "Rate : 95 [37120/49000 (76%)]\tLoss: 2.148550\t Accuracy:20.852%\n",
      "Rate : 95 [37760/49000 (77%)]\tLoss: 2.140835\t Accuracy:20.843%\n",
      "Rate : 95 [38400/49000 (78%)]\tLoss: 2.085383\t Accuracy:20.829%\n",
      "Rate : 95 [39040/49000 (80%)]\tLoss: 1.944197\t Accuracy:20.808%\n",
      "Rate : 95 [39680/49000 (81%)]\tLoss: 2.016881\t Accuracy:20.853%\n",
      "Rate : 95 [40320/49000 (82%)]\tLoss: 2.146794\t Accuracy:20.881%\n",
      "Rate : 95 [40960/49000 (84%)]\tLoss: 2.241330\t Accuracy:20.909%\n",
      "Rate : 95 [41600/49000 (85%)]\tLoss: 2.110745\t Accuracy:20.902%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate : 95 [42240/49000 (86%)]\tLoss: 2.026985\t Accuracy:20.886%\n",
      "Rate : 95 [42880/49000 (87%)]\tLoss: 2.003364\t Accuracy:20.913%\n",
      "Rate : 95 [43520/49000 (89%)]\tLoss: 2.143719\t Accuracy:20.888%\n",
      "Rate : 95 [44160/49000 (90%)]\tLoss: 1.963371\t Accuracy:20.884%\n",
      "Rate : 95 [44800/49000 (91%)]\tLoss: 2.012301\t Accuracy:20.873%\n",
      "Rate : 95 [45440/49000 (93%)]\tLoss: 2.063858\t Accuracy:20.861%\n",
      "Rate : 95 [46080/49000 (94%)]\tLoss: 2.029886\t Accuracy:20.864%\n",
      "Rate : 95 [46720/49000 (95%)]\tLoss: 2.255351\t Accuracy:20.808%\n",
      "Rate : 95 [47360/49000 (97%)]\tLoss: 2.205945\t Accuracy:20.782%\n",
      "Rate : 95 [48000/49000 (98%)]\tLoss: 2.035656\t Accuracy:20.780%\n",
      "Rate : 95 [48640/49000 (99%)]\tLoss: 2.118158\t Accuracy:20.755%\n",
      "Rate : 95 [0/49000 (0%)]\tLoss: 2.021364\t Accuracy:9.375%\n",
      "Rate : 95 [640/49000 (1%)]\tLoss: 1.996522\t Accuracy:21.577%\n",
      "Rate : 95 [1280/49000 (3%)]\tLoss: 2.176027\t Accuracy:20.198%\n",
      "Rate : 95 [1920/49000 (4%)]\tLoss: 2.007594\t Accuracy:19.928%\n",
      "Rate : 95 [2560/49000 (5%)]\tLoss: 2.063580\t Accuracy:19.753%\n",
      "Rate : 95 [3200/49000 (7%)]\tLoss: 1.783230\t Accuracy:19.493%\n",
      "Rate : 95 [3840/49000 (8%)]\tLoss: 2.092041\t Accuracy:19.215%\n",
      "Rate : 95 [4480/49000 (9%)]\tLoss: 2.304791\t Accuracy:19.260%\n",
      "Rate : 95 [5120/49000 (10%)]\tLoss: 1.925295\t Accuracy:19.526%\n",
      "Rate : 95 [5760/49000 (12%)]\tLoss: 1.983851\t Accuracy:19.682%\n",
      "Rate : 95 [6400/49000 (13%)]\tLoss: 1.931413\t Accuracy:19.232%\n",
      "Rate : 95 [7040/49000 (14%)]\tLoss: 1.835058\t Accuracy:19.358%\n",
      "Rate : 95 [7680/49000 (16%)]\tLoss: 2.014631\t Accuracy:19.204%\n",
      "Rate : 95 [8320/49000 (17%)]\tLoss: 1.903632\t Accuracy:19.349%\n",
      "Rate : 95 [8960/49000 (18%)]\tLoss: 1.840243\t Accuracy:19.273%\n",
      "Rate : 95 [9600/49000 (20%)]\tLoss: 1.990165\t Accuracy:19.290%\n",
      "Rate : 95 [10240/49000 (21%)]\tLoss: 1.817924\t Accuracy:19.324%\n",
      "Rate : 95 [10880/49000 (22%)]\tLoss: 2.145576\t Accuracy:19.373%\n",
      "Rate : 95 [11520/49000 (23%)]\tLoss: 2.121809\t Accuracy:19.365%\n",
      "Rate : 95 [12160/49000 (25%)]\tLoss: 1.998111\t Accuracy:19.423%\n",
      "Rate : 95 [12800/49000 (26%)]\tLoss: 2.066254\t Accuracy:19.444%\n",
      "Rate : 95 [13440/49000 (27%)]\tLoss: 2.007053\t Accuracy:19.470%\n",
      "Rate : 95 [14080/49000 (29%)]\tLoss: 2.258723\t Accuracy:19.579%\n",
      "Rate : 95 [14720/49000 (30%)]\tLoss: 2.056370\t Accuracy:19.726%\n",
      "Rate : 95 [15360/49000 (31%)]\tLoss: 1.980115\t Accuracy:19.751%\n",
      "Rate : 95 [16000/49000 (33%)]\tLoss: 2.030121\t Accuracy:19.842%\n",
      "Rate : 95 [16640/49000 (34%)]\tLoss: 2.026856\t Accuracy:19.902%\n",
      "Rate : 95 [17280/49000 (35%)]\tLoss: 2.178964\t Accuracy:19.848%\n",
      "Rate : 95 [17920/49000 (37%)]\tLoss: 1.973457\t Accuracy:19.775%\n",
      "Rate : 95 [18560/49000 (38%)]\tLoss: 1.996800\t Accuracy:19.880%\n",
      "Rate : 95 [19200/49000 (39%)]\tLoss: 2.028724\t Accuracy:19.889%\n",
      "Rate : 95 [19840/49000 (40%)]\tLoss: 2.053646\t Accuracy:19.807%\n",
      "Rate : 95 [20480/49000 (42%)]\tLoss: 2.150102\t Accuracy:19.798%\n",
      "Rate : 95 [21120/49000 (43%)]\tLoss: 1.817366\t Accuracy:19.861%\n",
      "Rate : 95 [21760/49000 (44%)]\tLoss: 2.282654\t Accuracy:19.847%\n",
      "Rate : 95 [22400/49000 (46%)]\tLoss: 2.162012\t Accuracy:19.784%\n",
      "Rate : 95 [23040/49000 (47%)]\tLoss: 2.197788\t Accuracy:19.782%\n",
      "Rate : 95 [23680/49000 (48%)]\tLoss: 2.218391\t Accuracy:19.762%\n",
      "Rate : 95 [24320/49000 (50%)]\tLoss: 2.077777\t Accuracy:19.736%\n",
      "Rate : 95 [24960/49000 (51%)]\tLoss: 2.071942\t Accuracy:19.678%\n",
      "Rate : 95 [25600/49000 (52%)]\tLoss: 1.923170\t Accuracy:19.702%\n",
      "Rate : 95 [26240/49000 (54%)]\tLoss: 2.069577\t Accuracy:19.675%\n",
      "Rate : 95 [26880/49000 (55%)]\tLoss: 1.959557\t Accuracy:19.672%\n",
      "Rate : 95 [27520/49000 (56%)]\tLoss: 1.986574\t Accuracy:19.726%\n",
      "Rate : 95 [28160/49000 (57%)]\tLoss: 2.069310\t Accuracy:19.725%\n",
      "Rate : 95 [28800/49000 (59%)]\tLoss: 2.008138\t Accuracy:19.773%\n",
      "Rate : 95 [29440/49000 (60%)]\tLoss: 1.861097\t Accuracy:19.781%\n",
      "Rate : 95 [30080/49000 (61%)]\tLoss: 2.062355\t Accuracy:19.753%\n",
      "Rate : 95 [30720/49000 (63%)]\tLoss: 2.029928\t Accuracy:19.739%\n",
      "Rate : 95 [31360/49000 (64%)]\tLoss: 2.032785\t Accuracy:19.715%\n",
      "Rate : 95 [32000/49000 (65%)]\tLoss: 1.946822\t Accuracy:19.740%\n",
      "Rate : 95 [32640/49000 (67%)]\tLoss: 2.034016\t Accuracy:19.736%\n",
      "Rate : 95 [33280/49000 (68%)]\tLoss: 2.195857\t Accuracy:19.687%\n",
      "Rate : 95 [33920/49000 (69%)]\tLoss: 1.882439\t Accuracy:19.707%\n",
      "Rate : 95 [34560/49000 (70%)]\tLoss: 2.176314\t Accuracy:19.768%\n",
      "Rate : 95 [35200/49000 (72%)]\tLoss: 2.117910\t Accuracy:19.777%\n",
      "Rate : 95 [35840/49000 (73%)]\tLoss: 1.922013\t Accuracy:19.801%\n",
      "Rate : 95 [36480/49000 (74%)]\tLoss: 2.145695\t Accuracy:19.799%\n",
      "Rate : 95 [37120/49000 (76%)]\tLoss: 2.140301\t Accuracy:19.816%\n",
      "Rate : 95 [37760/49000 (77%)]\tLoss: 2.114834\t Accuracy:19.814%\n",
      "Rate : 95 [38400/49000 (78%)]\tLoss: 2.066186\t Accuracy:19.814%\n",
      "Rate : 95 [39040/49000 (80%)]\tLoss: 1.928957\t Accuracy:19.758%\n",
      "Rate : 95 [39680/49000 (81%)]\tLoss: 1.954116\t Accuracy:19.730%\n",
      "Rate : 95 [40320/49000 (82%)]\tLoss: 2.152390\t Accuracy:19.736%\n",
      "Rate : 95 [40960/49000 (84%)]\tLoss: 2.080952\t Accuracy:19.745%\n",
      "Rate : 95 [41600/49000 (85%)]\tLoss: 2.089796\t Accuracy:19.756%\n",
      "Rate : 95 [42240/49000 (86%)]\tLoss: 1.977679\t Accuracy:19.727%\n",
      "Rate : 95 [42880/49000 (87%)]\tLoss: 2.006083\t Accuracy:19.722%\n",
      "Rate : 95 [43520/49000 (89%)]\tLoss: 2.113870\t Accuracy:19.691%\n",
      "Rate : 95 [44160/49000 (90%)]\tLoss: 1.972843\t Accuracy:19.728%\n",
      "Rate : 95 [44800/49000 (91%)]\tLoss: 2.013243\t Accuracy:19.729%\n",
      "Rate : 95 [45440/49000 (93%)]\tLoss: 2.105714\t Accuracy:19.737%\n",
      "Rate : 95 [46080/49000 (94%)]\tLoss: 2.004464\t Accuracy:19.758%\n",
      "Rate : 95 [46720/49000 (95%)]\tLoss: 2.197281\t Accuracy:19.723%\n",
      "Rate : 95 [47360/49000 (97%)]\tLoss: 2.219145\t Accuracy:19.712%\n",
      "Rate : 95 [48000/49000 (98%)]\tLoss: 1.953505\t Accuracy:19.726%\n",
      "Rate : 95 [48640/49000 (99%)]\tLoss: 2.078621\t Accuracy:19.716%\n",
      "Rate : 96 [0/49000 (0%)]\tLoss: 2.246648\t Accuracy:3.125%\n",
      "Rate : 96 [640/49000 (1%)]\tLoss: 2.238414\t Accuracy:11.607%\n",
      "Rate : 96 [1280/49000 (3%)]\tLoss: 2.291190\t Accuracy:10.976%\n",
      "Rate : 96 [1920/49000 (4%)]\tLoss: 2.271876\t Accuracy:10.656%\n",
      "Rate : 96 [2560/49000 (5%)]\tLoss: 2.253876\t Accuracy:10.918%\n",
      "Rate : 96 [3200/49000 (7%)]\tLoss: 2.255549\t Accuracy:10.829%\n",
      "Rate : 96 [3840/49000 (8%)]\tLoss: 2.239679\t Accuracy:10.821%\n",
      "Rate : 96 [4480/49000 (9%)]\tLoss: 2.399211\t Accuracy:10.572%\n",
      "Rate : 96 [5120/49000 (10%)]\tLoss: 2.235205\t Accuracy:10.540%\n",
      "Rate : 96 [5760/49000 (12%)]\tLoss: 2.250558\t Accuracy:10.601%\n",
      "Rate : 96 [6400/49000 (13%)]\tLoss: 2.286946\t Accuracy:10.494%\n",
      "Rate : 96 [7040/49000 (14%)]\tLoss: 2.231865\t Accuracy:10.450%\n",
      "Rate : 96 [7680/49000 (16%)]\tLoss: 2.281238\t Accuracy:10.438%\n",
      "Rate : 96 [8320/49000 (17%)]\tLoss: 2.238809\t Accuracy:10.632%\n",
      "Rate : 96 [8960/49000 (18%)]\tLoss: 2.212683\t Accuracy:10.976%\n",
      "Rate : 96 [9600/49000 (20%)]\tLoss: 2.227366\t Accuracy:11.296%\n",
      "Rate : 96 [10240/49000 (21%)]\tLoss: 2.220887\t Accuracy:11.546%\n",
      "Rate : 96 [10880/49000 (22%)]\tLoss: 2.312725\t Accuracy:11.785%\n",
      "Rate : 96 [11520/49000 (23%)]\tLoss: 2.234997\t Accuracy:11.851%\n",
      "Rate : 96 [12160/49000 (25%)]\tLoss: 2.267464\t Accuracy:11.704%\n",
      "Rate : 96 [12800/49000 (26%)]\tLoss: 2.276201\t Accuracy:11.923%\n",
      "Rate : 96 [13440/49000 (27%)]\tLoss: 2.336413\t Accuracy:12.151%\n",
      "Rate : 96 [14080/49000 (29%)]\tLoss: 2.369810\t Accuracy:12.372%\n",
      "Rate : 96 [14720/49000 (30%)]\tLoss: 2.269855\t Accuracy:12.629%\n",
      "Rate : 96 [15360/49000 (31%)]\tLoss: 2.254678\t Accuracy:12.818%\n",
      "Rate : 96 [16000/49000 (33%)]\tLoss: 2.240020\t Accuracy:13.036%\n",
      "Rate : 96 [16640/49000 (34%)]\tLoss: 2.289365\t Accuracy:13.172%\n",
      "Rate : 96 [17280/49000 (35%)]\tLoss: 2.330509\t Accuracy:13.199%\n",
      "Rate : 96 [17920/49000 (37%)]\tLoss: 2.241557\t Accuracy:13.235%\n",
      "Rate : 96 [18560/49000 (38%)]\tLoss: 2.252671\t Accuracy:13.441%\n",
      "Rate : 96 [19200/49000 (39%)]\tLoss: 2.226195\t Accuracy:13.571%\n",
      "Rate : 96 [19840/49000 (40%)]\tLoss: 2.224778\t Accuracy:13.506%\n",
      "Rate : 96 [20480/49000 (42%)]\tLoss: 2.243913\t Accuracy:13.480%\n",
      "Rate : 96 [21120/49000 (43%)]\tLoss: 2.239747\t Accuracy:13.384%\n",
      "Rate : 96 [21760/49000 (44%)]\tLoss: 2.261474\t Accuracy:13.326%\n",
      "Rate : 96 [22400/49000 (46%)]\tLoss: 2.239333\t Accuracy:13.209%\n",
      "Rate : 96 [23040/49000 (47%)]\tLoss: 2.274840\t Accuracy:13.154%\n",
      "Rate : 96 [23680/49000 (48%)]\tLoss: 2.338552\t Accuracy:13.044%\n",
      "Rate : 96 [24320/49000 (50%)]\tLoss: 2.285772\t Accuracy:13.079%\n",
      "Rate : 96 [24960/49000 (51%)]\tLoss: 2.300660\t Accuracy:13.068%\n",
      "Rate : 96 [25600/49000 (52%)]\tLoss: 2.272565\t Accuracy:13.144%\n",
      "Rate : 96 [26240/49000 (54%)]\tLoss: 2.285859\t Accuracy:13.193%\n",
      "Rate : 96 [26880/49000 (55%)]\tLoss: 2.273269\t Accuracy:13.236%\n",
      "Rate : 96 [27520/49000 (56%)]\tLoss: 2.250190\t Accuracy:13.317%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate : 96 [28160/49000 (57%)]\tLoss: 2.269961\t Accuracy:13.373%\n",
      "Rate : 96 [28800/49000 (59%)]\tLoss: 2.211398\t Accuracy:13.478%\n",
      "Rate : 96 [29440/49000 (60%)]\tLoss: 2.187734\t Accuracy:13.525%\n",
      "Rate : 96 [30080/49000 (61%)]\tLoss: 2.251719\t Accuracy:13.539%\n",
      "Rate : 96 [30720/49000 (63%)]\tLoss: 2.226590\t Accuracy:13.573%\n",
      "Rate : 96 [31360/49000 (64%)]\tLoss: 2.234541\t Accuracy:13.596%\n",
      "Rate : 96 [32000/49000 (65%)]\tLoss: 2.215400\t Accuracy:13.652%\n",
      "Rate : 96 [32640/49000 (67%)]\tLoss: 2.264533\t Accuracy:13.697%\n",
      "Rate : 96 [33280/49000 (68%)]\tLoss: 2.277979\t Accuracy:13.692%\n",
      "Rate : 96 [33920/49000 (69%)]\tLoss: 2.219693\t Accuracy:13.643%\n",
      "Rate : 96 [34560/49000 (70%)]\tLoss: 2.295726\t Accuracy:13.700%\n",
      "Rate : 96 [35200/49000 (72%)]\tLoss: 2.232343\t Accuracy:13.658%\n",
      "Rate : 96 [35840/49000 (73%)]\tLoss: 2.250822\t Accuracy:13.623%\n",
      "Rate : 96 [36480/49000 (74%)]\tLoss: 2.241390\t Accuracy:13.554%\n",
      "Rate : 96 [37120/49000 (76%)]\tLoss: 2.217467\t Accuracy:13.504%\n",
      "Rate : 96 [37760/49000 (77%)]\tLoss: 2.213330\t Accuracy:13.476%\n",
      "Rate : 96 [38400/49000 (78%)]\tLoss: 2.185641\t Accuracy:13.434%\n",
      "Rate : 96 [39040/49000 (80%)]\tLoss: 2.141545\t Accuracy:13.345%\n",
      "Rate : 96 [39680/49000 (81%)]\tLoss: 2.199833\t Accuracy:13.316%\n",
      "Rate : 96 [40320/49000 (82%)]\tLoss: 2.165819\t Accuracy:13.278%\n",
      "Rate : 96 [40960/49000 (84%)]\tLoss: 2.163182\t Accuracy:13.268%\n",
      "Rate : 96 [41600/49000 (85%)]\tLoss: 2.237209\t Accuracy:13.235%\n",
      "Rate : 96 [42240/49000 (86%)]\tLoss: 2.160123\t Accuracy:13.179%\n",
      "Rate : 96 [42880/49000 (87%)]\tLoss: 2.176717\t Accuracy:13.148%\n",
      "Rate : 96 [43520/49000 (89%)]\tLoss: 2.210354\t Accuracy:13.097%\n",
      "Rate : 96 [44160/49000 (90%)]\tLoss: 2.164756\t Accuracy:13.075%\n",
      "Rate : 96 [44800/49000 (91%)]\tLoss: 2.155343\t Accuracy:13.046%\n",
      "Rate : 96 [45440/49000 (93%)]\tLoss: 2.160448\t Accuracy:13.043%\n",
      "Rate : 96 [46080/49000 (94%)]\tLoss: 2.165383\t Accuracy:13.023%\n",
      "Rate : 96 [46720/49000 (95%)]\tLoss: 2.226675\t Accuracy:13.005%\n",
      "Rate : 96 [47360/49000 (97%)]\tLoss: 2.245473\t Accuracy:12.971%\n",
      "Rate : 96 [48000/49000 (98%)]\tLoss: 2.127986\t Accuracy:12.971%\n",
      "Rate : 96 [48640/49000 (99%)]\tLoss: 2.166105\t Accuracy:12.960%\n",
      "Rate : 96 [0/49000 (0%)]\tLoss: 2.171420\t Accuracy:6.250%\n",
      "Rate : 96 [640/49000 (1%)]\tLoss: 2.187970\t Accuracy:13.690%\n",
      "Rate : 96 [1280/49000 (3%)]\tLoss: 2.307769\t Accuracy:12.957%\n",
      "Rate : 96 [1920/49000 (4%)]\tLoss: 2.161377\t Accuracy:12.602%\n",
      "Rate : 96 [2560/49000 (5%)]\tLoss: 2.148185\t Accuracy:12.616%\n",
      "Rate : 96 [3200/49000 (7%)]\tLoss: 2.062489\t Accuracy:12.624%\n",
      "Rate : 96 [3840/49000 (8%)]\tLoss: 2.183743\t Accuracy:12.577%\n",
      "Rate : 96 [4480/49000 (9%)]\tLoss: 2.370630\t Accuracy:12.855%\n",
      "Rate : 96 [5120/49000 (10%)]\tLoss: 2.022939\t Accuracy:13.412%\n",
      "Rate : 96 [5760/49000 (12%)]\tLoss: 2.297626\t Accuracy:13.657%\n",
      "Rate : 96 [6400/49000 (13%)]\tLoss: 2.208201\t Accuracy:13.448%\n",
      "Rate : 96 [7040/49000 (14%)]\tLoss: 2.053794\t Accuracy:13.575%\n",
      "Rate : 96 [7680/49000 (16%)]\tLoss: 2.156381\t Accuracy:13.589%\n",
      "Rate : 96 [8320/49000 (17%)]\tLoss: 2.055709\t Accuracy:13.937%\n",
      "Rate : 96 [8960/49000 (18%)]\tLoss: 1.990962\t Accuracy:14.079%\n",
      "Rate : 96 [9600/49000 (20%)]\tLoss: 2.049433\t Accuracy:14.286%\n",
      "Rate : 96 [10240/49000 (21%)]\tLoss: 2.138996\t Accuracy:14.525%\n",
      "Rate : 96 [10880/49000 (22%)]\tLoss: 2.162985\t Accuracy:14.699%\n",
      "Rate : 96 [11520/49000 (23%)]\tLoss: 2.113913\t Accuracy:14.794%\n",
      "Rate : 96 [12160/49000 (25%)]\tLoss: 2.158711\t Accuracy:15.002%\n",
      "Rate : 96 [12800/49000 (26%)]\tLoss: 2.191954\t Accuracy:15.165%\n",
      "Rate : 96 [13440/49000 (27%)]\tLoss: 2.153468\t Accuracy:15.313%\n",
      "Rate : 96 [14080/49000 (29%)]\tLoss: 2.243504\t Accuracy:15.554%\n",
      "Rate : 96 [14720/49000 (30%)]\tLoss: 2.141482\t Accuracy:15.754%\n",
      "Rate : 96 [15360/49000 (31%)]\tLoss: 2.120027\t Accuracy:15.846%\n",
      "Rate : 96 [16000/49000 (33%)]\tLoss: 2.105114\t Accuracy:15.962%\n",
      "Rate : 96 [16640/49000 (34%)]\tLoss: 2.151143\t Accuracy:16.075%\n",
      "Rate : 96 [17280/49000 (35%)]\tLoss: 2.262778\t Accuracy:16.081%\n",
      "Rate : 96 [17920/49000 (37%)]\tLoss: 2.028314\t Accuracy:16.076%\n",
      "Rate : 96 [18560/49000 (38%)]\tLoss: 1.985032\t Accuracy:16.233%\n",
      "Rate : 96 [19200/49000 (39%)]\tLoss: 2.079970\t Accuracy:16.311%\n",
      "Rate : 96 [19840/49000 (40%)]\tLoss: 2.146138\t Accuracy:16.264%\n",
      "Rate : 96 [20480/49000 (42%)]\tLoss: 2.182895\t Accuracy:16.312%\n",
      "Rate : 96 [21120/49000 (43%)]\tLoss: 1.914725\t Accuracy:16.410%\n",
      "Rate : 96 [21760/49000 (44%)]\tLoss: 2.325319\t Accuracy:16.423%\n",
      "Rate : 96 [22400/49000 (46%)]\tLoss: 2.164208\t Accuracy:16.410%\n",
      "Rate : 96 [23040/49000 (47%)]\tLoss: 2.220135\t Accuracy:16.436%\n",
      "Rate : 96 [23680/49000 (48%)]\tLoss: 2.253952\t Accuracy:16.460%\n",
      "Rate : 96 [24320/49000 (50%)]\tLoss: 2.111866\t Accuracy:16.450%\n",
      "Rate : 96 [24960/49000 (51%)]\tLoss: 2.234054\t Accuracy:16.433%\n",
      "Rate : 96 [25600/49000 (52%)]\tLoss: 2.123738\t Accuracy:16.491%\n",
      "Rate : 96 [26240/49000 (54%)]\tLoss: 2.167112\t Accuracy:16.512%\n",
      "Rate : 96 [26880/49000 (55%)]\tLoss: 2.026507\t Accuracy:16.573%\n",
      "Rate : 96 [27520/49000 (56%)]\tLoss: 2.033902\t Accuracy:16.645%\n",
      "Rate : 96 [28160/49000 (57%)]\tLoss: 2.123895\t Accuracy:16.689%\n",
      "Rate : 96 [28800/49000 (59%)]\tLoss: 2.013357\t Accuracy:16.759%\n",
      "Rate : 96 [29440/49000 (60%)]\tLoss: 1.968025\t Accuracy:16.806%\n",
      "Rate : 96 [30080/49000 (61%)]\tLoss: 2.108809\t Accuracy:16.797%\n",
      "Rate : 96 [30720/49000 (63%)]\tLoss: 2.102280\t Accuracy:16.831%\n",
      "Rate : 96 [31360/49000 (64%)]\tLoss: 2.179225\t Accuracy:16.839%\n",
      "Rate : 96 [32000/49000 (65%)]\tLoss: 1.930375\t Accuracy:16.883%\n",
      "Rate : 96 [32640/49000 (67%)]\tLoss: 2.124408\t Accuracy:16.914%\n",
      "Rate : 96 [33280/49000 (68%)]\tLoss: 2.165163\t Accuracy:16.886%\n",
      "Rate : 96 [33920/49000 (69%)]\tLoss: 1.989614\t Accuracy:16.930%\n",
      "Rate : 96 [34560/49000 (70%)]\tLoss: 2.312109\t Accuracy:16.987%\n",
      "Rate : 96 [35200/49000 (72%)]\tLoss: 2.166212\t Accuracy:17.007%\n",
      "Rate : 96 [35840/49000 (73%)]\tLoss: 2.157264\t Accuracy:17.058%\n",
      "Rate : 96 [36480/49000 (74%)]\tLoss: 2.180928\t Accuracy:17.077%\n",
      "Rate : 96 [37120/49000 (76%)]\tLoss: 2.123925\t Accuracy:17.105%\n",
      "Rate : 96 [37760/49000 (77%)]\tLoss: 2.145617\t Accuracy:17.139%\n",
      "Rate : 96 [38400/49000 (78%)]\tLoss: 2.129107\t Accuracy:17.160%\n",
      "Rate : 96 [39040/49000 (80%)]\tLoss: 2.035679\t Accuracy:17.135%\n",
      "Rate : 96 [39680/49000 (81%)]\tLoss: 2.041413\t Accuracy:17.123%\n",
      "Rate : 96 [40320/49000 (82%)]\tLoss: 2.101218\t Accuracy:17.159%\n",
      "Rate : 96 [40960/49000 (84%)]\tLoss: 2.133619\t Accuracy:17.177%\n",
      "Rate : 96 [41600/49000 (85%)]\tLoss: 2.165789\t Accuracy:17.203%\n",
      "Rate : 96 [42240/49000 (86%)]\tLoss: 2.058699\t Accuracy:17.191%\n",
      "Rate : 96 [42880/49000 (87%)]\tLoss: 2.070823\t Accuracy:17.198%\n",
      "Rate : 96 [43520/49000 (89%)]\tLoss: 2.116384\t Accuracy:17.182%\n",
      "Rate : 96 [44160/49000 (90%)]\tLoss: 1.984344\t Accuracy:17.229%\n",
      "Rate : 96 [44800/49000 (91%)]\tLoss: 2.044310\t Accuracy:17.247%\n",
      "Rate : 96 [45440/49000 (93%)]\tLoss: 2.191273\t Accuracy:17.261%\n",
      "Rate : 96 [46080/49000 (94%)]\tLoss: 2.079190\t Accuracy:17.301%\n",
      "Rate : 96 [46720/49000 (95%)]\tLoss: 2.111505\t Accuracy:17.289%\n",
      "Rate : 96 [47360/49000 (97%)]\tLoss: 2.197049\t Accuracy:17.307%\n",
      "Rate : 96 [48000/49000 (98%)]\tLoss: 2.001634\t Accuracy:17.347%\n",
      "Rate : 96 [48640/49000 (99%)]\tLoss: 2.084292\t Accuracy:17.347%\n",
      "Rate : 97 [0/49000 (0%)]\tLoss: 2.988552\t Accuracy:6.250%\n",
      "Rate : 97 [640/49000 (1%)]\tLoss: 2.664438\t Accuracy:11.012%\n",
      "Rate : 97 [1280/49000 (3%)]\tLoss: 2.661335\t Accuracy:10.595%\n",
      "Rate : 97 [1920/49000 (4%)]\tLoss: 2.436401\t Accuracy:10.348%\n",
      "Rate : 97 [2560/49000 (5%)]\tLoss: 2.397635\t Accuracy:9.799%\n",
      "Rate : 97 [3200/49000 (7%)]\tLoss: 2.224479\t Accuracy:9.561%\n",
      "Rate : 97 [3840/49000 (8%)]\tLoss: 2.348832\t Accuracy:9.323%\n",
      "Rate : 97 [4480/49000 (9%)]\tLoss: 2.496305\t Accuracy:9.530%\n",
      "Rate : 97 [5120/49000 (10%)]\tLoss: 2.219436\t Accuracy:9.783%\n",
      "Rate : 97 [5760/49000 (12%)]\tLoss: 2.277045\t Accuracy:10.307%\n",
      "Rate : 97 [6400/49000 (13%)]\tLoss: 2.302227\t Accuracy:10.401%\n",
      "Rate : 97 [7040/49000 (14%)]\tLoss: 2.224848\t Accuracy:10.888%\n",
      "Rate : 97 [7680/49000 (16%)]\tLoss: 2.293868\t Accuracy:11.138%\n",
      "Rate : 97 [8320/49000 (17%)]\tLoss: 2.268213\t Accuracy:11.602%\n",
      "Rate : 97 [8960/49000 (18%)]\tLoss: 2.219018\t Accuracy:11.877%\n",
      "Rate : 97 [9600/49000 (20%)]\tLoss: 2.233706\t Accuracy:12.137%\n",
      "Rate : 97 [10240/49000 (21%)]\tLoss: 2.223341\t Accuracy:12.335%\n",
      "Rate : 97 [10880/49000 (22%)]\tLoss: 2.322778\t Accuracy:12.537%\n",
      "Rate : 97 [11520/49000 (23%)]\tLoss: 2.252883\t Accuracy:12.734%\n",
      "Rate : 97 [12160/49000 (25%)]\tLoss: 2.275356\t Accuracy:12.959%\n",
      "Rate : 97 [12800/49000 (26%)]\tLoss: 2.284547\t Accuracy:13.116%\n",
      "Rate : 97 [13440/49000 (27%)]\tLoss: 2.354533\t Accuracy:13.287%\n",
      "Rate : 97 [14080/49000 (29%)]\tLoss: 2.382915\t Accuracy:13.485%\n",
      "Rate : 97 [14720/49000 (30%)]\tLoss: 2.269236\t Accuracy:13.693%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate : 97 [15360/49000 (31%)]\tLoss: 2.272720\t Accuracy:13.832%\n",
      "Rate : 97 [16000/49000 (33%)]\tLoss: 2.243033\t Accuracy:14.009%\n",
      "Rate : 97 [16640/49000 (34%)]\tLoss: 2.294134\t Accuracy:14.113%\n",
      "Rate : 97 [17280/49000 (35%)]\tLoss: 2.329976\t Accuracy:14.117%\n",
      "Rate : 97 [17920/49000 (37%)]\tLoss: 2.246353\t Accuracy:14.043%\n",
      "Rate : 97 [18560/49000 (38%)]\tLoss: 2.266382\t Accuracy:14.022%\n",
      "Rate : 97 [19200/49000 (39%)]\tLoss: 2.239723\t Accuracy:13.935%\n",
      "Rate : 97 [19840/49000 (40%)]\tLoss: 2.227458\t Accuracy:13.813%\n",
      "Rate : 97 [20480/49000 (42%)]\tLoss: 2.254349\t Accuracy:13.797%\n",
      "Rate : 97 [21120/49000 (43%)]\tLoss: 2.262101\t Accuracy:13.696%\n",
      "Rate : 97 [21760/49000 (44%)]\tLoss: 2.282128\t Accuracy:13.629%\n",
      "Rate : 97 [22400/49000 (46%)]\tLoss: 2.254308\t Accuracy:13.512%\n",
      "Rate : 97 [23040/49000 (47%)]\tLoss: 2.280868\t Accuracy:13.454%\n",
      "Rate : 97 [23680/49000 (48%)]\tLoss: 2.350784\t Accuracy:13.335%\n",
      "Rate : 97 [24320/49000 (50%)]\tLoss: 2.288186\t Accuracy:13.243%\n",
      "Rate : 97 [24960/49000 (51%)]\tLoss: 2.306688\t Accuracy:13.108%\n",
      "Rate : 97 [25600/49000 (52%)]\tLoss: 2.323270\t Accuracy:13.066%\n",
      "Rate : 97 [26240/49000 (54%)]\tLoss: 2.294314\t Accuracy:13.018%\n",
      "Rate : 97 [26880/49000 (55%)]\tLoss: 2.293439\t Accuracy:12.953%\n",
      "Rate : 97 [27520/49000 (56%)]\tLoss: 2.278917\t Accuracy:12.917%\n",
      "Rate : 97 [28160/49000 (57%)]\tLoss: 2.267906\t Accuracy:12.855%\n",
      "Rate : 97 [28800/49000 (59%)]\tLoss: 2.252289\t Accuracy:12.802%\n",
      "Rate : 97 [29440/49000 (60%)]\tLoss: 2.242302\t Accuracy:12.748%\n",
      "Rate : 97 [30080/49000 (61%)]\tLoss: 2.282101\t Accuracy:12.669%\n",
      "Rate : 97 [30720/49000 (63%)]\tLoss: 2.253277\t Accuracy:12.633%\n",
      "Rate : 97 [31360/49000 (64%)]\tLoss: 2.251771\t Accuracy:12.573%\n",
      "Rate : 97 [32000/49000 (65%)]\tLoss: 2.286060\t Accuracy:12.547%\n",
      "Rate : 97 [32640/49000 (67%)]\tLoss: 2.283324\t Accuracy:12.509%\n",
      "Rate : 97 [33280/49000 (68%)]\tLoss: 2.315520\t Accuracy:12.425%\n",
      "Rate : 97 [33920/49000 (69%)]\tLoss: 2.223447\t Accuracy:12.412%\n",
      "Rate : 97 [34560/49000 (70%)]\tLoss: 2.277816\t Accuracy:12.384%\n",
      "Rate : 97 [35200/49000 (72%)]\tLoss: 2.278362\t Accuracy:12.341%\n",
      "Rate : 97 [35840/49000 (73%)]\tLoss: 2.306448\t Accuracy:12.310%\n",
      "Rate : 97 [36480/49000 (74%)]\tLoss: 2.243627\t Accuracy:12.286%\n",
      "Rate : 97 [37120/49000 (76%)]\tLoss: 2.267807\t Accuracy:12.274%\n",
      "Rate : 97 [37760/49000 (77%)]\tLoss: 2.279703\t Accuracy:12.230%\n",
      "Rate : 97 [38400/49000 (78%)]\tLoss: 2.227183\t Accuracy:12.206%\n",
      "Rate : 97 [39040/49000 (80%)]\tLoss: 2.207464\t Accuracy:12.170%\n",
      "Rate : 97 [39680/49000 (81%)]\tLoss: 2.266088\t Accuracy:12.105%\n",
      "Rate : 97 [40320/49000 (82%)]\tLoss: 2.198949\t Accuracy:12.099%\n",
      "Rate : 97 [40960/49000 (84%)]\tLoss: 2.266704\t Accuracy:12.066%\n",
      "Rate : 97 [41600/49000 (85%)]\tLoss: 2.322413\t Accuracy:12.063%\n",
      "Rate : 97 [42240/49000 (86%)]\tLoss: 2.244731\t Accuracy:12.015%\n",
      "Rate : 97 [42880/49000 (87%)]\tLoss: 2.288319\t Accuracy:11.980%\n",
      "Rate : 97 [43520/49000 (89%)]\tLoss: 2.260068\t Accuracy:11.944%\n",
      "Rate : 97 [44160/49000 (90%)]\tLoss: 2.258860\t Accuracy:11.948%\n",
      "Rate : 97 [44800/49000 (91%)]\tLoss: 2.227946\t Accuracy:11.925%\n",
      "Rate : 97 [45440/49000 (93%)]\tLoss: 2.212685\t Accuracy:11.897%\n",
      "Rate : 97 [46080/49000 (94%)]\tLoss: 2.288153\t Accuracy:11.895%\n",
      "Rate : 97 [46720/49000 (95%)]\tLoss: 2.307124\t Accuracy:11.869%\n",
      "Rate : 97 [47360/49000 (97%)]\tLoss: 2.261433\t Accuracy:11.848%\n",
      "Rate : 97 [48000/49000 (98%)]\tLoss: 2.219507\t Accuracy:11.836%\n",
      "Rate : 97 [48640/49000 (99%)]\tLoss: 2.254992\t Accuracy:11.806%\n",
      "Rate : 97 [0/49000 (0%)]\tLoss: 2.279413\t Accuracy:3.125%\n",
      "Rate : 97 [640/49000 (1%)]\tLoss: 2.243640\t Accuracy:11.012%\n",
      "Rate : 97 [1280/49000 (3%)]\tLoss: 2.310592\t Accuracy:10.137%\n",
      "Rate : 97 [1920/49000 (4%)]\tLoss: 2.266915\t Accuracy:10.143%\n",
      "Rate : 97 [2560/49000 (5%)]\tLoss: 2.271486\t Accuracy:10.648%\n",
      "Rate : 97 [3200/49000 (7%)]\tLoss: 2.252235\t Accuracy:10.613%\n",
      "Rate : 97 [3840/49000 (8%)]\tLoss: 2.251932\t Accuracy:10.718%\n",
      "Rate : 97 [4480/49000 (9%)]\tLoss: 2.414708\t Accuracy:10.483%\n",
      "Rate : 97 [5120/49000 (10%)]\tLoss: 2.237674\t Accuracy:10.481%\n",
      "Rate : 97 [5760/49000 (12%)]\tLoss: 2.255429\t Accuracy:10.376%\n",
      "Rate : 97 [6400/49000 (13%)]\tLoss: 2.311585\t Accuracy:10.090%\n",
      "Rate : 97 [7040/49000 (14%)]\tLoss: 2.237080\t Accuracy:10.096%\n",
      "Rate : 97 [7680/49000 (16%)]\tLoss: 2.285824\t Accuracy:9.997%\n",
      "Rate : 97 [8320/49000 (17%)]\tLoss: 2.259941\t Accuracy:10.057%\n",
      "Rate : 97 [8960/49000 (18%)]\tLoss: 2.223504\t Accuracy:9.987%\n",
      "Rate : 97 [9600/49000 (20%)]\tLoss: 2.229954\t Accuracy:9.956%\n",
      "Rate : 97 [10240/49000 (21%)]\tLoss: 2.240264\t Accuracy:9.901%\n",
      "Rate : 97 [10880/49000 (22%)]\tLoss: 2.306489\t Accuracy:9.934%\n",
      "Rate : 97 [11520/49000 (23%)]\tLoss: 2.236892\t Accuracy:10.007%\n",
      "Rate : 97 [12160/49000 (25%)]\tLoss: 2.275613\t Accuracy:9.957%\n",
      "Rate : 97 [12800/49000 (26%)]\tLoss: 2.275586\t Accuracy:9.959%\n",
      "Rate : 97 [13440/49000 (27%)]\tLoss: 2.351642\t Accuracy:9.999%\n",
      "Rate : 97 [14080/49000 (29%)]\tLoss: 2.382259\t Accuracy:10.041%\n",
      "Rate : 97 [14720/49000 (30%)]\tLoss: 2.265534\t Accuracy:10.107%\n",
      "Rate : 97 [15360/49000 (31%)]\tLoss: 2.268401\t Accuracy:10.142%\n",
      "Rate : 97 [16000/49000 (33%)]\tLoss: 2.242394\t Accuracy:10.161%\n",
      "Rate : 97 [16640/49000 (34%)]\tLoss: 2.291365\t Accuracy:10.203%\n",
      "Rate : 97 [17280/49000 (35%)]\tLoss: 2.325710\t Accuracy:10.161%\n",
      "Rate : 97 [17920/49000 (37%)]\tLoss: 2.247872\t Accuracy:10.155%\n",
      "Rate : 97 [18560/49000 (38%)]\tLoss: 2.268070\t Accuracy:10.268%\n",
      "Rate : 97 [19200/49000 (39%)]\tLoss: 2.238155\t Accuracy:10.306%\n",
      "Rate : 97 [19840/49000 (40%)]\tLoss: 2.226510\t Accuracy:10.301%\n",
      "Rate : 97 [20480/49000 (42%)]\tLoss: 2.251431\t Accuracy:10.394%\n",
      "Rate : 97 [21120/49000 (43%)]\tLoss: 2.264451\t Accuracy:10.396%\n",
      "Rate : 97 [21760/49000 (44%)]\tLoss: 2.280518\t Accuracy:10.426%\n",
      "Rate : 97 [22400/49000 (46%)]\tLoss: 2.250668\t Accuracy:10.400%\n",
      "Rate : 97 [23040/49000 (47%)]\tLoss: 2.276997\t Accuracy:10.428%\n",
      "Rate : 97 [23680/49000 (48%)]\tLoss: 2.351017\t Accuracy:10.391%\n",
      "Rate : 97 [24320/49000 (50%)]\tLoss: 2.287980\t Accuracy:10.377%\n",
      "Rate : 97 [24960/49000 (51%)]\tLoss: 2.306370\t Accuracy:10.315%\n",
      "Rate : 97 [25600/49000 (52%)]\tLoss: 2.325465\t Accuracy:10.343%\n",
      "Rate : 97 [26240/49000 (54%)]\tLoss: 2.293586\t Accuracy:10.361%\n",
      "Rate : 97 [26880/49000 (55%)]\tLoss: 2.292853\t Accuracy:10.360%\n",
      "Rate : 97 [27520/49000 (56%)]\tLoss: 2.278567\t Accuracy:10.384%\n",
      "Rate : 97 [28160/49000 (57%)]\tLoss: 2.267794\t Accuracy:10.379%\n",
      "Rate : 97 [28800/49000 (59%)]\tLoss: 2.252155\t Accuracy:10.381%\n",
      "Rate : 97 [29440/49000 (60%)]\tLoss: 2.241392\t Accuracy:10.379%\n",
      "Rate : 97 [30080/49000 (61%)]\tLoss: 2.282126\t Accuracy:10.351%\n",
      "Rate : 97 [30720/49000 (63%)]\tLoss: 2.252721\t Accuracy:10.364%\n",
      "Rate : 97 [31360/49000 (64%)]\tLoss: 2.250853\t Accuracy:10.350%\n",
      "Rate : 97 [32000/49000 (65%)]\tLoss: 2.285850\t Accuracy:10.368%\n",
      "Rate : 97 [32640/49000 (67%)]\tLoss: 2.282812\t Accuracy:10.373%\n",
      "Rate : 97 [33280/49000 (68%)]\tLoss: 2.316439\t Accuracy:10.330%\n",
      "Rate : 97 [33920/49000 (69%)]\tLoss: 2.223493\t Accuracy:10.356%\n",
      "Rate : 97 [34560/49000 (70%)]\tLoss: 2.277570\t Accuracy:10.367%\n",
      "Rate : 97 [35200/49000 (72%)]\tLoss: 2.278788\t Accuracy:10.360%\n",
      "Rate : 97 [35840/49000 (73%)]\tLoss: 2.306855\t Accuracy:10.365%\n",
      "Rate : 97 [36480/49000 (74%)]\tLoss: 2.243897\t Accuracy:10.375%\n",
      "Rate : 97 [37120/49000 (76%)]\tLoss: 2.267516\t Accuracy:10.395%\n",
      "Rate : 97 [37760/49000 (77%)]\tLoss: 2.279506\t Accuracy:10.383%\n",
      "Rate : 97 [38400/49000 (78%)]\tLoss: 2.226872\t Accuracy:10.390%\n",
      "Rate : 97 [39040/49000 (80%)]\tLoss: 2.207120\t Accuracy:10.383%\n",
      "Rate : 97 [39680/49000 (81%)]\tLoss: 2.265799\t Accuracy:10.347%\n",
      "Rate : 97 [40320/49000 (82%)]\tLoss: 2.197884\t Accuracy:10.369%\n",
      "Rate : 97 [40960/49000 (84%)]\tLoss: 2.266512\t Accuracy:10.363%\n",
      "Rate : 97 [41600/49000 (85%)]\tLoss: 2.323165\t Accuracy:10.386%\n",
      "Rate : 97 [42240/49000 (86%)]\tLoss: 2.244194\t Accuracy:10.364%\n",
      "Rate : 97 [42880/49000 (87%)]\tLoss: 2.288245\t Accuracy:10.354%\n",
      "Rate : 97 [43520/49000 (89%)]\tLoss: 2.259481\t Accuracy:10.342%\n",
      "Rate : 97 [44160/49000 (90%)]\tLoss: 2.258641\t Accuracy:10.368%\n",
      "Rate : 97 [44800/49000 (91%)]\tLoss: 2.227409\t Accuracy:10.368%\n",
      "Rate : 97 [45440/49000 (93%)]\tLoss: 2.212408\t Accuracy:10.362%\n",
      "Rate : 97 [46080/49000 (94%)]\tLoss: 2.287971\t Accuracy:10.381%\n",
      "Rate : 97 [46720/49000 (95%)]\tLoss: 2.306845\t Accuracy:10.376%\n",
      "Rate : 97 [47360/49000 (97%)]\tLoss: 2.261067\t Accuracy:10.375%\n",
      "Rate : 97 [48000/49000 (98%)]\tLoss: 2.219002\t Accuracy:10.383%\n",
      "Rate : 97 [48640/49000 (99%)]\tLoss: 2.254851\t Accuracy:10.371%\n",
      "Rate : 98 [0/49000 (0%)]\tLoss: 2.420406\t Accuracy:3.125%\n",
      "Rate : 98 [640/49000 (1%)]\tLoss: 2.266335\t Accuracy:11.161%\n",
      "Rate : 98 [1280/49000 (3%)]\tLoss: 2.284024\t Accuracy:10.518%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate : 98 [1920/49000 (4%)]\tLoss: 2.353261\t Accuracy:10.553%\n",
      "Rate : 98 [2560/49000 (5%)]\tLoss: 2.312190\t Accuracy:10.880%\n",
      "Rate : 98 [3200/49000 (7%)]\tLoss: 2.295125\t Accuracy:10.922%\n",
      "Rate : 98 [3840/49000 (8%)]\tLoss: 2.293232\t Accuracy:11.028%\n",
      "Rate : 98 [4480/49000 (9%)]\tLoss: 2.284256\t Accuracy:10.926%\n",
      "Rate : 98 [5120/49000 (10%)]\tLoss: 2.299199\t Accuracy:10.908%\n",
      "Rate : 98 [5760/49000 (12%)]\tLoss: 2.309382\t Accuracy:10.773%\n",
      "Rate : 98 [6400/49000 (13%)]\tLoss: 2.337334\t Accuracy:10.370%\n",
      "Rate : 98 [7040/49000 (14%)]\tLoss: 2.290235\t Accuracy:10.365%\n",
      "Rate : 98 [7680/49000 (16%)]\tLoss: 2.306007\t Accuracy:10.309%\n",
      "Rate : 98 [8320/49000 (17%)]\tLoss: 2.287733\t Accuracy:10.273%\n",
      "Rate : 98 [8960/49000 (18%)]\tLoss: 2.298998\t Accuracy:10.287%\n",
      "Rate : 98 [9600/49000 (20%)]\tLoss: 2.287854\t Accuracy:10.361%\n",
      "Rate : 98 [10240/49000 (21%)]\tLoss: 2.293469\t Accuracy:10.378%\n",
      "Rate : 98 [10880/49000 (22%)]\tLoss: 2.318102\t Accuracy:10.484%\n",
      "Rate : 98 [11520/49000 (23%)]\tLoss: 2.286529\t Accuracy:10.587%\n",
      "Rate : 98 [12160/49000 (25%)]\tLoss: 2.299140\t Accuracy:10.540%\n",
      "Rate : 98 [12800/49000 (26%)]\tLoss: 2.309047\t Accuracy:10.513%\n",
      "Rate : 98 [13440/49000 (27%)]\tLoss: 2.311160\t Accuracy:10.451%\n",
      "Rate : 98 [14080/49000 (29%)]\tLoss: 2.312369\t Accuracy:10.523%\n",
      "Rate : 98 [14720/49000 (30%)]\tLoss: 2.306796\t Accuracy:10.548%\n",
      "Rate : 98 [15360/49000 (31%)]\tLoss: 2.290780\t Accuracy:10.609%\n",
      "Rate : 98 [16000/49000 (33%)]\tLoss: 2.304919\t Accuracy:10.629%\n",
      "Rate : 98 [16640/49000 (34%)]\tLoss: 2.319448\t Accuracy:10.611%\n",
      "Rate : 98 [17280/49000 (35%)]\tLoss: 2.327168\t Accuracy:10.588%\n",
      "Rate : 98 [17920/49000 (37%)]\tLoss: 2.300895\t Accuracy:10.634%\n",
      "Rate : 98 [18560/49000 (38%)]\tLoss: 2.315514\t Accuracy:10.623%\n",
      "Rate : 98 [19200/49000 (39%)]\tLoss: 2.297085\t Accuracy:10.623%\n",
      "Rate : 98 [19840/49000 (40%)]\tLoss: 2.290935\t Accuracy:10.563%\n",
      "Rate : 98 [20480/49000 (42%)]\tLoss: 2.289466\t Accuracy:10.643%\n",
      "Rate : 98 [21120/49000 (43%)]\tLoss: 2.307353\t Accuracy:10.628%\n",
      "Rate : 98 [21760/49000 (44%)]\tLoss: 2.302100\t Accuracy:10.642%\n",
      "Rate : 98 [22400/49000 (46%)]\tLoss: 2.294842\t Accuracy:10.619%\n",
      "Rate : 98 [23040/49000 (47%)]\tLoss: 2.317471\t Accuracy:10.576%\n",
      "Rate : 98 [23680/49000 (48%)]\tLoss: 2.317334\t Accuracy:10.560%\n",
      "Rate : 98 [24320/49000 (50%)]\tLoss: 2.302345\t Accuracy:10.574%\n",
      "Rate : 98 [24960/49000 (51%)]\tLoss: 2.314116\t Accuracy:10.607%\n",
      "Rate : 98 [25600/49000 (52%)]\tLoss: 2.307047\t Accuracy:10.620%\n",
      "Rate : 98 [26240/49000 (54%)]\tLoss: 2.291598\t Accuracy:10.635%\n",
      "Rate : 98 [26880/49000 (55%)]\tLoss: 2.311682\t Accuracy:10.635%\n",
      "Rate : 98 [27520/49000 (56%)]\tLoss: 2.306897\t Accuracy:10.671%\n",
      "Rate : 98 [28160/49000 (57%)]\tLoss: 2.312046\t Accuracy:10.723%\n",
      "Rate : 98 [28800/49000 (59%)]\tLoss: 2.286719\t Accuracy:10.731%\n",
      "Rate : 98 [29440/49000 (60%)]\tLoss: 2.300848\t Accuracy:10.790%\n",
      "Rate : 98 [30080/49000 (61%)]\tLoss: 2.310602\t Accuracy:10.760%\n",
      "Rate : 98 [30720/49000 (63%)]\tLoss: 2.293321\t Accuracy:10.799%\n",
      "Rate : 98 [31360/49000 (64%)]\tLoss: 2.309971\t Accuracy:10.812%\n",
      "Rate : 98 [32000/49000 (65%)]\tLoss: 2.310173\t Accuracy:10.799%\n",
      "Rate : 98 [32640/49000 (67%)]\tLoss: 2.311555\t Accuracy:10.844%\n",
      "Rate : 98 [33280/49000 (68%)]\tLoss: 2.304058\t Accuracy:10.861%\n",
      "Rate : 98 [33920/49000 (69%)]\tLoss: 2.297821\t Accuracy:10.862%\n",
      "Rate : 98 [34560/49000 (70%)]\tLoss: 2.304997\t Accuracy:10.823%\n",
      "Rate : 98 [35200/49000 (72%)]\tLoss: 2.314886\t Accuracy:10.806%\n",
      "Rate : 98 [35840/49000 (73%)]\tLoss: 2.303353\t Accuracy:10.819%\n",
      "Rate : 98 [36480/49000 (74%)]\tLoss: 2.286719\t Accuracy:10.840%\n",
      "Rate : 98 [37120/49000 (76%)]\tLoss: 2.307844\t Accuracy:10.823%\n",
      "Rate : 98 [37760/49000 (77%)]\tLoss: 2.305987\t Accuracy:10.836%\n",
      "Rate : 98 [38400/49000 (78%)]\tLoss: 2.303373\t Accuracy:10.863%\n",
      "Rate : 98 [39040/49000 (80%)]\tLoss: 2.280793\t Accuracy:10.898%\n",
      "Rate : 98 [39680/49000 (81%)]\tLoss: 2.321644\t Accuracy:10.939%\n",
      "Rate : 98 [40320/49000 (82%)]\tLoss: 2.275530\t Accuracy:10.981%\n",
      "Rate : 98 [40960/49000 (84%)]\tLoss: 2.306447\t Accuracy:10.997%\n",
      "Rate : 98 [41600/49000 (85%)]\tLoss: 2.293069\t Accuracy:11.001%\n",
      "Rate : 98 [42240/49000 (86%)]\tLoss: 2.296600\t Accuracy:11.003%\n",
      "Rate : 98 [42880/49000 (87%)]\tLoss: 2.299314\t Accuracy:11.025%\n",
      "Rate : 98 [43520/49000 (89%)]\tLoss: 2.304092\t Accuracy:11.019%\n",
      "Rate : 98 [44160/49000 (90%)]\tLoss: 2.294796\t Accuracy:11.013%\n",
      "Rate : 98 [44800/49000 (91%)]\tLoss: 2.289433\t Accuracy:11.006%\n",
      "Rate : 98 [45440/49000 (93%)]\tLoss: 2.287280\t Accuracy:11.000%\n",
      "Rate : 98 [46080/49000 (94%)]\tLoss: 2.306867\t Accuracy:11.017%\n",
      "Rate : 98 [46720/49000 (95%)]\tLoss: 2.325537\t Accuracy:11.028%\n",
      "Rate : 98 [47360/49000 (97%)]\tLoss: 2.284331\t Accuracy:11.021%\n",
      "Rate : 98 [48000/49000 (98%)]\tLoss: 2.307395\t Accuracy:11.005%\n",
      "Rate : 98 [48640/49000 (99%)]\tLoss: 2.277182\t Accuracy:10.996%\n",
      "Rate : 98 [0/49000 (0%)]\tLoss: 2.272014\t Accuracy:31.250%\n",
      "Rate : 98 [640/49000 (1%)]\tLoss: 2.295950\t Accuracy:11.607%\n",
      "Rate : 98 [1280/49000 (3%)]\tLoss: 2.295106\t Accuracy:11.814%\n",
      "Rate : 98 [1920/49000 (4%)]\tLoss: 2.312099\t Accuracy:11.424%\n",
      "Rate : 98 [2560/49000 (5%)]\tLoss: 2.298210\t Accuracy:11.343%\n",
      "Rate : 98 [3200/49000 (7%)]\tLoss: 2.297259\t Accuracy:11.417%\n",
      "Rate : 98 [3840/49000 (8%)]\tLoss: 2.282798\t Accuracy:11.648%\n",
      "Rate : 98 [4480/49000 (9%)]\tLoss: 2.288901\t Accuracy:11.525%\n",
      "Rate : 98 [5120/49000 (10%)]\tLoss: 2.307877\t Accuracy:11.355%\n",
      "Rate : 98 [5760/49000 (12%)]\tLoss: 2.305136\t Accuracy:11.481%\n",
      "Rate : 98 [6400/49000 (13%)]\tLoss: 2.328059\t Accuracy:11.318%\n",
      "Rate : 98 [7040/49000 (14%)]\tLoss: 2.295891\t Accuracy:11.312%\n",
      "Rate : 98 [7680/49000 (16%)]\tLoss: 2.305229\t Accuracy:11.294%\n",
      "Rate : 98 [8320/49000 (17%)]\tLoss: 2.279834\t Accuracy:11.183%\n",
      "Rate : 98 [8960/49000 (18%)]\tLoss: 2.297116\t Accuracy:11.132%\n",
      "Rate : 98 [9600/49000 (20%)]\tLoss: 2.290829\t Accuracy:11.150%\n",
      "Rate : 98 [10240/49000 (21%)]\tLoss: 2.296206\t Accuracy:11.118%\n",
      "Rate : 98 [10880/49000 (22%)]\tLoss: 2.312955\t Accuracy:11.180%\n",
      "Rate : 98 [11520/49000 (23%)]\tLoss: 2.285545\t Accuracy:11.245%\n",
      "Rate : 98 [12160/49000 (25%)]\tLoss: 2.297695\t Accuracy:11.163%\n",
      "Rate : 98 [12800/49000 (26%)]\tLoss: 2.306727\t Accuracy:11.105%\n",
      "Rate : 98 [13440/49000 (27%)]\tLoss: 2.308226\t Accuracy:11.015%\n",
      "Rate : 98 [14080/49000 (29%)]\tLoss: 2.312042\t Accuracy:11.062%\n",
      "Rate : 98 [14720/49000 (30%)]\tLoss: 2.306319\t Accuracy:11.063%\n",
      "Rate : 98 [15360/49000 (31%)]\tLoss: 2.290184\t Accuracy:11.103%\n",
      "Rate : 98 [16000/49000 (33%)]\tLoss: 2.305530\t Accuracy:11.103%\n",
      "Rate : 98 [16640/49000 (34%)]\tLoss: 2.318446\t Accuracy:11.090%\n",
      "Rate : 98 [17280/49000 (35%)]\tLoss: 2.325944\t Accuracy:11.050%\n",
      "Rate : 98 [17920/49000 (37%)]\tLoss: 2.301446\t Accuracy:11.080%\n",
      "Rate : 98 [18560/49000 (38%)]\tLoss: 2.315921\t Accuracy:11.053%\n",
      "Rate : 98 [19200/49000 (39%)]\tLoss: 2.297135\t Accuracy:11.049%\n",
      "Rate : 98 [19840/49000 (40%)]\tLoss: 2.291352\t Accuracy:11.000%\n",
      "Rate : 98 [20480/49000 (42%)]\tLoss: 2.289600\t Accuracy:11.023%\n",
      "Rate : 98 [21120/49000 (43%)]\tLoss: 2.307405\t Accuracy:10.997%\n",
      "Rate : 98 [21760/49000 (44%)]\tLoss: 2.301736\t Accuracy:10.999%\n",
      "Rate : 98 [22400/49000 (46%)]\tLoss: 2.294593\t Accuracy:10.971%\n",
      "Rate : 98 [23040/49000 (47%)]\tLoss: 2.317071\t Accuracy:10.918%\n",
      "Rate : 98 [23680/49000 (48%)]\tLoss: 2.317154\t Accuracy:10.919%\n",
      "Rate : 98 [24320/49000 (50%)]\tLoss: 2.302376\t Accuracy:10.923%\n",
      "Rate : 98 [24960/49000 (51%)]\tLoss: 2.313979\t Accuracy:10.948%\n",
      "Rate : 98 [25600/49000 (52%)]\tLoss: 2.307085\t Accuracy:10.951%\n",
      "Rate : 98 [26240/49000 (54%)]\tLoss: 2.291561\t Accuracy:10.958%\n",
      "Rate : 98 [26880/49000 (55%)]\tLoss: 2.311483\t Accuracy:10.951%\n",
      "Rate : 98 [27520/49000 (56%)]\tLoss: 2.306825\t Accuracy:10.979%\n",
      "Rate : 98 [28160/49000 (57%)]\tLoss: 2.312098\t Accuracy:11.024%\n",
      "Rate : 98 [28800/49000 (59%)]\tLoss: 2.286808\t Accuracy:11.026%\n",
      "Rate : 98 [29440/49000 (60%)]\tLoss: 2.300855\t Accuracy:11.078%\n",
      "Rate : 98 [30080/49000 (61%)]\tLoss: 2.310570\t Accuracy:11.042%\n",
      "Rate : 98 [30720/49000 (63%)]\tLoss: 2.293296\t Accuracy:11.076%\n",
      "Rate : 98 [31360/49000 (64%)]\tLoss: 2.309978\t Accuracy:11.082%\n",
      "Rate : 98 [32000/49000 (65%)]\tLoss: 2.310119\t Accuracy:11.064%\n",
      "Rate : 98 [32640/49000 (67%)]\tLoss: 2.311437\t Accuracy:11.104%\n",
      "Rate : 98 [33280/49000 (68%)]\tLoss: 2.304056\t Accuracy:11.116%\n",
      "Rate : 98 [33920/49000 (69%)]\tLoss: 2.297965\t Accuracy:11.113%\n",
      "Rate : 98 [34560/49000 (70%)]\tLoss: 2.304978\t Accuracy:11.069%\n",
      "Rate : 98 [35200/49000 (72%)]\tLoss: 2.314924\t Accuracy:11.047%\n",
      "Rate : 98 [35840/49000 (73%)]\tLoss: 2.303292\t Accuracy:11.056%\n",
      "Rate : 98 [36480/49000 (74%)]\tLoss: 2.286786\t Accuracy:11.073%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate : 98 [37120/49000 (76%)]\tLoss: 2.307814\t Accuracy:11.052%\n",
      "Rate : 98 [37760/49000 (77%)]\tLoss: 2.305999\t Accuracy:11.061%\n",
      "Rate : 98 [38400/49000 (78%)]\tLoss: 2.303353\t Accuracy:11.085%\n",
      "Rate : 98 [39040/49000 (80%)]\tLoss: 2.280845\t Accuracy:11.115%\n",
      "Rate : 98 [39680/49000 (81%)]\tLoss: 2.321677\t Accuracy:11.153%\n",
      "Rate : 98 [40320/49000 (82%)]\tLoss: 2.275624\t Accuracy:11.192%\n",
      "Rate : 98 [40960/49000 (84%)]\tLoss: 2.306455\t Accuracy:11.205%\n",
      "Rate : 98 [41600/49000 (85%)]\tLoss: 2.293033\t Accuracy:11.205%\n",
      "Rate : 98 [42240/49000 (86%)]\tLoss: 2.296624\t Accuracy:11.204%\n",
      "Rate : 98 [42880/49000 (87%)]\tLoss: 2.299313\t Accuracy:11.223%\n",
      "Rate : 98 [43520/49000 (89%)]\tLoss: 2.304070\t Accuracy:11.214%\n",
      "Rate : 98 [44160/49000 (90%)]\tLoss: 2.294827\t Accuracy:11.206%\n",
      "Rate : 98 [44800/49000 (91%)]\tLoss: 2.289444\t Accuracy:11.195%\n",
      "Rate : 98 [45440/49000 (93%)]\tLoss: 2.287299\t Accuracy:11.187%\n",
      "Rate : 98 [46080/49000 (94%)]\tLoss: 2.306902\t Accuracy:11.201%\n",
      "Rate : 98 [46720/49000 (95%)]\tLoss: 2.325536\t Accuracy:11.210%\n",
      "Rate : 98 [47360/49000 (97%)]\tLoss: 2.284349\t Accuracy:11.200%\n",
      "Rate : 98 [48000/49000 (98%)]\tLoss: 2.307369\t Accuracy:11.182%\n",
      "Rate : 98 [48640/49000 (99%)]\tLoss: 2.277199\t Accuracy:11.171%\n",
      "Rate : 99 [0/49000 (0%)]\tLoss: 2.272021\t Accuracy:31.250%\n",
      "Rate : 99 [640/49000 (1%)]\tLoss: 2.295977\t Accuracy:11.607%\n",
      "Rate : 99 [1280/49000 (3%)]\tLoss: 2.295156\t Accuracy:11.814%\n",
      "Rate : 99 [1920/49000 (4%)]\tLoss: 2.312089\t Accuracy:11.424%\n",
      "Rate : 99 [2560/49000 (5%)]\tLoss: 2.298208\t Accuracy:11.343%\n",
      "Rate : 99 [3200/49000 (7%)]\tLoss: 2.297303\t Accuracy:11.417%\n",
      "Rate : 99 [3840/49000 (8%)]\tLoss: 2.282784\t Accuracy:11.648%\n",
      "Rate : 99 [4480/49000 (9%)]\tLoss: 2.288851\t Accuracy:11.525%\n",
      "Rate : 99 [5120/49000 (10%)]\tLoss: 2.307856\t Accuracy:11.355%\n",
      "Rate : 99 [5760/49000 (12%)]\tLoss: 2.305142\t Accuracy:11.481%\n",
      "Rate : 99 [6400/49000 (13%)]\tLoss: 2.328002\t Accuracy:11.318%\n",
      "Rate : 99 [7040/49000 (14%)]\tLoss: 2.295932\t Accuracy:11.312%\n",
      "Rate : 99 [7680/49000 (16%)]\tLoss: 2.305224\t Accuracy:11.294%\n",
      "Rate : 99 [8320/49000 (17%)]\tLoss: 2.279811\t Accuracy:11.183%\n",
      "Rate : 99 [8960/49000 (18%)]\tLoss: 2.297108\t Accuracy:11.132%\n",
      "Rate : 99 [9600/49000 (20%)]\tLoss: 2.290864\t Accuracy:11.150%\n",
      "Rate : 99 [10240/49000 (21%)]\tLoss: 2.296200\t Accuracy:11.118%\n",
      "Rate : 99 [10880/49000 (22%)]\tLoss: 2.312933\t Accuracy:11.180%\n",
      "Rate : 99 [11520/49000 (23%)]\tLoss: 2.285542\t Accuracy:11.245%\n",
      "Rate : 99 [12160/49000 (25%)]\tLoss: 2.297688\t Accuracy:11.163%\n",
      "Rate : 99 [12800/49000 (26%)]\tLoss: 2.306729\t Accuracy:11.105%\n",
      "Rate : 99 [13440/49000 (27%)]\tLoss: 2.308212\t Accuracy:11.015%\n",
      "Rate : 99 [14080/49000 (29%)]\tLoss: 2.312065\t Accuracy:11.062%\n",
      "Rate : 99 [14720/49000 (30%)]\tLoss: 2.306345\t Accuracy:11.063%\n",
      "Rate : 99 [15360/49000 (31%)]\tLoss: 2.290181\t Accuracy:11.103%\n",
      "Rate : 99 [16000/49000 (33%)]\tLoss: 2.305504\t Accuracy:11.103%\n",
      "Rate : 99 [16640/49000 (34%)]\tLoss: 2.318476\t Accuracy:11.072%\n",
      "Rate : 99 [17280/49000 (35%)]\tLoss: 2.325995\t Accuracy:11.033%\n",
      "Rate : 99 [17920/49000 (37%)]\tLoss: 2.301450\t Accuracy:11.063%\n",
      "Rate : 99 [18560/49000 (38%)]\tLoss: 2.315917\t Accuracy:11.037%\n",
      "Rate : 99 [19200/49000 (39%)]\tLoss: 2.297111\t Accuracy:11.023%\n",
      "Rate : 99 [19840/49000 (40%)]\tLoss: 2.291300\t Accuracy:10.975%\n",
      "Rate : 99 [20480/49000 (42%)]\tLoss: 2.289527\t Accuracy:10.998%\n",
      "Rate : 99 [21120/49000 (43%)]\tLoss: 2.307495\t Accuracy:10.973%\n",
      "Rate : 99 [21760/49000 (44%)]\tLoss: 2.301750\t Accuracy:10.977%\n",
      "Rate : 99 [22400/49000 (46%)]\tLoss: 2.294565\t Accuracy:10.949%\n",
      "Rate : 99 [23040/49000 (47%)]\tLoss: 2.317032\t Accuracy:10.896%\n",
      "Rate : 99 [23680/49000 (48%)]\tLoss: 2.317148\t Accuracy:10.897%\n",
      "Rate : 99 [24320/49000 (50%)]\tLoss: 2.302363\t Accuracy:10.903%\n",
      "Rate : 99 [24960/49000 (51%)]\tLoss: 2.313952\t Accuracy:10.927%\n",
      "Rate : 99 [25600/49000 (52%)]\tLoss: 2.307069\t Accuracy:10.932%\n",
      "Rate : 99 [26240/49000 (54%)]\tLoss: 2.291541\t Accuracy:10.939%\n",
      "Rate : 99 [26880/49000 (55%)]\tLoss: 2.311476\t Accuracy:10.932%\n",
      "Rate : 99 [27520/49000 (56%)]\tLoss: 2.306823\t Accuracy:10.961%\n",
      "Rate : 99 [28160/49000 (57%)]\tLoss: 2.312102\t Accuracy:11.007%\n",
      "Rate : 99 [28800/49000 (59%)]\tLoss: 2.286814\t Accuracy:11.009%\n",
      "Rate : 99 [29440/49000 (60%)]\tLoss: 2.300847\t Accuracy:11.061%\n",
      "Rate : 99 [30080/49000 (61%)]\tLoss: 2.310576\t Accuracy:11.026%\n",
      "Rate : 99 [30720/49000 (63%)]\tLoss: 2.293298\t Accuracy:11.059%\n",
      "Rate : 99 [31360/49000 (64%)]\tLoss: 2.309980\t Accuracy:11.067%\n",
      "Rate : 99 [32000/49000 (65%)]\tLoss: 2.310118\t Accuracy:11.048%\n",
      "Rate : 99 [32640/49000 (67%)]\tLoss: 2.311424\t Accuracy:11.089%\n",
      "Rate : 99 [33280/49000 (68%)]\tLoss: 2.304064\t Accuracy:11.101%\n",
      "Rate : 99 [33920/49000 (69%)]\tLoss: 2.297987\t Accuracy:11.098%\n",
      "Rate : 99 [34560/49000 (70%)]\tLoss: 2.304982\t Accuracy:11.055%\n",
      "Rate : 99 [35200/49000 (72%)]\tLoss: 2.314943\t Accuracy:11.033%\n",
      "Rate : 99 [35840/49000 (73%)]\tLoss: 2.303282\t Accuracy:11.042%\n",
      "Rate : 99 [36480/49000 (74%)]\tLoss: 2.286792\t Accuracy:11.059%\n",
      "Rate : 99 [37120/49000 (76%)]\tLoss: 2.307813\t Accuracy:11.038%\n",
      "Rate : 99 [37760/49000 (77%)]\tLoss: 2.306001\t Accuracy:11.047%\n",
      "Rate : 99 [38400/49000 (78%)]\tLoss: 2.303348\t Accuracy:11.072%\n",
      "Rate : 99 [39040/49000 (80%)]\tLoss: 2.280850\t Accuracy:11.103%\n",
      "Rate : 99 [39680/49000 (81%)]\tLoss: 2.321689\t Accuracy:11.140%\n",
      "Rate : 99 [40320/49000 (82%)]\tLoss: 2.275640\t Accuracy:11.179%\n",
      "Rate : 99 [40960/49000 (84%)]\tLoss: 2.306458\t Accuracy:11.192%\n",
      "Rate : 99 [41600/49000 (85%)]\tLoss: 2.293028\t Accuracy:11.193%\n",
      "Rate : 99 [42240/49000 (86%)]\tLoss: 2.296629\t Accuracy:11.192%\n",
      "Rate : 99 [42880/49000 (87%)]\tLoss: 2.299307\t Accuracy:11.211%\n",
      "Rate : 99 [43520/49000 (89%)]\tLoss: 2.304065\t Accuracy:11.203%\n",
      "Rate : 99 [44160/49000 (90%)]\tLoss: 2.294830\t Accuracy:11.194%\n",
      "Rate : 99 [44800/49000 (91%)]\tLoss: 2.289443\t Accuracy:11.184%\n",
      "Rate : 99 [45440/49000 (93%)]\tLoss: 2.287300\t Accuracy:11.176%\n",
      "Rate : 99 [46080/49000 (94%)]\tLoss: 2.306910\t Accuracy:11.190%\n",
      "Rate : 99 [46720/49000 (95%)]\tLoss: 2.325541\t Accuracy:11.200%\n",
      "Rate : 99 [47360/49000 (97%)]\tLoss: 2.284351\t Accuracy:11.190%\n",
      "Rate : 99 [48000/49000 (98%)]\tLoss: 2.307364\t Accuracy:11.172%\n",
      "Rate : 99 [48640/49000 (99%)]\tLoss: 2.277200\t Accuracy:11.160%\n",
      "Rate : 99 [0/49000 (0%)]\tLoss: 2.272016\t Accuracy:31.250%\n",
      "Rate : 99 [640/49000 (1%)]\tLoss: 2.295985\t Accuracy:11.607%\n",
      "Rate : 99 [1280/49000 (3%)]\tLoss: 2.295166\t Accuracy:11.814%\n",
      "Rate : 99 [1920/49000 (4%)]\tLoss: 2.312088\t Accuracy:11.424%\n",
      "Rate : 99 [2560/49000 (5%)]\tLoss: 2.298207\t Accuracy:11.343%\n",
      "Rate : 99 [3200/49000 (7%)]\tLoss: 2.297313\t Accuracy:11.417%\n",
      "Rate : 99 [3840/49000 (8%)]\tLoss: 2.282778\t Accuracy:11.648%\n",
      "Rate : 99 [4480/49000 (9%)]\tLoss: 2.288839\t Accuracy:11.525%\n",
      "Rate : 99 [5120/49000 (10%)]\tLoss: 2.307853\t Accuracy:11.355%\n",
      "Rate : 99 [5760/49000 (12%)]\tLoss: 2.305144\t Accuracy:11.481%\n",
      "Rate : 99 [6400/49000 (13%)]\tLoss: 2.327992\t Accuracy:11.318%\n",
      "Rate : 99 [7040/49000 (14%)]\tLoss: 2.295943\t Accuracy:11.312%\n",
      "Rate : 99 [7680/49000 (16%)]\tLoss: 2.305224\t Accuracy:11.294%\n",
      "Rate : 99 [8320/49000 (17%)]\tLoss: 2.279803\t Accuracy:11.183%\n",
      "Rate : 99 [8960/49000 (18%)]\tLoss: 2.297106\t Accuracy:11.132%\n",
      "Rate : 99 [9600/49000 (20%)]\tLoss: 2.290872\t Accuracy:11.150%\n",
      "Rate : 99 [10240/49000 (21%)]\tLoss: 2.296199\t Accuracy:11.118%\n",
      "Rate : 99 [10880/49000 (22%)]\tLoss: 2.312929\t Accuracy:11.180%\n",
      "Rate : 99 [11520/49000 (23%)]\tLoss: 2.285541\t Accuracy:11.245%\n",
      "Rate : 99 [12160/49000 (25%)]\tLoss: 2.297688\t Accuracy:11.163%\n",
      "Rate : 99 [12800/49000 (26%)]\tLoss: 2.306730\t Accuracy:11.105%\n",
      "Rate : 99 [13440/49000 (27%)]\tLoss: 2.308210\t Accuracy:11.015%\n",
      "Rate : 99 [14080/49000 (29%)]\tLoss: 2.312069\t Accuracy:11.062%\n",
      "Rate : 99 [14720/49000 (30%)]\tLoss: 2.306350\t Accuracy:11.063%\n",
      "Rate : 99 [15360/49000 (31%)]\tLoss: 2.290180\t Accuracy:11.103%\n",
      "Rate : 99 [16000/49000 (33%)]\tLoss: 2.305499\t Accuracy:11.103%\n",
      "Rate : 99 [16640/49000 (34%)]\tLoss: 2.318484\t Accuracy:11.072%\n",
      "Rate : 99 [17280/49000 (35%)]\tLoss: 2.326007\t Accuracy:11.033%\n",
      "Rate : 99 [17920/49000 (37%)]\tLoss: 2.301450\t Accuracy:11.063%\n",
      "Rate : 99 [18560/49000 (38%)]\tLoss: 2.315918\t Accuracy:11.037%\n",
      "Rate : 99 [19200/49000 (39%)]\tLoss: 2.297106\t Accuracy:11.023%\n",
      "Rate : 99 [19840/49000 (40%)]\tLoss: 2.291288\t Accuracy:10.975%\n",
      "Rate : 99 [20480/49000 (42%)]\tLoss: 2.289512\t Accuracy:10.998%\n",
      "Rate : 99 [21120/49000 (43%)]\tLoss: 2.307513\t Accuracy:10.973%\n",
      "Rate : 99 [21760/49000 (44%)]\tLoss: 2.301752\t Accuracy:10.977%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate : 99 [22400/49000 (46%)]\tLoss: 2.294560\t Accuracy:10.949%\n",
      "Rate : 99 [23040/49000 (47%)]\tLoss: 2.317025\t Accuracy:10.896%\n",
      "Rate : 99 [23680/49000 (48%)]\tLoss: 2.317147\t Accuracy:10.897%\n",
      "Rate : 99 [24320/49000 (50%)]\tLoss: 2.302361\t Accuracy:10.903%\n",
      "Rate : 99 [24960/49000 (51%)]\tLoss: 2.313948\t Accuracy:10.927%\n",
      "Rate : 99 [25600/49000 (52%)]\tLoss: 2.307066\t Accuracy:10.932%\n",
      "Rate : 99 [26240/49000 (54%)]\tLoss: 2.291538\t Accuracy:10.939%\n",
      "Rate : 99 [26880/49000 (55%)]\tLoss: 2.311474\t Accuracy:10.932%\n",
      "Rate : 99 [27520/49000 (56%)]\tLoss: 2.306821\t Accuracy:10.961%\n",
      "Rate : 99 [28160/49000 (57%)]\tLoss: 2.312104\t Accuracy:11.007%\n",
      "Rate : 99 [28800/49000 (59%)]\tLoss: 2.286815\t Accuracy:11.009%\n",
      "Rate : 99 [29440/49000 (60%)]\tLoss: 2.300845\t Accuracy:11.061%\n",
      "Rate : 99 [30080/49000 (61%)]\tLoss: 2.310578\t Accuracy:11.026%\n",
      "Rate : 99 [30720/49000 (63%)]\tLoss: 2.293298\t Accuracy:11.059%\n",
      "Rate : 99 [31360/49000 (64%)]\tLoss: 2.309980\t Accuracy:11.067%\n",
      "Rate : 99 [32000/49000 (65%)]\tLoss: 2.310118\t Accuracy:11.048%\n",
      "Rate : 99 [32640/49000 (67%)]\tLoss: 2.311422\t Accuracy:11.089%\n",
      "Rate : 99 [33280/49000 (68%)]\tLoss: 2.304066\t Accuracy:11.101%\n",
      "Rate : 99 [33920/49000 (69%)]\tLoss: 2.297991\t Accuracy:11.098%\n",
      "Rate : 99 [34560/49000 (70%)]\tLoss: 2.304983\t Accuracy:11.055%\n",
      "Rate : 99 [35200/49000 (72%)]\tLoss: 2.314948\t Accuracy:11.033%\n",
      "Rate : 99 [35840/49000 (73%)]\tLoss: 2.303281\t Accuracy:11.042%\n",
      "Rate : 99 [36480/49000 (74%)]\tLoss: 2.286793\t Accuracy:11.059%\n",
      "Rate : 99 [37120/49000 (76%)]\tLoss: 2.307813\t Accuracy:11.038%\n",
      "Rate : 99 [37760/49000 (77%)]\tLoss: 2.306002\t Accuracy:11.047%\n",
      "Rate : 99 [38400/49000 (78%)]\tLoss: 2.303347\t Accuracy:11.072%\n",
      "Rate : 99 [39040/49000 (80%)]\tLoss: 2.280851\t Accuracy:11.103%\n",
      "Rate : 99 [39680/49000 (81%)]\tLoss: 2.321692\t Accuracy:11.140%\n",
      "Rate : 99 [40320/49000 (82%)]\tLoss: 2.275643\t Accuracy:11.179%\n",
      "Rate : 99 [40960/49000 (84%)]\tLoss: 2.306459\t Accuracy:11.192%\n",
      "Rate : 99 [41600/49000 (85%)]\tLoss: 2.293028\t Accuracy:11.193%\n",
      "Rate : 99 [42240/49000 (86%)]\tLoss: 2.296629\t Accuracy:11.192%\n",
      "Rate : 99 [42880/49000 (87%)]\tLoss: 2.299305\t Accuracy:11.211%\n",
      "Rate : 99 [43520/49000 (89%)]\tLoss: 2.304065\t Accuracy:11.203%\n",
      "Rate : 99 [44160/49000 (90%)]\tLoss: 2.294830\t Accuracy:11.194%\n",
      "Rate : 99 [44800/49000 (91%)]\tLoss: 2.289443\t Accuracy:11.184%\n",
      "Rate : 99 [45440/49000 (93%)]\tLoss: 2.287300\t Accuracy:11.176%\n",
      "Rate : 99 [46080/49000 (94%)]\tLoss: 2.306911\t Accuracy:11.190%\n",
      "Rate : 99 [46720/49000 (95%)]\tLoss: 2.325542\t Accuracy:11.200%\n",
      "Rate : 99 [47360/49000 (97%)]\tLoss: 2.284352\t Accuracy:11.190%\n",
      "Rate : 99 [48000/49000 (98%)]\tLoss: 2.307363\t Accuracy:11.172%\n",
      "Rate : 99 [48640/49000 (99%)]\tLoss: 2.277199\t Accuracy:11.160%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "pruned_model_99 = smart_prune_shed(pruned_model_90, train_loader, [\n",
    "    (90, 2),\n",
    "    (92, 2),\n",
    "    (94, 2),\n",
    "    (95, 2),\n",
    "    (96, 2),\n",
    "    (97, 2),\n",
    "    (98, 2),\n",
    "    (99, 2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.113% \n"
     ]
    }
   ],
   "source": [
    "evaluate(pruned_model_99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2220"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_pruned_weights(pruned_model_99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что метод все таки имеет свои ограничения. По логу видно, что где-то в районе 94% мы видимо задели какой-то очень важный участок сети, после удаления которого она уже не смогла восстановится.\n",
    "\n",
    "Однако результат в 90% - это тоже вполне неплохо!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Готовые реализации\n",
    "\n",
    "Сама техника достаточно популярна и часто имеет уже готовые реализации. В Pytorch имеется отдельный модуль для проведения прореживания сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PytorchPrunedMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PytorchPrunedMLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(784,250)\n",
    "        self.linear2 = nn.Linear(250,100)\n",
    "        self.linear3 = nn.Linear(100,10)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        X = F.relu(self.linear1(X))\n",
    "        X = F.relu(self.linear2(X))\n",
    "        X = self.linear3(X)\n",
    "        return F.log_softmax(X, dim=1)\n",
    "    \n",
    "    def prune(self, rate):\n",
    "        # Используем l1_unstructured вместо нашего подхода\n",
    "        # unstructured говорит о том, что нет ограничений на удаляемые веса\n",
    "        # l1 говорит о том, что нужно смотреть на модуль веса\n",
    "        prune.l1_unstructured(self.linear1, 'weight', amount=rate)\n",
    "        prune.l1_unstructured(self.linear2, 'weight', amount=rate)\n",
    "        prune.l1_unstructured(self.linear3, 'weight', amount=rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 [0/49000 (0%)]\tLoss: 8.752557\t Accuracy:9.375%\n",
      "Epoch : 0 [1600/49000 (3%)]\tLoss: 0.328137\t Accuracy:69.179%\n",
      "Epoch : 0 [3200/49000 (7%)]\tLoss: 0.348055\t Accuracy:76.825%\n",
      "Epoch : 0 [4800/49000 (10%)]\tLoss: 0.260353\t Accuracy:79.988%\n",
      "Epoch : 0 [6400/49000 (13%)]\tLoss: 0.274885\t Accuracy:82.478%\n",
      "Epoch : 0 [8000/49000 (16%)]\tLoss: 0.284318\t Accuracy:83.877%\n",
      "Epoch : 0 [9600/49000 (20%)]\tLoss: 0.142767\t Accuracy:84.759%\n",
      "Epoch : 0 [11200/49000 (23%)]\tLoss: 0.533794\t Accuracy:85.684%\n",
      "Epoch : 0 [12800/49000 (26%)]\tLoss: 0.383269\t Accuracy:86.386%\n",
      "Epoch : 0 [14400/49000 (29%)]\tLoss: 0.217726\t Accuracy:86.973%\n",
      "Epoch : 0 [16000/49000 (33%)]\tLoss: 0.247426\t Accuracy:87.506%\n",
      "Epoch : 0 [17600/49000 (36%)]\tLoss: 0.295171\t Accuracy:87.903%\n",
      "Epoch : 0 [19200/49000 (39%)]\tLoss: 0.768353\t Accuracy:88.228%\n",
      "Epoch : 0 [20800/49000 (42%)]\tLoss: 0.029675\t Accuracy:88.609%\n",
      "Epoch : 0 [22400/49000 (46%)]\tLoss: 0.423048\t Accuracy:88.886%\n",
      "Epoch : 0 [24000/49000 (49%)]\tLoss: 0.439481\t Accuracy:89.160%\n",
      "Epoch : 0 [25600/49000 (52%)]\tLoss: 0.070248\t Accuracy:89.310%\n",
      "Epoch : 0 [27200/49000 (55%)]\tLoss: 0.486616\t Accuracy:89.472%\n",
      "Epoch : 0 [28800/49000 (59%)]\tLoss: 0.065676\t Accuracy:89.720%\n",
      "Epoch : 0 [30400/49000 (62%)]\tLoss: 0.271769\t Accuracy:89.905%\n",
      "Epoch : 0 [32000/49000 (65%)]\tLoss: 0.097908\t Accuracy:90.122%\n",
      "Epoch : 0 [33600/49000 (69%)]\tLoss: 0.354305\t Accuracy:90.316%\n",
      "Epoch : 0 [35200/49000 (72%)]\tLoss: 0.064940\t Accuracy:90.458%\n",
      "Epoch : 0 [36800/49000 (75%)]\tLoss: 0.327679\t Accuracy:90.620%\n",
      "Epoch : 0 [38400/49000 (78%)]\tLoss: 0.144602\t Accuracy:90.758%\n",
      "Epoch : 0 [40000/49000 (82%)]\tLoss: 0.118381\t Accuracy:90.912%\n",
      "Epoch : 0 [41600/49000 (85%)]\tLoss: 0.199216\t Accuracy:91.057%\n",
      "Epoch : 0 [43200/49000 (88%)]\tLoss: 0.189860\t Accuracy:91.157%\n",
      "Epoch : 0 [44800/49000 (91%)]\tLoss: 0.413594\t Accuracy:91.263%\n",
      "Epoch : 0 [46400/49000 (95%)]\tLoss: 0.041479\t Accuracy:91.387%\n",
      "Epoch : 0 [48000/49000 (98%)]\tLoss: 0.092959\t Accuracy:91.470%\n",
      "Epoch : 1 [0/49000 (0%)]\tLoss: 0.266363\t Accuracy:93.750%\n",
      "Epoch : 1 [1600/49000 (3%)]\tLoss: 0.055156\t Accuracy:95.098%\n",
      "Epoch : 1 [3200/49000 (7%)]\tLoss: 0.137875\t Accuracy:94.833%\n",
      "Epoch : 1 [4800/49000 (10%)]\tLoss: 0.111777\t Accuracy:95.137%\n",
      "Epoch : 1 [6400/49000 (13%)]\tLoss: 0.073176\t Accuracy:95.165%\n",
      "Epoch : 1 [8000/49000 (16%)]\tLoss: 0.270996\t Accuracy:95.107%\n",
      "Epoch : 1 [9600/49000 (20%)]\tLoss: 0.057180\t Accuracy:95.017%\n",
      "Epoch : 1 [11200/49000 (23%)]\tLoss: 0.232776\t Accuracy:94.961%\n",
      "Epoch : 1 [12800/49000 (26%)]\tLoss: 0.398035\t Accuracy:95.036%\n",
      "Epoch : 1 [14400/49000 (29%)]\tLoss: 0.192055\t Accuracy:94.956%\n",
      "Epoch : 1 [16000/49000 (33%)]\tLoss: 0.071758\t Accuracy:94.948%\n",
      "Epoch : 1 [17600/49000 (36%)]\tLoss: 0.023633\t Accuracy:94.998%\n",
      "Epoch : 1 [19200/49000 (39%)]\tLoss: 0.323150\t Accuracy:94.998%\n",
      "Epoch : 1 [20800/49000 (42%)]\tLoss: 0.050068\t Accuracy:95.008%\n",
      "Epoch : 1 [22400/49000 (46%)]\tLoss: 0.120434\t Accuracy:94.985%\n",
      "Epoch : 1 [24000/49000 (49%)]\tLoss: 0.077826\t Accuracy:95.002%\n",
      "Epoch : 1 [25600/49000 (52%)]\tLoss: 0.059897\t Accuracy:94.979%\n",
      "Epoch : 1 [27200/49000 (55%)]\tLoss: 0.461971\t Accuracy:94.954%\n",
      "Epoch : 1 [28800/49000 (59%)]\tLoss: 0.231923\t Accuracy:94.981%\n",
      "Epoch : 1 [30400/49000 (62%)]\tLoss: 0.124759\t Accuracy:94.999%\n",
      "Epoch : 1 [32000/49000 (65%)]\tLoss: 0.175783\t Accuracy:95.052%\n",
      "Epoch : 1 [33600/49000 (69%)]\tLoss: 0.133314\t Accuracy:95.091%\n",
      "Epoch : 1 [35200/49000 (72%)]\tLoss: 0.115610\t Accuracy:95.093%\n",
      "Epoch : 1 [36800/49000 (75%)]\tLoss: 0.016148\t Accuracy:95.140%\n",
      "Epoch : 1 [38400/49000 (78%)]\tLoss: 0.090979\t Accuracy:95.163%\n",
      "Epoch : 1 [40000/49000 (82%)]\tLoss: 0.209329\t Accuracy:95.216%\n",
      "Epoch : 1 [41600/49000 (85%)]\tLoss: 0.198735\t Accuracy:95.294%\n",
      "Epoch : 1 [43200/49000 (88%)]\tLoss: 0.124494\t Accuracy:95.323%\n",
      "Epoch : 1 [44800/49000 (91%)]\tLoss: 0.288605\t Accuracy:95.336%\n",
      "Epoch : 1 [46400/49000 (95%)]\tLoss: 0.030877\t Accuracy:95.387%\n",
      "Epoch : 1 [48000/49000 (98%)]\tLoss: 0.017310\t Accuracy:95.397%\n",
      "Epoch : 2 [0/49000 (0%)]\tLoss: 0.137043\t Accuracy:96.875%\n",
      "Epoch : 2 [1600/49000 (3%)]\tLoss: 0.110494\t Accuracy:96.507%\n",
      "Epoch : 2 [3200/49000 (7%)]\tLoss: 0.049812\t Accuracy:96.689%\n",
      "Epoch : 2 [4800/49000 (10%)]\tLoss: 0.209668\t Accuracy:96.233%\n",
      "Epoch : 2 [6400/49000 (13%)]\tLoss: 0.052262\t Accuracy:96.362%\n",
      "Epoch : 2 [8000/49000 (16%)]\tLoss: 0.042578\t Accuracy:96.240%\n",
      "Epoch : 2 [9600/49000 (20%)]\tLoss: 0.035879\t Accuracy:96.179%\n",
      "Epoch : 2 [11200/49000 (23%)]\tLoss: 0.273037\t Accuracy:96.065%\n",
      "Epoch : 2 [12800/49000 (26%)]\tLoss: 0.306209\t Accuracy:96.057%\n",
      "Epoch : 2 [14400/49000 (29%)]\tLoss: 0.176272\t Accuracy:96.113%\n",
      "Epoch : 2 [16000/49000 (33%)]\tLoss: 0.086812\t Accuracy:96.145%\n",
      "Epoch : 2 [17600/49000 (36%)]\tLoss: 0.121697\t Accuracy:96.143%\n",
      "Epoch : 2 [19200/49000 (39%)]\tLoss: 0.439350\t Accuracy:96.111%\n",
      "Epoch : 2 [20800/49000 (42%)]\tLoss: 0.010299\t Accuracy:96.030%\n",
      "Epoch : 2 [22400/49000 (46%)]\tLoss: 0.177998\t Accuracy:96.010%\n",
      "Epoch : 2 [24000/49000 (49%)]\tLoss: 0.094246\t Accuracy:95.947%\n",
      "Epoch : 2 [25600/49000 (52%)]\tLoss: 0.092448\t Accuracy:95.970%\n",
      "Epoch : 2 [27200/49000 (55%)]\tLoss: 0.214489\t Accuracy:95.968%\n",
      "Epoch : 2 [28800/49000 (59%)]\tLoss: 0.104497\t Accuracy:96.011%\n",
      "Epoch : 2 [30400/49000 (62%)]\tLoss: 0.237579\t Accuracy:96.017%\n",
      "Epoch : 2 [32000/49000 (65%)]\tLoss: 0.011109\t Accuracy:96.088%\n",
      "Epoch : 2 [33600/49000 (69%)]\tLoss: 0.127554\t Accuracy:96.111%\n",
      "Epoch : 2 [35200/49000 (72%)]\tLoss: 0.046794\t Accuracy:96.094%\n",
      "Epoch : 2 [36800/49000 (75%)]\tLoss: 0.107856\t Accuracy:96.112%\n",
      "Epoch : 2 [38400/49000 (78%)]\tLoss: 0.158532\t Accuracy:96.123%\n",
      "Epoch : 2 [40000/49000 (82%)]\tLoss: 0.256969\t Accuracy:96.123%\n",
      "Epoch : 2 [41600/49000 (85%)]\tLoss: 0.087037\t Accuracy:96.154%\n",
      "Epoch : 2 [43200/49000 (88%)]\tLoss: 0.098182\t Accuracy:96.179%\n",
      "Epoch : 2 [44800/49000 (91%)]\tLoss: 0.129102\t Accuracy:96.179%\n",
      "Epoch : 2 [46400/49000 (95%)]\tLoss: 0.010749\t Accuracy:96.197%\n",
      "Epoch : 2 [48000/49000 (98%)]\tLoss: 0.081919\t Accuracy:96.190%\n",
      "Epoch : 3 [0/49000 (0%)]\tLoss: 0.065258\t Accuracy:96.875%\n",
      "Epoch : 3 [1600/49000 (3%)]\tLoss: 0.067349\t Accuracy:96.814%\n",
      "Epoch : 3 [3200/49000 (7%)]\tLoss: 0.053883\t Accuracy:96.627%\n",
      "Epoch : 3 [4800/49000 (10%)]\tLoss: 0.069199\t Accuracy:96.523%\n",
      "Epoch : 3 [6400/49000 (13%)]\tLoss: 0.002654\t Accuracy:96.797%\n",
      "Epoch : 3 [8000/49000 (16%)]\tLoss: 0.165492\t Accuracy:96.489%\n",
      "Epoch : 3 [9600/49000 (20%)]\tLoss: 0.053892\t Accuracy:96.470%\n",
      "Epoch : 3 [11200/49000 (23%)]\tLoss: 0.182808\t Accuracy:96.554%\n",
      "Epoch : 3 [12800/49000 (26%)]\tLoss: 0.163896\t Accuracy:96.594%\n",
      "Epoch : 3 [14400/49000 (29%)]\tLoss: 0.066412\t Accuracy:96.653%\n",
      "Epoch : 3 [16000/49000 (33%)]\tLoss: 0.200641\t Accuracy:96.675%\n",
      "Epoch : 3 [17600/49000 (36%)]\tLoss: 0.038016\t Accuracy:96.716%\n",
      "Epoch : 3 [19200/49000 (39%)]\tLoss: 0.226017\t Accuracy:96.703%\n",
      "Epoch : 3 [20800/49000 (42%)]\tLoss: 0.010349\t Accuracy:96.712%\n",
      "Epoch : 3 [22400/49000 (46%)]\tLoss: 0.367582\t Accuracy:96.719%\n",
      "Epoch : 3 [24000/49000 (49%)]\tLoss: 0.111751\t Accuracy:96.684%\n",
      "Epoch : 3 [25600/49000 (52%)]\tLoss: 0.170642\t Accuracy:96.637%\n",
      "Epoch : 3 [27200/49000 (55%)]\tLoss: 1.033486\t Accuracy:96.614%\n",
      "Epoch : 3 [28800/49000 (59%)]\tLoss: 0.161623\t Accuracy:96.639%\n",
      "Epoch : 3 [30400/49000 (62%)]\tLoss: 0.058980\t Accuracy:96.619%\n",
      "Epoch : 3 [32000/49000 (65%)]\tLoss: 0.058076\t Accuracy:96.641%\n",
      "Epoch : 3 [33600/49000 (69%)]\tLoss: 0.204643\t Accuracy:96.670%\n",
      "Epoch : 3 [35200/49000 (72%)]\tLoss: 0.064739\t Accuracy:96.671%\n",
      "Epoch : 3 [36800/49000 (75%)]\tLoss: 0.022647\t Accuracy:96.669%\n",
      "Epoch : 3 [38400/49000 (78%)]\tLoss: 0.071490\t Accuracy:96.675%\n",
      "Epoch : 3 [40000/49000 (82%)]\tLoss: 0.087144\t Accuracy:96.635%\n",
      "Epoch : 3 [41600/49000 (85%)]\tLoss: 0.181756\t Accuracy:96.664%\n",
      "Epoch : 3 [43200/49000 (88%)]\tLoss: 0.005839\t Accuracy:96.706%\n",
      "Epoch : 3 [44800/49000 (91%)]\tLoss: 0.141244\t Accuracy:96.737%\n",
      "Epoch : 3 [46400/49000 (95%)]\tLoss: 0.026308\t Accuracy:96.776%\n",
      "Epoch : 3 [48000/49000 (98%)]\tLoss: 0.012498\t Accuracy:96.777%\n",
      "Epoch : 4 [0/49000 (0%)]\tLoss: 0.053175\t Accuracy:100.000%\n",
      "Epoch : 4 [1600/49000 (3%)]\tLoss: 0.079510\t Accuracy:96.998%\n",
      "Epoch : 4 [3200/49000 (7%)]\tLoss: 0.179215\t Accuracy:96.875%\n",
      "Epoch : 4 [4800/49000 (10%)]\tLoss: 0.160457\t Accuracy:96.978%\n",
      "Epoch : 4 [6400/49000 (13%)]\tLoss: 0.087734\t Accuracy:97.093%\n",
      "Epoch : 4 [8000/49000 (16%)]\tLoss: 0.067092\t Accuracy:97.099%\n",
      "Epoch : 4 [9600/49000 (20%)]\tLoss: 0.011775\t Accuracy:96.989%\n",
      "Epoch : 4 [11200/49000 (23%)]\tLoss: 0.096461\t Accuracy:96.902%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 4 [12800/49000 (26%)]\tLoss: 0.186442\t Accuracy:96.930%\n",
      "Epoch : 4 [14400/49000 (29%)]\tLoss: 0.031756\t Accuracy:96.889%\n",
      "Epoch : 4 [16000/49000 (33%)]\tLoss: 0.049820\t Accuracy:96.956%\n",
      "Epoch : 4 [17600/49000 (36%)]\tLoss: 0.069461\t Accuracy:96.943%\n",
      "Epoch : 4 [19200/49000 (39%)]\tLoss: 0.364155\t Accuracy:96.948%\n",
      "Epoch : 4 [20800/49000 (42%)]\tLoss: 0.004673\t Accuracy:96.918%\n",
      "Epoch : 4 [22400/49000 (46%)]\tLoss: 0.272254\t Accuracy:96.897%\n",
      "Epoch : 4 [24000/49000 (49%)]\tLoss: 0.197898\t Accuracy:96.896%\n",
      "Epoch : 4 [25600/49000 (52%)]\tLoss: 0.007085\t Accuracy:96.871%\n",
      "Epoch : 4 [27200/49000 (55%)]\tLoss: 0.201744\t Accuracy:96.915%\n",
      "Epoch : 4 [28800/49000 (59%)]\tLoss: 0.098582\t Accuracy:96.927%\n",
      "Epoch : 4 [30400/49000 (62%)]\tLoss: 0.009788\t Accuracy:96.918%\n",
      "Epoch : 4 [32000/49000 (65%)]\tLoss: 0.016327\t Accuracy:96.937%\n",
      "Epoch : 4 [33600/49000 (69%)]\tLoss: 0.130120\t Accuracy:96.967%\n",
      "Epoch : 4 [35200/49000 (72%)]\tLoss: 0.122135\t Accuracy:96.969%\n",
      "Epoch : 4 [36800/49000 (75%)]\tLoss: 0.119698\t Accuracy:96.997%\n",
      "Epoch : 4 [38400/49000 (78%)]\tLoss: 0.125789\t Accuracy:97.018%\n",
      "Epoch : 4 [40000/49000 (82%)]\tLoss: 0.087052\t Accuracy:97.027%\n",
      "Epoch : 4 [41600/49000 (85%)]\tLoss: 0.008042\t Accuracy:97.036%\n",
      "Epoch : 4 [43200/49000 (88%)]\tLoss: 0.101792\t Accuracy:97.048%\n",
      "Epoch : 4 [44800/49000 (91%)]\tLoss: 0.027414\t Accuracy:97.051%\n",
      "Epoch : 4 [46400/49000 (95%)]\tLoss: 0.010077\t Accuracy:97.077%\n",
      "Epoch : 4 [48000/49000 (98%)]\tLoss: 0.005471\t Accuracy:97.098%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "ppmlp = PytorchPrunedMLP()\n",
    "fit(ppmlp, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.960% \n"
     ]
    }
   ],
   "source": [
    "evaluate(ppmlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ppmlp.prune(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.953% \n"
     ]
    }
   ],
   "source": [
    "evaluate(ppmlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calc_pytorch_weights(model):\n",
    "    result = 0\n",
    "    for layer in model.children():\n",
    "        if hasattr(layer, 'weight_mask'):\n",
    "            result += int(torch.sum(layer.weight_mask.reshape(-1)).item())\n",
    "        else:\n",
    "            result += len(layer.weight.reshape(-1))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111000"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_pytorch_weights(ppmlp) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Точно таким же образом мы только что выкинули 50% самых слабый весов из сети.\n",
    "\n",
    "### Групповое (структурированное) прореживание\n",
    "\n",
    "В библиотеке реализованы также и более продвинутые версии этого алгоритма. Например мы можем делать более структурированное прореживание, удаляя не единичные связи, а целиком нейроны из сети. \n",
    "\n",
    "Для того, чтобы понять, насколько тот или иной нейрон важен для работы сети, будем смотреть на все веса, связанные с ним. Если веса значительно отличаются от нуля, значит нейрон важный, если близки к нулю - значит скорее всего его можно удалить.\n",
    "\n",
    "Понимать, насколько группа нейронов близка к нулю можно разными способами. Наиболее популярный - L-нормы. Так например при L1 мы будем смотреть на сумму по модулю все весов для нейрона, а при L2 - на корень из суммы квадратов весов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class StructuredPrunedMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StructuredPrunedMLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(784,250)\n",
    "        self.linear2 = nn.Linear(250,100)\n",
    "        self.linear3 = nn.Linear(100,10)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        X = F.relu(self.linear1(X))\n",
    "        X = F.relu(self.linear2(X))\n",
    "        X = self.linear3(X)\n",
    "        return F.log_softmax(X, dim=1)\n",
    "    \n",
    "    def prune(self, rate):\n",
    "        # Используем ln_structured для удаления нейронов целиком\n",
    "        # Для оценивания значимости нейрона будем использовать L2, поэтому указываем n=2\n",
    "        # Указываем dim=1 - это укажет, как именно нужно групировать веса. Для dim=1 - это группировка по нейронам\n",
    "        prune.ln_structured(self.linear1, 'weight', amount=rate, n=2, dim=1)\n",
    "        prune.ln_structured(self.linear2, 'weight', amount=rate, n=2, dim=1)\n",
    "        # В последнем слое удалять нейроны нельзя, потому как они отвечают за ответ сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 [0/49000 (0%)]\tLoss: 8.752557\t Accuracy:9.375%\n",
      "Epoch : 0 [1600/49000 (3%)]\tLoss: 0.328137\t Accuracy:69.179%\n",
      "Epoch : 0 [3200/49000 (7%)]\tLoss: 0.348055\t Accuracy:76.825%\n",
      "Epoch : 0 [4800/49000 (10%)]\tLoss: 0.260353\t Accuracy:79.988%\n",
      "Epoch : 0 [6400/49000 (13%)]\tLoss: 0.274885\t Accuracy:82.478%\n",
      "Epoch : 0 [8000/49000 (16%)]\tLoss: 0.284318\t Accuracy:83.877%\n",
      "Epoch : 0 [9600/49000 (20%)]\tLoss: 0.142767\t Accuracy:84.759%\n",
      "Epoch : 0 [11200/49000 (23%)]\tLoss: 0.533794\t Accuracy:85.684%\n",
      "Epoch : 0 [12800/49000 (26%)]\tLoss: 0.383269\t Accuracy:86.386%\n",
      "Epoch : 0 [14400/49000 (29%)]\tLoss: 0.217726\t Accuracy:86.973%\n",
      "Epoch : 0 [16000/49000 (33%)]\tLoss: 0.247426\t Accuracy:87.506%\n",
      "Epoch : 0 [17600/49000 (36%)]\tLoss: 0.295171\t Accuracy:87.903%\n",
      "Epoch : 0 [19200/49000 (39%)]\tLoss: 0.768353\t Accuracy:88.228%\n",
      "Epoch : 0 [20800/49000 (42%)]\tLoss: 0.029675\t Accuracy:88.609%\n",
      "Epoch : 0 [22400/49000 (46%)]\tLoss: 0.423048\t Accuracy:88.886%\n",
      "Epoch : 0 [24000/49000 (49%)]\tLoss: 0.439481\t Accuracy:89.160%\n",
      "Epoch : 0 [25600/49000 (52%)]\tLoss: 0.070248\t Accuracy:89.310%\n",
      "Epoch : 0 [27200/49000 (55%)]\tLoss: 0.486616\t Accuracy:89.472%\n",
      "Epoch : 0 [28800/49000 (59%)]\tLoss: 0.065676\t Accuracy:89.720%\n",
      "Epoch : 0 [30400/49000 (62%)]\tLoss: 0.271769\t Accuracy:89.905%\n",
      "Epoch : 0 [32000/49000 (65%)]\tLoss: 0.097908\t Accuracy:90.122%\n",
      "Epoch : 0 [33600/49000 (69%)]\tLoss: 0.354305\t Accuracy:90.316%\n",
      "Epoch : 0 [35200/49000 (72%)]\tLoss: 0.064940\t Accuracy:90.458%\n",
      "Epoch : 0 [36800/49000 (75%)]\tLoss: 0.327679\t Accuracy:90.620%\n",
      "Epoch : 0 [38400/49000 (78%)]\tLoss: 0.144602\t Accuracy:90.758%\n",
      "Epoch : 0 [40000/49000 (82%)]\tLoss: 0.118381\t Accuracy:90.912%\n",
      "Epoch : 0 [41600/49000 (85%)]\tLoss: 0.199216\t Accuracy:91.057%\n",
      "Epoch : 0 [43200/49000 (88%)]\tLoss: 0.189860\t Accuracy:91.157%\n",
      "Epoch : 0 [44800/49000 (91%)]\tLoss: 0.413594\t Accuracy:91.263%\n",
      "Epoch : 0 [46400/49000 (95%)]\tLoss: 0.041479\t Accuracy:91.387%\n",
      "Epoch : 0 [48000/49000 (98%)]\tLoss: 0.092959\t Accuracy:91.470%\n",
      "Epoch : 1 [0/49000 (0%)]\tLoss: 0.266363\t Accuracy:93.750%\n",
      "Epoch : 1 [1600/49000 (3%)]\tLoss: 0.055156\t Accuracy:95.098%\n",
      "Epoch : 1 [3200/49000 (7%)]\tLoss: 0.137875\t Accuracy:94.833%\n",
      "Epoch : 1 [4800/49000 (10%)]\tLoss: 0.111777\t Accuracy:95.137%\n",
      "Epoch : 1 [6400/49000 (13%)]\tLoss: 0.073176\t Accuracy:95.165%\n",
      "Epoch : 1 [8000/49000 (16%)]\tLoss: 0.270996\t Accuracy:95.107%\n",
      "Epoch : 1 [9600/49000 (20%)]\tLoss: 0.057180\t Accuracy:95.017%\n",
      "Epoch : 1 [11200/49000 (23%)]\tLoss: 0.232776\t Accuracy:94.961%\n",
      "Epoch : 1 [12800/49000 (26%)]\tLoss: 0.398035\t Accuracy:95.036%\n",
      "Epoch : 1 [14400/49000 (29%)]\tLoss: 0.192055\t Accuracy:94.956%\n",
      "Epoch : 1 [16000/49000 (33%)]\tLoss: 0.071758\t Accuracy:94.948%\n",
      "Epoch : 1 [17600/49000 (36%)]\tLoss: 0.023633\t Accuracy:94.998%\n",
      "Epoch : 1 [19200/49000 (39%)]\tLoss: 0.323150\t Accuracy:94.998%\n",
      "Epoch : 1 [20800/49000 (42%)]\tLoss: 0.050068\t Accuracy:95.008%\n",
      "Epoch : 1 [22400/49000 (46%)]\tLoss: 0.120434\t Accuracy:94.985%\n",
      "Epoch : 1 [24000/49000 (49%)]\tLoss: 0.077826\t Accuracy:95.002%\n",
      "Epoch : 1 [25600/49000 (52%)]\tLoss: 0.059897\t Accuracy:94.979%\n",
      "Epoch : 1 [27200/49000 (55%)]\tLoss: 0.461971\t Accuracy:94.954%\n",
      "Epoch : 1 [28800/49000 (59%)]\tLoss: 0.231923\t Accuracy:94.981%\n",
      "Epoch : 1 [30400/49000 (62%)]\tLoss: 0.124759\t Accuracy:94.999%\n",
      "Epoch : 1 [32000/49000 (65%)]\tLoss: 0.175783\t Accuracy:95.052%\n",
      "Epoch : 1 [33600/49000 (69%)]\tLoss: 0.133314\t Accuracy:95.091%\n",
      "Epoch : 1 [35200/49000 (72%)]\tLoss: 0.115610\t Accuracy:95.093%\n",
      "Epoch : 1 [36800/49000 (75%)]\tLoss: 0.016148\t Accuracy:95.140%\n",
      "Epoch : 1 [38400/49000 (78%)]\tLoss: 0.090979\t Accuracy:95.163%\n",
      "Epoch : 1 [40000/49000 (82%)]\tLoss: 0.209329\t Accuracy:95.216%\n",
      "Epoch : 1 [41600/49000 (85%)]\tLoss: 0.198735\t Accuracy:95.294%\n",
      "Epoch : 1 [43200/49000 (88%)]\tLoss: 0.124494\t Accuracy:95.323%\n",
      "Epoch : 1 [44800/49000 (91%)]\tLoss: 0.288605\t Accuracy:95.336%\n",
      "Epoch : 1 [46400/49000 (95%)]\tLoss: 0.030877\t Accuracy:95.387%\n",
      "Epoch : 1 [48000/49000 (98%)]\tLoss: 0.017310\t Accuracy:95.397%\n",
      "Epoch : 2 [0/49000 (0%)]\tLoss: 0.137043\t Accuracy:96.875%\n",
      "Epoch : 2 [1600/49000 (3%)]\tLoss: 0.110494\t Accuracy:96.507%\n",
      "Epoch : 2 [3200/49000 (7%)]\tLoss: 0.049812\t Accuracy:96.689%\n",
      "Epoch : 2 [4800/49000 (10%)]\tLoss: 0.209668\t Accuracy:96.233%\n",
      "Epoch : 2 [6400/49000 (13%)]\tLoss: 0.052262\t Accuracy:96.362%\n",
      "Epoch : 2 [8000/49000 (16%)]\tLoss: 0.042578\t Accuracy:96.240%\n",
      "Epoch : 2 [9600/49000 (20%)]\tLoss: 0.035879\t Accuracy:96.179%\n",
      "Epoch : 2 [11200/49000 (23%)]\tLoss: 0.273037\t Accuracy:96.065%\n",
      "Epoch : 2 [12800/49000 (26%)]\tLoss: 0.306209\t Accuracy:96.057%\n",
      "Epoch : 2 [14400/49000 (29%)]\tLoss: 0.176272\t Accuracy:96.113%\n",
      "Epoch : 2 [16000/49000 (33%)]\tLoss: 0.086812\t Accuracy:96.145%\n",
      "Epoch : 2 [17600/49000 (36%)]\tLoss: 0.121697\t Accuracy:96.143%\n",
      "Epoch : 2 [19200/49000 (39%)]\tLoss: 0.439350\t Accuracy:96.111%\n",
      "Epoch : 2 [20800/49000 (42%)]\tLoss: 0.010299\t Accuracy:96.030%\n",
      "Epoch : 2 [22400/49000 (46%)]\tLoss: 0.177998\t Accuracy:96.010%\n",
      "Epoch : 2 [24000/49000 (49%)]\tLoss: 0.094246\t Accuracy:95.947%\n",
      "Epoch : 2 [25600/49000 (52%)]\tLoss: 0.092448\t Accuracy:95.970%\n",
      "Epoch : 2 [27200/49000 (55%)]\tLoss: 0.214489\t Accuracy:95.968%\n",
      "Epoch : 2 [28800/49000 (59%)]\tLoss: 0.104497\t Accuracy:96.011%\n",
      "Epoch : 2 [30400/49000 (62%)]\tLoss: 0.237579\t Accuracy:96.017%\n",
      "Epoch : 2 [32000/49000 (65%)]\tLoss: 0.011109\t Accuracy:96.088%\n",
      "Epoch : 2 [33600/49000 (69%)]\tLoss: 0.127554\t Accuracy:96.111%\n",
      "Epoch : 2 [35200/49000 (72%)]\tLoss: 0.046794\t Accuracy:96.094%\n",
      "Epoch : 2 [36800/49000 (75%)]\tLoss: 0.107856\t Accuracy:96.112%\n",
      "Epoch : 2 [38400/49000 (78%)]\tLoss: 0.158532\t Accuracy:96.123%\n",
      "Epoch : 2 [40000/49000 (82%)]\tLoss: 0.256969\t Accuracy:96.123%\n",
      "Epoch : 2 [41600/49000 (85%)]\tLoss: 0.087037\t Accuracy:96.154%\n",
      "Epoch : 2 [43200/49000 (88%)]\tLoss: 0.098182\t Accuracy:96.179%\n",
      "Epoch : 2 [44800/49000 (91%)]\tLoss: 0.129102\t Accuracy:96.179%\n",
      "Epoch : 2 [46400/49000 (95%)]\tLoss: 0.010749\t Accuracy:96.197%\n",
      "Epoch : 2 [48000/49000 (98%)]\tLoss: 0.081919\t Accuracy:96.190%\n",
      "Epoch : 3 [0/49000 (0%)]\tLoss: 0.065258\t Accuracy:96.875%\n",
      "Epoch : 3 [1600/49000 (3%)]\tLoss: 0.067349\t Accuracy:96.814%\n",
      "Epoch : 3 [3200/49000 (7%)]\tLoss: 0.053883\t Accuracy:96.627%\n",
      "Epoch : 3 [4800/49000 (10%)]\tLoss: 0.069199\t Accuracy:96.523%\n",
      "Epoch : 3 [6400/49000 (13%)]\tLoss: 0.002654\t Accuracy:96.797%\n",
      "Epoch : 3 [8000/49000 (16%)]\tLoss: 0.165492\t Accuracy:96.489%\n",
      "Epoch : 3 [9600/49000 (20%)]\tLoss: 0.053892\t Accuracy:96.470%\n",
      "Epoch : 3 [11200/49000 (23%)]\tLoss: 0.182808\t Accuracy:96.554%\n",
      "Epoch : 3 [12800/49000 (26%)]\tLoss: 0.163896\t Accuracy:96.594%\n",
      "Epoch : 3 [14400/49000 (29%)]\tLoss: 0.066412\t Accuracy:96.653%\n",
      "Epoch : 3 [16000/49000 (33%)]\tLoss: 0.200641\t Accuracy:96.675%\n",
      "Epoch : 3 [17600/49000 (36%)]\tLoss: 0.038016\t Accuracy:96.716%\n",
      "Epoch : 3 [19200/49000 (39%)]\tLoss: 0.226017\t Accuracy:96.703%\n",
      "Epoch : 3 [20800/49000 (42%)]\tLoss: 0.010349\t Accuracy:96.712%\n",
      "Epoch : 3 [22400/49000 (46%)]\tLoss: 0.367582\t Accuracy:96.719%\n",
      "Epoch : 3 [24000/49000 (49%)]\tLoss: 0.111751\t Accuracy:96.684%\n",
      "Epoch : 3 [25600/49000 (52%)]\tLoss: 0.170642\t Accuracy:96.637%\n",
      "Epoch : 3 [27200/49000 (55%)]\tLoss: 1.033486\t Accuracy:96.614%\n",
      "Epoch : 3 [28800/49000 (59%)]\tLoss: 0.161623\t Accuracy:96.639%\n",
      "Epoch : 3 [30400/49000 (62%)]\tLoss: 0.058980\t Accuracy:96.619%\n",
      "Epoch : 3 [32000/49000 (65%)]\tLoss: 0.058076\t Accuracy:96.641%\n",
      "Epoch : 3 [33600/49000 (69%)]\tLoss: 0.204643\t Accuracy:96.670%\n",
      "Epoch : 3 [35200/49000 (72%)]\tLoss: 0.064739\t Accuracy:96.671%\n",
      "Epoch : 3 [36800/49000 (75%)]\tLoss: 0.022647\t Accuracy:96.669%\n",
      "Epoch : 3 [38400/49000 (78%)]\tLoss: 0.071490\t Accuracy:96.675%\n",
      "Epoch : 3 [40000/49000 (82%)]\tLoss: 0.087144\t Accuracy:96.635%\n",
      "Epoch : 3 [41600/49000 (85%)]\tLoss: 0.181756\t Accuracy:96.664%\n",
      "Epoch : 3 [43200/49000 (88%)]\tLoss: 0.005839\t Accuracy:96.706%\n",
      "Epoch : 3 [44800/49000 (91%)]\tLoss: 0.141244\t Accuracy:96.737%\n",
      "Epoch : 3 [46400/49000 (95%)]\tLoss: 0.026308\t Accuracy:96.776%\n",
      "Epoch : 3 [48000/49000 (98%)]\tLoss: 0.012498\t Accuracy:96.777%\n",
      "Epoch : 4 [0/49000 (0%)]\tLoss: 0.053175\t Accuracy:100.000%\n",
      "Epoch : 4 [1600/49000 (3%)]\tLoss: 0.079510\t Accuracy:96.998%\n",
      "Epoch : 4 [3200/49000 (7%)]\tLoss: 0.179215\t Accuracy:96.875%\n",
      "Epoch : 4 [4800/49000 (10%)]\tLoss: 0.160457\t Accuracy:96.978%\n",
      "Epoch : 4 [6400/49000 (13%)]\tLoss: 0.087734\t Accuracy:97.093%\n",
      "Epoch : 4 [8000/49000 (16%)]\tLoss: 0.067092\t Accuracy:97.099%\n",
      "Epoch : 4 [9600/49000 (20%)]\tLoss: 0.011775\t Accuracy:96.989%\n",
      "Epoch : 4 [11200/49000 (23%)]\tLoss: 0.096461\t Accuracy:96.902%\n",
      "Epoch : 4 [12800/49000 (26%)]\tLoss: 0.186442\t Accuracy:96.930%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 4 [14400/49000 (29%)]\tLoss: 0.031756\t Accuracy:96.889%\n",
      "Epoch : 4 [16000/49000 (33%)]\tLoss: 0.049820\t Accuracy:96.956%\n",
      "Epoch : 4 [17600/49000 (36%)]\tLoss: 0.069461\t Accuracy:96.943%\n",
      "Epoch : 4 [19200/49000 (39%)]\tLoss: 0.364155\t Accuracy:96.948%\n",
      "Epoch : 4 [20800/49000 (42%)]\tLoss: 0.004673\t Accuracy:96.918%\n",
      "Epoch : 4 [22400/49000 (46%)]\tLoss: 0.272254\t Accuracy:96.897%\n",
      "Epoch : 4 [24000/49000 (49%)]\tLoss: 0.197898\t Accuracy:96.896%\n",
      "Epoch : 4 [25600/49000 (52%)]\tLoss: 0.007085\t Accuracy:96.871%\n",
      "Epoch : 4 [27200/49000 (55%)]\tLoss: 0.201744\t Accuracy:96.915%\n",
      "Epoch : 4 [28800/49000 (59%)]\tLoss: 0.098582\t Accuracy:96.927%\n",
      "Epoch : 4 [30400/49000 (62%)]\tLoss: 0.009788\t Accuracy:96.918%\n",
      "Epoch : 4 [32000/49000 (65%)]\tLoss: 0.016327\t Accuracy:96.937%\n",
      "Epoch : 4 [33600/49000 (69%)]\tLoss: 0.130120\t Accuracy:96.967%\n",
      "Epoch : 4 [35200/49000 (72%)]\tLoss: 0.122135\t Accuracy:96.969%\n",
      "Epoch : 4 [36800/49000 (75%)]\tLoss: 0.119698\t Accuracy:96.997%\n",
      "Epoch : 4 [38400/49000 (78%)]\tLoss: 0.125789\t Accuracy:97.018%\n",
      "Epoch : 4 [40000/49000 (82%)]\tLoss: 0.087052\t Accuracy:97.027%\n",
      "Epoch : 4 [41600/49000 (85%)]\tLoss: 0.008042\t Accuracy:97.036%\n",
      "Epoch : 4 [43200/49000 (88%)]\tLoss: 0.101792\t Accuracy:97.048%\n",
      "Epoch : 4 [44800/49000 (91%)]\tLoss: 0.027414\t Accuracy:97.051%\n",
      "Epoch : 4 [46400/49000 (95%)]\tLoss: 0.010077\t Accuracy:97.077%\n",
      "Epoch : 4 [48000/49000 (98%)]\tLoss: 0.005471\t Accuracy:97.098%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "spmlp = StructuredPrunedMLP()\n",
    "fit(spmlp, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.960% \n"
     ]
    }
   ],
   "source": [
    "evaluate(spmlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spmlp.prune(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.919% \n"
     ]
    }
   ],
   "source": [
    "evaluate(spmlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111500"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_pytorch_weights(spmlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно посмотреть на устройство весов в наших последних двух моделях\n",
    "\n",
    "У модели, которую прореживали по весам, у каждого нейрона отключены какие-то элементы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
       "        1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "        1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
       "        1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "        0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppmlp.linear1.weight_mask.T[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У модели, которую прореживали по нейронам, нейрон или отключен совсем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spmlp.linear1.weight_mask.T[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Или работает целиком"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spmlp.linear1.weight_mask.T[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дистилляция знаний из модели\n",
    "\n",
    "В этой лабораторной попробуем использовать метод дистиляции, чтобы обучить небольшую модель, которая бы \"впитала\" знания большой модели и работала примерно так же хорошо."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe1bc295ad0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED=9876\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этот раз будем использовать более продвинутую версию mnist - fashion mnist. Задача заключается в том, чтобы по картинке 28x28 понять какой тип одежды изображен."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv(\"data/fashion-mnist_train.csv\")\n",
    "test_csv = pd.read_csv(\"data/fashion-mnist_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      2       0       0       0       0       0       0       0       0   \n",
       "1      9       0       0       0       0       0       0       0       0   \n",
       "2      6       0       0       0       0       0       0       0       5   \n",
       "3      0       0       0       0       1       2       0       0       0   \n",
       "4      3       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0        30        43         0   \n",
       "3       0  ...         3         0         0         0         0         1   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_csv['label'].values\n",
    "X_train = train_csv.drop(['label'],axis=1).values\n",
    "\n",
    "y_test = test_csv['label'].values\n",
    "X_test = test_csv.drop(['label'],axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe13089fd30>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe130911820>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[1].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "torch_X_train = torch.from_numpy(X_train).type(torch.LongTensor)\n",
    "torch_y_train = torch.from_numpy(y_train).type(torch.LongTensor)\n",
    "torch_X_test = torch.from_numpy(X_test).type(torch.LongTensor)\n",
    "torch_y_test = torch.from_numpy(y_test).type(torch.LongTensor)\n",
    "\n",
    "train = torch.utils.data.TensorDataset(torch_X_train,torch_y_train)\n",
    "test = torch.utils.data.TensorDataset(torch_X_test,torch_y_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size = BATCH_SIZE, shuffle = False)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size = BATCH_SIZE, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве базовой модели, с которой будем сравнивать, возьмем простую нейронную сеть с тремя полносвязными слоями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (linear1): Linear(in_features=784, out_features=250, bias=True)\n",
      "  (linear2): Linear(in_features=250, out_features=100, bias=True)\n",
      "  (linear3): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(784,250)\n",
    "        self.linear2 = nn.Linear(250,100)\n",
    "        self.linear3 = nn.Linear(100,10)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        X = F.relu(self.linear1(X))\n",
    "        X = F.relu(self.linear2(X))\n",
    "        X = self.linear3(X)\n",
    "        return X\n",
    "\n",
    "mlp = MLP()\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, train_loader, epoch_number=5):\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    error = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epoch_number):\n",
    "        correct = 0\n",
    "        \n",
    "        for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "            var_X_batch = Variable(X_batch).float()\n",
    "            var_y_batch = Variable(y_batch)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(var_X_batch)\n",
    "            loss = error(output, var_y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            predicted = torch.max(output.data, 1)[1] \n",
    "            correct += (predicted == var_y_batch).sum()\n",
    "            if batch_idx % 200 == 0:\n",
    "                print('Epoch : {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Accuracy:{:.3f}%'.format(\n",
    "                    epoch, batch_idx*len(X_batch), len(train_loader.dataset), 100.*batch_idx / len(train_loader), loss.data, float(correct*100) / float(BATCH_SIZE*(batch_idx+1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    correct = 0 \n",
    "    for test_imgs, test_labels in test_loader:\n",
    "        test_imgs = Variable(test_imgs).float()\n",
    "        \n",
    "        output = model(test_imgs)\n",
    "        predicted = torch.max(output,1)[1]\n",
    "        correct += (predicted == test_labels).sum()\n",
    "    print(\"Test accuracy:{:.3f}% \".format( float(correct) / (len(test_loader)*BATCH_SIZE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модель и посмотрим, какое качество она выдает на наших данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 [0/60000 (0%)]\tLoss: 17.452322\t Accuracy:3.125%\n",
      "Epoch : 0 [6400/60000 (11%)]\tLoss: 0.776435\t Accuracy:69.030%\n",
      "Epoch : 0 [12800/60000 (21%)]\tLoss: 0.852823\t Accuracy:73.566%\n",
      "Epoch : 0 [19200/60000 (32%)]\tLoss: 0.474309\t Accuracy:75.109%\n",
      "Epoch : 0 [25600/60000 (43%)]\tLoss: 0.257116\t Accuracy:76.537%\n",
      "Epoch : 0 [32000/60000 (53%)]\tLoss: 0.651054\t Accuracy:77.316%\n",
      "Epoch : 0 [38400/60000 (64%)]\tLoss: 0.526560\t Accuracy:78.029%\n",
      "Epoch : 0 [44800/60000 (75%)]\tLoss: 0.484273\t Accuracy:78.689%\n",
      "Epoch : 0 [51200/60000 (85%)]\tLoss: 0.247172\t Accuracy:79.109%\n",
      "Epoch : 0 [57600/60000 (96%)]\tLoss: 0.271017\t Accuracy:79.508%\n",
      "Epoch : 1 [0/60000 (0%)]\tLoss: 0.501332\t Accuracy:84.375%\n",
      "Epoch : 1 [6400/60000 (11%)]\tLoss: 0.580329\t Accuracy:83.458%\n",
      "Epoch : 1 [12800/60000 (21%)]\tLoss: 0.802431\t Accuracy:83.728%\n",
      "Epoch : 1 [19200/60000 (32%)]\tLoss: 0.240229\t Accuracy:83.689%\n",
      "Epoch : 1 [25600/60000 (43%)]\tLoss: 0.274157\t Accuracy:83.926%\n",
      "Epoch : 1 [32000/60000 (53%)]\tLoss: 0.478150\t Accuracy:84.000%\n",
      "Epoch : 1 [38400/60000 (64%)]\tLoss: 0.489609\t Accuracy:84.094%\n",
      "Epoch : 1 [44800/60000 (75%)]\tLoss: 0.181453\t Accuracy:84.197%\n",
      "Epoch : 1 [51200/60000 (85%)]\tLoss: 0.197932\t Accuracy:84.215%\n",
      "Epoch : 1 [57600/60000 (96%)]\tLoss: 0.141351\t Accuracy:84.195%\n",
      "Epoch : 2 [0/60000 (0%)]\tLoss: 0.445465\t Accuracy:81.250%\n",
      "Epoch : 2 [6400/60000 (11%)]\tLoss: 0.550776\t Accuracy:85.463%\n",
      "Epoch : 2 [12800/60000 (21%)]\tLoss: 0.784535\t Accuracy:85.318%\n",
      "Epoch : 2 [19200/60000 (32%)]\tLoss: 0.302194\t Accuracy:85.285%\n",
      "Epoch : 2 [25600/60000 (43%)]\tLoss: 0.259635\t Accuracy:85.327%\n",
      "Epoch : 2 [32000/60000 (53%)]\tLoss: 0.511338\t Accuracy:85.343%\n",
      "Epoch : 2 [38400/60000 (64%)]\tLoss: 0.508349\t Accuracy:85.364%\n",
      "Epoch : 2 [44800/60000 (75%)]\tLoss: 0.222788\t Accuracy:85.426%\n",
      "Epoch : 2 [51200/60000 (85%)]\tLoss: 0.201509\t Accuracy:85.433%\n",
      "Epoch : 2 [57600/60000 (96%)]\tLoss: 0.129544\t Accuracy:85.394%\n",
      "Epoch : 3 [0/60000 (0%)]\tLoss: 0.464175\t Accuracy:84.375%\n",
      "Epoch : 3 [6400/60000 (11%)]\tLoss: 0.430747\t Accuracy:86.132%\n",
      "Epoch : 3 [12800/60000 (21%)]\tLoss: 0.550969\t Accuracy:86.354%\n",
      "Epoch : 3 [19200/60000 (32%)]\tLoss: 0.218653\t Accuracy:86.210%\n",
      "Epoch : 3 [25600/60000 (43%)]\tLoss: 0.227416\t Accuracy:86.384%\n",
      "Epoch : 3 [32000/60000 (53%)]\tLoss: 0.443824\t Accuracy:86.342%\n",
      "Epoch : 3 [38400/60000 (64%)]\tLoss: 0.636973\t Accuracy:86.355%\n",
      "Epoch : 3 [44800/60000 (75%)]\tLoss: 0.212215\t Accuracy:86.338%\n",
      "Epoch : 3 [51200/60000 (85%)]\tLoss: 0.139430\t Accuracy:86.311%\n",
      "Epoch : 3 [57600/60000 (96%)]\tLoss: 0.111888\t Accuracy:86.277%\n",
      "Epoch : 4 [0/60000 (0%)]\tLoss: 0.366418\t Accuracy:90.625%\n",
      "Epoch : 4 [6400/60000 (11%)]\tLoss: 0.328854\t Accuracy:86.350%\n",
      "Epoch : 4 [12800/60000 (21%)]\tLoss: 0.548842\t Accuracy:86.417%\n",
      "Epoch : 4 [19200/60000 (32%)]\tLoss: 0.252456\t Accuracy:86.678%\n",
      "Epoch : 4 [25600/60000 (43%)]\tLoss: 0.216246\t Accuracy:86.689%\n",
      "Epoch : 4 [32000/60000 (53%)]\tLoss: 0.361780\t Accuracy:86.535%\n",
      "Epoch : 4 [38400/60000 (64%)]\tLoss: 0.453401\t Accuracy:86.615%\n",
      "Epoch : 4 [44800/60000 (75%)]\tLoss: 0.158731\t Accuracy:86.730%\n",
      "Epoch : 4 [51200/60000 (85%)]\tLoss: 0.094140\t Accuracy:86.754%\n",
      "Epoch : 4 [57600/60000 (96%)]\tLoss: 0.209827\t Accuracy:86.691%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "fit(mlp, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.862% \n"
     ]
    }
   ],
   "source": [
    "evaluate(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/fashion-mnist_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df.drop(['label'],axis=1).values\n",
    "y_test = df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_data = [X_test[0].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = torch.tensor(r_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "in2 = Variable(torch.from_numpy(np.array(r_data)).type(torch.LongTensor).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = mlp(in2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs = softmax(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = torch.argmax(pred_probs[0]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.624250590801239"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_probs[0][index].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.7410e-01, 1.0486e-07, 1.7212e-04, 1.4004e-03, 2.4034e-05, 1.4562e-08,\n",
       "         6.2425e-01, 2.7763e-10, 5.1771e-05, 9.1662e-09]],\n",
       "       grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  4.8889, -10.1986,  -2.7952,  ..., -16.1326,  -3.9966, -12.6357],\n",
       "        [ -4.5517,  23.1163,  -6.3998,  ..., -34.8441, -14.2545, -31.1900],\n",
       "        [  3.2077,  -0.6260,   4.9138,  ..., -15.2540,   1.1353, -15.1236],\n",
       "        ...,\n",
       "        [ 13.8483, -18.7048, -13.2403,  ...,  -3.9779,  40.8589,  -2.1101],\n",
       "        [  5.4096,  -4.1935,  -0.1891,  ..., -11.3076,   7.3288, -12.9073],\n",
       "        [  0.1158,   0.8623,   1.8577,  ...,  -5.5888,  -1.1536,  -6.9834]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp(Variable(torch_X_test).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,   0,   0,   0,   0,   9,   8,   0,   0,  34,  29,   7,\n",
       "           0,  11,  24,   0,   0,   3,   3,   1,   0,   1,   1,   0,   0,   0,\n",
       "           0,   0,   4,   0,   0,   1,   0,   0,   0,   0,   0,  44,  88,  99,\n",
       "         122, 123,  80,   0,   0,   0,   0,   1,   1,   1,   0,   0,   0,   0,\n",
       "           0,   0,   1,   2,   0,   0,   0,   3,  46, 174, 249,  67,   0,  94,\n",
       "         210,  61,  14, 212, 157,  37,   0,   0,   0,   0,   1,   0,   0,   0,\n",
       "           0,   0,   2,   2,   0,  23, 168, 206, 242, 239, 238, 214, 125,  61,\n",
       "         113,  74, 133, 236, 238, 236, 203, 184,  20,   0,   1,   0,   0,   0,\n",
       "           0,   0,   1,   0,   0, 175, 245, 223, 207, 205, 206, 216, 255, 237,\n",
       "         251, 232, 223, 212, 200, 205, 216, 249, 173,   0,   0,   2,   0,   0,\n",
       "           0,   0,   7,   0,  53, 225, 201, 197, 200, 201, 206, 199, 197, 185,\n",
       "         194, 204, 232, 226, 249, 219, 194, 205, 229,  33,   0,   1,   0,   0,\n",
       "           0,   0,   1,   0, 133, 223, 208, 192, 195, 233, 226, 216, 191, 210,\n",
       "         188, 236, 186,   0,  50, 234, 207, 208, 231, 133,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0, 216, 218, 216, 194, 229, 172,  64, 219, 201, 200,\n",
       "         200, 247,  68,  72,  54, 165, 237, 212, 219, 226,   0,   0,   0,   0,\n",
       "           0,   0,   0,  50, 221, 207, 220, 211, 207, 165, 138, 205, 192, 191,\n",
       "         190, 232, 119, 113,  67, 173, 237, 217, 208, 221,  29,   0,   0,   0,\n",
       "           0,   0,   0, 131, 216, 200, 219, 207, 212, 231, 226, 193, 214, 224,\n",
       "         206, 203, 230, 122, 112, 234, 224, 214, 204, 224, 123,   0,   0,   0,\n",
       "           0,   0,   0, 195, 212, 204, 211, 203, 205, 200, 184, 213, 162, 138,\n",
       "         193, 207, 203, 231, 245, 208, 220, 211, 203, 219, 179,   0,   0,   0,\n",
       "           0,   0,   8, 185, 191, 218, 233, 219, 201, 221, 213, 246, 114, 127,\n",
       "          80, 129, 232, 198, 218, 207, 236, 227, 220, 216, 172,  21,   0,   0,\n",
       "           0,   0,  21,   4,   5,  64, 160, 224, 224, 144, 187, 197, 211, 207,\n",
       "         186, 192, 210, 212, 218, 225, 236, 177, 106,  56,  28,   1,   0,   0,\n",
       "           0,   0,   1,   1,   0,   2,   0, 116, 252,  96, 120,  51,  73,  70,\n",
       "         123,  79,  76,  64, 162, 252, 118,   1,   3,   0,   4,   2,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0, 115, 226, 145, 170, 155, 165, 161,\n",
       "         159, 125, 175, 140, 174, 236,  95,   0,   2,   2,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   1,   2,   0, 131, 225, 204, 217, 221, 220, 217,\n",
       "         224, 231, 226, 237, 203, 237, 102,   0,   4,   2,   1,   2,   0,   0,\n",
       "           0,   0,   1,   1,   0,   3,   0, 135, 223, 201, 199, 194, 198, 195,\n",
       "         198, 192, 203, 199, 207, 231, 112,   0,   4,   0,   0,   0,   0,   0,\n",
       "           0,   0,   1,   1,   0,   1,   0, 134, 223, 199, 206, 199, 201, 200,\n",
       "         203, 206, 207, 210, 206, 227, 119,   0,   3,   0,   0,   1,   0,   0,\n",
       "           0,   0,   0,   0,   0,   1,   0, 139, 223, 198, 204, 200, 201, 200,\n",
       "         201, 204, 206, 208, 206, 229, 128,   0,   4,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   1,   0, 145, 223, 195, 205, 201, 201, 200,\n",
       "         204, 204, 206, 211, 205, 230, 139,   0,   2,   0,   0,   0,   0,   0,\n",
       "           0,   0,   1,   0,   1,   0,   0, 157, 221, 194, 204, 204, 201, 201,\n",
       "         203, 205, 208, 211, 204, 230, 148,   0,   2,   0,   1,   1,   0,   0,\n",
       "           0,   0,   1,   1,   1,   0,   0, 166, 220, 194, 203, 203, 205, 203,\n",
       "         203, 206, 207, 212, 204, 230, 157,   0,   2,   1,   1,   1,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0, 171, 221, 195, 206, 200, 199, 203,\n",
       "         203, 205, 206, 207, 204, 226, 181,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   1,   0,   0, 165, 224, 197, 201, 208, 199, 204,\n",
       "         205, 207, 210, 213, 207, 229, 187,   0,   1,   2,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0, 128, 201, 203, 201, 207, 211, 203,\n",
       "         205, 206, 210, 213, 205, 225, 191,   0,   0,   2,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   1,   1,   0, 141, 201, 191, 188, 194, 187, 187,\n",
       "         191, 193, 195, 199, 199, 218, 161,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   1,   0, 212, 240, 213, 239, 233, 239, 231,\n",
       "         232, 236, 242, 245, 224, 245, 234,   0,   3,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,  37,  69,  94, 123, 127, 138, 138,\n",
       "         142, 145, 135, 125, 103,  87,  56,   0,   0,   0,   0,   0,   0,   0]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Float but got scalar type Long for argument #2 'mat1' in call to _th_addmm",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-18ec1931368c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-ac19b64aeb96>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1672\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1673\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1674\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1676\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Float but got scalar type Long for argument #2 'mat1' in call to _th_addmm"
     ]
    }
   ],
   "source": [
    "mlp(in2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на размер модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_weights(model):\n",
    "    result = 0\n",
    "    for layer in model.children():\n",
    "        result += len(layer.weight.reshape(-1))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222000"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_weights(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Достаточно большое количество параметров. \n",
    "\n",
    "Попробуем сделать модель сильно меньше и улучить ее, дистилировав в нее знания из большой модели.\n",
    "\n",
    "В качестве такой модели возьмем сеть с одним скрытым слоем на 16 нейронов: 784 - 16 - 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StudentMLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(784,16)\n",
    "        self.linear2 = nn.Linear(16,10)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        X = F.relu(self.linear1(X))\n",
    "        X = self.linear2(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "smlp_simple = StudentMLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12704"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_weights(smlp_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12 000 - это примерно 5% от изначальной сети. Попробуем просто обучить ее на данных и посмотрим, какое качество вообще она может выдать самостоятельно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 [0/60000 (0%)]\tLoss: 50.468872\t Accuracy:3.125%\n",
      "Epoch : 0 [6400/60000 (11%)]\tLoss: 1.258342\t Accuracy:37.749%\n",
      "Epoch : 0 [12800/60000 (21%)]\tLoss: 1.803313\t Accuracy:46.875%\n",
      "Epoch : 0 [19200/60000 (32%)]\tLoss: 1.222115\t Accuracy:49.792%\n",
      "Epoch : 0 [25600/60000 (43%)]\tLoss: 1.066455\t Accuracy:50.534%\n",
      "Epoch : 0 [32000/60000 (53%)]\tLoss: 1.179734\t Accuracy:51.074%\n",
      "Epoch : 0 [38400/60000 (64%)]\tLoss: 1.131818\t Accuracy:51.837%\n",
      "Epoch : 0 [44800/60000 (75%)]\tLoss: 0.964317\t Accuracy:52.621%\n",
      "Epoch : 0 [51200/60000 (85%)]\tLoss: 0.881685\t Accuracy:53.279%\n",
      "Epoch : 0 [57600/60000 (96%)]\tLoss: 0.697759\t Accuracy:53.824%\n",
      "Epoch : 1 [0/60000 (0%)]\tLoss: 1.039469\t Accuracy:50.000%\n",
      "Epoch : 1 [6400/60000 (11%)]\tLoss: 0.804221\t Accuracy:59.904%\n",
      "Epoch : 1 [12800/60000 (21%)]\tLoss: 1.402066\t Accuracy:60.263%\n",
      "Epoch : 1 [19200/60000 (32%)]\tLoss: 0.846651\t Accuracy:60.472%\n",
      "Epoch : 1 [25600/60000 (43%)]\tLoss: 0.866411\t Accuracy:60.725%\n",
      "Epoch : 1 [32000/60000 (53%)]\tLoss: 0.843927\t Accuracy:60.845%\n",
      "Epoch : 1 [38400/60000 (64%)]\tLoss: 0.654948\t Accuracy:60.947%\n",
      "Epoch : 1 [44800/60000 (75%)]\tLoss: 0.848737\t Accuracy:61.153%\n",
      "Epoch : 1 [51200/60000 (85%)]\tLoss: 0.768755\t Accuracy:61.286%\n",
      "Epoch : 1 [57600/60000 (96%)]\tLoss: 0.608364\t Accuracy:61.365%\n",
      "Epoch : 2 [0/60000 (0%)]\tLoss: 0.939342\t Accuracy:53.125%\n",
      "Epoch : 2 [6400/60000 (11%)]\tLoss: 0.851079\t Accuracy:62.469%\n",
      "Epoch : 2 [12800/60000 (21%)]\tLoss: 1.287918\t Accuracy:62.531%\n",
      "Epoch : 2 [19200/60000 (32%)]\tLoss: 0.738902\t Accuracy:62.464%\n",
      "Epoch : 2 [25600/60000 (43%)]\tLoss: 0.787582\t Accuracy:62.773%\n",
      "Epoch : 2 [32000/60000 (53%)]\tLoss: 0.953677\t Accuracy:62.847%\n",
      "Epoch : 2 [38400/60000 (64%)]\tLoss: 0.626779\t Accuracy:62.823%\n",
      "Epoch : 2 [44800/60000 (75%)]\tLoss: 0.809767\t Accuracy:62.859%\n",
      "Epoch : 2 [51200/60000 (85%)]\tLoss: 0.701146\t Accuracy:62.836%\n",
      "Epoch : 2 [57600/60000 (96%)]\tLoss: 0.632361\t Accuracy:62.835%\n",
      "Epoch : 3 [0/60000 (0%)]\tLoss: 0.891030\t Accuracy:50.000%\n",
      "Epoch : 3 [6400/60000 (11%)]\tLoss: 0.782907\t Accuracy:63.402%\n",
      "Epoch : 3 [12800/60000 (21%)]\tLoss: 1.277026\t Accuracy:63.373%\n",
      "Epoch : 3 [19200/60000 (32%)]\tLoss: 0.721843\t Accuracy:63.504%\n",
      "Epoch : 3 [25600/60000 (43%)]\tLoss: 0.766540\t Accuracy:63.904%\n",
      "Epoch : 3 [32000/60000 (53%)]\tLoss: 0.941640\t Accuracy:63.939%\n",
      "Epoch : 3 [38400/60000 (64%)]\tLoss: 0.624455\t Accuracy:63.900%\n",
      "Epoch : 3 [44800/60000 (75%)]\tLoss: 0.631647\t Accuracy:63.945%\n",
      "Epoch : 3 [51200/60000 (85%)]\tLoss: 0.589429\t Accuracy:63.929%\n",
      "Epoch : 3 [57600/60000 (96%)]\tLoss: 0.600482\t Accuracy:63.883%\n",
      "Epoch : 4 [0/60000 (0%)]\tLoss: 0.910582\t Accuracy:50.000%\n",
      "Epoch : 4 [6400/60000 (11%)]\tLoss: 0.855750\t Accuracy:63.604%\n",
      "Epoch : 4 [12800/60000 (21%)]\tLoss: 1.245522\t Accuracy:63.412%\n",
      "Epoch : 4 [19200/60000 (32%)]\tLoss: 0.718386\t Accuracy:63.389%\n",
      "Epoch : 4 [25600/60000 (43%)]\tLoss: 0.746499\t Accuracy:63.873%\n",
      "Epoch : 4 [32000/60000 (53%)]\tLoss: 0.831252\t Accuracy:63.861%\n",
      "Epoch : 4 [38400/60000 (64%)]\tLoss: 0.614386\t Accuracy:63.791%\n",
      "Epoch : 4 [44800/60000 (75%)]\tLoss: 0.679199\t Accuracy:63.881%\n",
      "Epoch : 4 [51200/60000 (85%)]\tLoss: 0.529205\t Accuracy:63.923%\n",
      "Epoch : 4 [57600/60000 (96%)]\tLoss: 0.587793\t Accuracy:63.879%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "fit(smlp_simple, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.636% \n"
     ]
    }
   ],
   "source": [
    "evaluate(smlp_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что модель слабее и качество упало примерно на 20%.\n",
    "\n",
    "Попробуем задействовать большую модель для улучшения качества. Для этого сконструируем более сложную функцию ошибки. \n",
    "\n",
    "Одно из слагаемых как и раньше - это просто кросс-энтропия для данных их обучающей выборки.\n",
    "\n",
    "Второе слагаемое - это кросс-энтропия между распределением вероятностей ответов, которое мы получили от большой модели и распределением веротяностей ответов, которое предсказывает наша модель. Мы также дополнительно сглаживаем эти распределения с помощью параметра \"температуры\" T. Это нужно, чтобы вытянуть из большой модели больше информации про данные.\n",
    "\n",
    "Складываем взвешенно эти два слагаемых с весами альфа и 1-альфа и для такой функции ошибки уже учим нашу модель как обычно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distill(teacher_model, student_model, train_loader, epoch_number=5, alpha=0.5, temperature=2):\n",
    "    def error_and_output(var_X_batch, var_y_batch): # Задаем нашу особую функцию ошибки\n",
    "        # Дивергенция Кульбака-Лейблера нужна, чтобы подсчитать кросс-энтропию между двумя распределениями\n",
    "        # А именно между распределениями ответов модели-учителя и модели-ученика\n",
    "        kldloss = nn.KLDivLoss()  \n",
    "        # Для подсчета ошибки на данных воспользуемся уже готовой функцией для кросс-энтропии\n",
    "        celoss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Считаем выходы из сети-учителя\n",
    "        teacher_logits = teacher_model(var_X_batch)\n",
    "        # И выходы из сети-ученика\n",
    "        student_logits = student_model(var_X_batch)\n",
    "        \n",
    "        # Рассчитываем распределение вероятностей ответов с помощью softmax с параметром T для сети-ученика\n",
    "        soft_predictions = F.log_softmax( student_logits / temperature, dim=1 )\n",
    "        # И для сети-учителя\n",
    "        soft_labels = F.softmax( teacher_logits / temperature, dim=1 )\n",
    "        # Считаем ошибку дистиляции - кросс-энтропию между распределениями ответов моделей\n",
    "        distillation_loss = kldloss(soft_predictions, soft_labels)\n",
    "        \n",
    "        # Считаем ошибку на данных - кросс-энтропию между распределением ответов сети-ученика и правильным ответом\n",
    "        student_loss = celoss(student_logits, var_y_batch)\n",
    "        \n",
    "        # Складываем с весами\n",
    "        return distillation_loss * alpha + student_loss * (1 - alpha), student_logits\n",
    "    \n",
    "    optimizer = torch.optim.Adam(student_model.parameters())\n",
    "    student_model.train()\n",
    "    \n",
    "    # Далее обучение проходит как обычно\n",
    "    for epoch in range(epoch_number):\n",
    "        correct = 0\n",
    "        for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "            \n",
    "            var_X_batch = Variable(X_batch).float()\n",
    "            var_y_batch = Variable(y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss, output = error_and_output(var_X_batch, var_y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            predicted = torch.max(output.data, 1)[1] \n",
    "            correct += (predicted == var_y_batch).sum()\n",
    "            if batch_idx % 200 == 0:\n",
    "                print('Epoch : {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Accuracy:{:.3f}%'.format(\n",
    "                    epoch, batch_idx*len(X_batch), len(train_loader.dataset), 100.*batch_idx / len(train_loader), loss.data, float(correct*100) / float(BATCH_SIZE*(batch_idx+1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем обучить таким образом модель с T = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:2398: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 [0/60000 (0%)]\tLoss: 18.051939\t Accuracy:9.375%\n",
      "Epoch : 0 [6400/60000 (11%)]\tLoss: 1.031293\t Accuracy:23.057%\n",
      "Epoch : 0 [12800/60000 (21%)]\tLoss: 0.922963\t Accuracy:29.832%\n",
      "Epoch : 0 [19200/60000 (32%)]\tLoss: 0.806358\t Accuracy:32.888%\n",
      "Epoch : 0 [25600/60000 (43%)]\tLoss: 0.655243\t Accuracy:34.765%\n",
      "Epoch : 0 [32000/60000 (53%)]\tLoss: 0.776560\t Accuracy:36.217%\n",
      "Epoch : 0 [38400/60000 (64%)]\tLoss: 0.883896\t Accuracy:37.510%\n",
      "Epoch : 0 [44800/60000 (75%)]\tLoss: 0.630573\t Accuracy:38.412%\n",
      "Epoch : 0 [51200/60000 (85%)]\tLoss: 0.589253\t Accuracy:39.659%\n",
      "Epoch : 0 [57600/60000 (96%)]\tLoss: 0.580639\t Accuracy:41.097%\n",
      "Epoch : 1 [0/60000 (0%)]\tLoss: 0.593468\t Accuracy:65.625%\n",
      "Epoch : 1 [6400/60000 (11%)]\tLoss: 0.730674\t Accuracy:55.348%\n",
      "Epoch : 1 [12800/60000 (21%)]\tLoss: 0.721611\t Accuracy:56.055%\n",
      "Epoch : 1 [19200/60000 (32%)]\tLoss: 0.451817\t Accuracy:56.453%\n",
      "Epoch : 1 [25600/60000 (43%)]\tLoss: 0.536171\t Accuracy:56.582%\n",
      "Epoch : 1 [32000/60000 (53%)]\tLoss: 0.541939\t Accuracy:56.721%\n",
      "Epoch : 1 [38400/60000 (64%)]\tLoss: 0.539851\t Accuracy:57.007%\n",
      "Epoch : 1 [44800/60000 (75%)]\tLoss: 0.502926\t Accuracy:57.218%\n",
      "Epoch : 1 [51200/60000 (85%)]\tLoss: 0.446669\t Accuracy:57.361%\n",
      "Epoch : 1 [57600/60000 (96%)]\tLoss: 0.464238\t Accuracy:57.326%\n",
      "Epoch : 2 [0/60000 (0%)]\tLoss: 0.466002\t Accuracy:68.750%\n",
      "Epoch : 2 [6400/60000 (11%)]\tLoss: 0.541438\t Accuracy:57.587%\n",
      "Epoch : 2 [12800/60000 (21%)]\tLoss: 0.681290\t Accuracy:58.627%\n",
      "Epoch : 2 [19200/60000 (32%)]\tLoss: 0.405805\t Accuracy:58.761%\n",
      "Epoch : 2 [25600/60000 (43%)]\tLoss: 0.512562\t Accuracy:58.903%\n",
      "Epoch : 2 [32000/60000 (53%)]\tLoss: 0.495704\t Accuracy:58.847%\n",
      "Epoch : 2 [38400/60000 (64%)]\tLoss: 0.476960\t Accuracy:59.630%\n",
      "Epoch : 2 [44800/60000 (75%)]\tLoss: 0.408009\t Accuracy:60.361%\n",
      "Epoch : 2 [51200/60000 (85%)]\tLoss: 0.311913\t Accuracy:60.985%\n",
      "Epoch : 2 [57600/60000 (96%)]\tLoss: 0.444845\t Accuracy:61.277%\n",
      "Epoch : 3 [0/60000 (0%)]\tLoss: 0.453435\t Accuracy:50.000%\n",
      "Epoch : 3 [6400/60000 (11%)]\tLoss: 0.480175\t Accuracy:66.651%\n",
      "Epoch : 3 [12800/60000 (21%)]\tLoss: 0.553000\t Accuracy:67.433%\n",
      "Epoch : 3 [19200/60000 (32%)]\tLoss: 0.371845\t Accuracy:68.027%\n",
      "Epoch : 3 [25600/60000 (43%)]\tLoss: 0.410831\t Accuracy:68.563%\n",
      "Epoch : 3 [32000/60000 (53%)]\tLoss: 0.402529\t Accuracy:68.622%\n",
      "Epoch : 3 [38400/60000 (64%)]\tLoss: 0.319330\t Accuracy:68.930%\n",
      "Epoch : 3 [44800/60000 (75%)]\tLoss: 0.366949\t Accuracy:69.334%\n",
      "Epoch : 3 [51200/60000 (85%)]\tLoss: 0.182976\t Accuracy:69.619%\n",
      "Epoch : 3 [57600/60000 (96%)]\tLoss: 0.266544\t Accuracy:69.887%\n",
      "Epoch : 4 [0/60000 (0%)]\tLoss: 0.435247\t Accuracy:59.375%\n",
      "Epoch : 4 [6400/60000 (11%)]\tLoss: 0.355297\t Accuracy:73.741%\n",
      "Epoch : 4 [12800/60000 (21%)]\tLoss: 0.528044\t Accuracy:73.504%\n",
      "Epoch : 4 [19200/60000 (32%)]\tLoss: 0.241415\t Accuracy:73.035%\n",
      "Epoch : 4 [25600/60000 (43%)]\tLoss: 0.380174\t Accuracy:73.244%\n",
      "Epoch : 4 [32000/60000 (53%)]\tLoss: 0.348821\t Accuracy:72.940%\n",
      "Epoch : 4 [38400/60000 (64%)]\tLoss: 0.307888\t Accuracy:73.129%\n",
      "Epoch : 4 [44800/60000 (75%)]\tLoss: 0.356647\t Accuracy:73.354%\n",
      "Epoch : 4 [51200/60000 (85%)]\tLoss: 0.154338\t Accuracy:73.456%\n",
      "Epoch : 4 [57600/60000 (96%)]\tLoss: 0.218767\t Accuracy:73.487%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "smlp = StudentMLP()\n",
    "distill(mlp, smlp, train_loader, temperature=10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.766% \n"
     ]
    }
   ],
   "source": [
    "evaluate(smlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество модели увеличилось почти на 15%! Хотя до большой сети все еще немного не дотягивает. \n",
    "\n",
    "Перебирая параметры можно пытаться получить качество еще лучше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 [0/60000 (0%)]\tLoss: 18.237700\t Accuracy:9.375%\n",
      "Epoch : 0 [6400/60000 (11%)]\tLoss: 0.973386\t Accuracy:18.517%\n",
      "Epoch : 0 [12800/60000 (21%)]\tLoss: 1.083604\t Accuracy:21.758%\n",
      "Epoch : 0 [19200/60000 (32%)]\tLoss: 0.913332\t Accuracy:26.711%\n",
      "Epoch : 0 [25600/60000 (43%)]\tLoss: 0.679522\t Accuracy:30.271%\n",
      "Epoch : 0 [32000/60000 (53%)]\tLoss: 0.789727\t Accuracy:32.455%\n",
      "Epoch : 0 [38400/60000 (64%)]\tLoss: 1.107024\t Accuracy:34.825%\n",
      "Epoch : 0 [44800/60000 (75%)]\tLoss: 0.658259\t Accuracy:36.708%\n",
      "Epoch : 0 [51200/60000 (85%)]\tLoss: 0.556574\t Accuracy:38.349%\n",
      "Epoch : 0 [57600/60000 (96%)]\tLoss: 0.552491\t Accuracy:39.764%\n",
      "Epoch : 1 [0/60000 (0%)]\tLoss: 0.653471\t Accuracy:59.375%\n",
      "Epoch : 1 [6400/60000 (11%)]\tLoss: 0.728044\t Accuracy:52.938%\n",
      "Epoch : 1 [12800/60000 (21%)]\tLoss: 0.763170\t Accuracy:54.317%\n",
      "Epoch : 1 [19200/60000 (32%)]\tLoss: 0.482753\t Accuracy:54.591%\n",
      "Epoch : 1 [25600/60000 (43%)]\tLoss: 0.554149\t Accuracy:55.126%\n",
      "Epoch : 1 [32000/60000 (53%)]\tLoss: 0.555983\t Accuracy:55.376%\n",
      "Epoch : 1 [38400/60000 (64%)]\tLoss: 0.562835\t Accuracy:55.847%\n",
      "Epoch : 1 [44800/60000 (75%)]\tLoss: 0.598392\t Accuracy:56.165%\n",
      "Epoch : 1 [51200/60000 (85%)]\tLoss: 0.462550\t Accuracy:56.416%\n",
      "Epoch : 1 [57600/60000 (96%)]\tLoss: 0.390540\t Accuracy:56.500%\n",
      "Epoch : 2 [0/60000 (0%)]\tLoss: 0.504578\t Accuracy:65.625%\n",
      "Epoch : 2 [6400/60000 (11%)]\tLoss: 0.536565\t Accuracy:57.447%\n",
      "Epoch : 2 [12800/60000 (21%)]\tLoss: 0.785148\t Accuracy:58.455%\n",
      "Epoch : 2 [19200/60000 (32%)]\tLoss: 0.404209\t Accuracy:58.496%\n",
      "Epoch : 2 [25600/60000 (43%)]\tLoss: 0.492473\t Accuracy:58.661%\n",
      "Epoch : 2 [32000/60000 (53%)]\tLoss: 0.576901\t Accuracy:58.944%\n",
      "Epoch : 2 [38400/60000 (64%)]\tLoss: 0.408108\t Accuracy:59.867%\n",
      "Epoch : 2 [44800/60000 (75%)]\tLoss: 0.441711\t Accuracy:60.611%\n",
      "Epoch : 2 [51200/60000 (85%)]\tLoss: 0.264188\t Accuracy:61.290%\n",
      "Epoch : 2 [57600/60000 (96%)]\tLoss: 0.298185\t Accuracy:62.205%\n",
      "Epoch : 3 [0/60000 (0%)]\tLoss: 0.408487\t Accuracy:65.625%\n",
      "Epoch : 3 [6400/60000 (11%)]\tLoss: 0.346104\t Accuracy:72.948%\n",
      "Epoch : 3 [12800/60000 (21%)]\tLoss: 0.642566\t Accuracy:73.379%\n",
      "Epoch : 3 [19200/60000 (32%)]\tLoss: 0.282369\t Accuracy:73.596%\n",
      "Epoch : 3 [25600/60000 (43%)]\tLoss: 0.331098\t Accuracy:73.947%\n",
      "Epoch : 3 [32000/60000 (53%)]\tLoss: 0.468232\t Accuracy:73.892%\n",
      "Epoch : 3 [38400/60000 (64%)]\tLoss: 0.284256\t Accuracy:74.040%\n",
      "Epoch : 3 [44800/60000 (75%)]\tLoss: 0.352014\t Accuracy:74.164%\n",
      "Epoch : 3 [51200/60000 (85%)]\tLoss: 0.222649\t Accuracy:74.350%\n",
      "Epoch : 3 [57600/60000 (96%)]\tLoss: 0.237457\t Accuracy:74.346%\n",
      "Epoch : 4 [0/60000 (0%)]\tLoss: 0.387972\t Accuracy:65.625%\n",
      "Epoch : 4 [6400/60000 (11%)]\tLoss: 0.332110\t Accuracy:75.529%\n",
      "Epoch : 4 [12800/60000 (21%)]\tLoss: 0.592508\t Accuracy:75.483%\n",
      "Epoch : 4 [19200/60000 (32%)]\tLoss: 0.286664\t Accuracy:75.770%\n",
      "Epoch : 4 [25600/60000 (43%)]\tLoss: 0.287513\t Accuracy:75.780%\n",
      "Epoch : 4 [32000/60000 (53%)]\tLoss: 0.415883\t Accuracy:75.659%\n",
      "Epoch : 4 [38400/60000 (64%)]\tLoss: 0.248189\t Accuracy:75.726%\n",
      "Epoch : 4 [44800/60000 (75%)]\tLoss: 0.315903\t Accuracy:75.796%\n",
      "Epoch : 4 [51200/60000 (85%)]\tLoss: 0.204783\t Accuracy:75.931%\n",
      "Epoch : 4 [57600/60000 (96%)]\tLoss: 0.163742\t Accuracy:75.786%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "smlp = StudentMLP()\n",
    "distill(mlp, smlp, train_loader, temperature=5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.765% \n"
     ]
    }
   ],
   "source": [
    "evaluate(smlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 [0/60000 (0%)]\tLoss: 18.807398\t Accuracy:9.375%\n",
      "Epoch : 0 [6400/60000 (11%)]\tLoss: 1.029747\t Accuracy:21.393%\n",
      "Epoch : 0 [12800/60000 (21%)]\tLoss: 1.008816\t Accuracy:24.096%\n",
      "Epoch : 0 [19200/60000 (32%)]\tLoss: 0.854174\t Accuracy:26.076%\n",
      "Epoch : 0 [25600/60000 (43%)]\tLoss: 0.772763\t Accuracy:28.043%\n",
      "Epoch : 0 [32000/60000 (53%)]\tLoss: 0.914996\t Accuracy:29.558%\n",
      "Epoch : 0 [38400/60000 (64%)]\tLoss: 1.309228\t Accuracy:31.419%\n",
      "Epoch : 0 [44800/60000 (75%)]\tLoss: 0.692897\t Accuracy:32.698%\n",
      "Epoch : 0 [51200/60000 (85%)]\tLoss: 0.718221\t Accuracy:33.930%\n",
      "Epoch : 0 [57600/60000 (96%)]\tLoss: 0.631064\t Accuracy:34.934%\n",
      "Epoch : 1 [0/60000 (0%)]\tLoss: 0.741131\t Accuracy:53.125%\n",
      "Epoch : 1 [6400/60000 (11%)]\tLoss: 0.753406\t Accuracy:45.989%\n",
      "Epoch : 1 [12800/60000 (21%)]\tLoss: 0.802172\t Accuracy:46.922%\n",
      "Epoch : 1 [19200/60000 (32%)]\tLoss: 0.566999\t Accuracy:48.284%\n",
      "Epoch : 1 [25600/60000 (43%)]\tLoss: 0.599084\t Accuracy:49.224%\n",
      "Epoch : 1 [32000/60000 (53%)]\tLoss: 0.639231\t Accuracy:49.348%\n",
      "Epoch : 1 [38400/60000 (64%)]\tLoss: 0.776577\t Accuracy:49.763%\n",
      "Epoch : 1 [44800/60000 (75%)]\tLoss: 0.533466\t Accuracy:50.038%\n",
      "Epoch : 1 [51200/60000 (85%)]\tLoss: 0.594709\t Accuracy:50.812%\n",
      "Epoch : 1 [57600/60000 (96%)]\tLoss: 0.417412\t Accuracy:51.555%\n",
      "Epoch : 2 [0/60000 (0%)]\tLoss: 0.534043\t Accuracy:65.625%\n",
      "Epoch : 2 [6400/60000 (11%)]\tLoss: 0.589834\t Accuracy:58.271%\n",
      "Epoch : 2 [12800/60000 (21%)]\tLoss: 0.708708\t Accuracy:58.720%\n",
      "Epoch : 2 [19200/60000 (32%)]\tLoss: 0.374961\t Accuracy:58.777%\n",
      "Epoch : 2 [25600/60000 (43%)]\tLoss: 0.541197\t Accuracy:58.903%\n",
      "Epoch : 2 [32000/60000 (53%)]\tLoss: 0.483617\t Accuracy:58.929%\n",
      "Epoch : 2 [38400/60000 (64%)]\tLoss: 0.580947\t Accuracy:59.104%\n",
      "Epoch : 2 [44800/60000 (75%)]\tLoss: 0.482395\t Accuracy:59.190%\n",
      "Epoch : 2 [51200/60000 (85%)]\tLoss: 0.456048\t Accuracy:59.242%\n",
      "Epoch : 2 [57600/60000 (96%)]\tLoss: 0.382235\t Accuracy:59.115%\n",
      "Epoch : 3 [0/60000 (0%)]\tLoss: 0.441396\t Accuracy:65.625%\n",
      "Epoch : 3 [6400/60000 (11%)]\tLoss: 0.514737\t Accuracy:59.810%\n",
      "Epoch : 3 [12800/60000 (21%)]\tLoss: 0.679386\t Accuracy:60.380%\n",
      "Epoch : 3 [19200/60000 (32%)]\tLoss: 0.339566\t Accuracy:60.956%\n",
      "Epoch : 3 [25600/60000 (43%)]\tLoss: 0.469344\t Accuracy:62.008%\n",
      "Epoch : 3 [32000/60000 (53%)]\tLoss: 0.462144\t Accuracy:63.118%\n",
      "Epoch : 3 [38400/60000 (64%)]\tLoss: 0.316459\t Accuracy:63.988%\n",
      "Epoch : 3 [44800/60000 (75%)]\tLoss: 0.395523\t Accuracy:64.793%\n",
      "Epoch : 3 [51200/60000 (85%)]\tLoss: 0.313539\t Accuracy:65.432%\n",
      "Epoch : 3 [57600/60000 (96%)]\tLoss: 0.290050\t Accuracy:65.972%\n",
      "Epoch : 4 [0/60000 (0%)]\tLoss: 0.382496\t Accuracy:56.250%\n",
      "Epoch : 4 [6400/60000 (11%)]\tLoss: 0.419162\t Accuracy:70.414%\n",
      "Epoch : 4 [12800/60000 (21%)]\tLoss: 0.540615\t Accuracy:70.340%\n",
      "Epoch : 4 [19200/60000 (32%)]\tLoss: 0.252920\t Accuracy:70.591%\n",
      "Epoch : 4 [25600/60000 (43%)]\tLoss: 0.413064\t Accuracy:70.915%\n",
      "Epoch : 4 [32000/60000 (53%)]\tLoss: 0.352745\t Accuracy:70.954%\n",
      "Epoch : 4 [38400/60000 (64%)]\tLoss: 0.293308\t Accuracy:70.884%\n",
      "Epoch : 4 [44800/60000 (75%)]\tLoss: 0.443041\t Accuracy:70.983%\n",
      "Epoch : 4 [51200/60000 (85%)]\tLoss: 0.314340\t Accuracy:71.034%\n",
      "Epoch : 4 [57600/60000 (96%)]\tLoss: 0.259581\t Accuracy:71.124%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "smlp = StudentMLP()\n",
    "distill(mlp, smlp, train_loader, temperature=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.718% \n"
     ]
    }
   ],
   "source": [
    "evaluate(smlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 [0/60000 (0%)]\tLoss: 19.113735\t Accuracy:9.375%\n",
      "Epoch : 0 [6400/60000 (11%)]\tLoss: 1.088714\t Accuracy:18.626%\n",
      "Epoch : 0 [12800/60000 (21%)]\tLoss: 1.027413\t Accuracy:20.940%\n",
      "Epoch : 0 [19200/60000 (32%)]\tLoss: 0.979876\t Accuracy:22.062%\n",
      "Epoch : 0 [25600/60000 (43%)]\tLoss: 0.832493\t Accuracy:23.763%\n",
      "Epoch : 0 [32000/60000 (53%)]\tLoss: 0.939549\t Accuracy:24.632%\n",
      "Epoch : 0 [38400/60000 (64%)]\tLoss: 1.208642\t Accuracy:26.517%\n",
      "Epoch : 0 [44800/60000 (75%)]\tLoss: 0.693540\t Accuracy:27.955%\n",
      "Epoch : 0 [51200/60000 (85%)]\tLoss: 0.701821\t Accuracy:29.780%\n",
      "Epoch : 0 [57600/60000 (96%)]\tLoss: 0.634311\t Accuracy:31.203%\n",
      "Epoch : 1 [0/60000 (0%)]\tLoss: 0.766185\t Accuracy:34.375%\n",
      "Epoch : 1 [6400/60000 (11%)]\tLoss: 0.766767\t Accuracy:43.766%\n",
      "Epoch : 1 [12800/60000 (21%)]\tLoss: 1.017757\t Accuracy:43.875%\n",
      "Epoch : 1 [19200/60000 (32%)]\tLoss: 0.648921\t Accuracy:43.734%\n",
      "Epoch : 1 [25600/60000 (43%)]\tLoss: 0.623931\t Accuracy:44.245%\n",
      "Epoch : 1 [32000/60000 (53%)]\tLoss: 0.683474\t Accuracy:44.150%\n",
      "Epoch : 1 [38400/60000 (64%)]\tLoss: 0.686766\t Accuracy:44.372%\n",
      "Epoch : 1 [44800/60000 (75%)]\tLoss: 0.621797\t Accuracy:44.370%\n",
      "Epoch : 1 [51200/60000 (85%)]\tLoss: 0.613578\t Accuracy:44.418%\n",
      "Epoch : 1 [57600/60000 (96%)]\tLoss: 0.566525\t Accuracy:44.434%\n",
      "Epoch : 2 [0/60000 (0%)]\tLoss: 0.766421\t Accuracy:31.250%\n",
      "Epoch : 2 [6400/60000 (11%)]\tLoss: 0.709590\t Accuracy:44.823%\n",
      "Epoch : 2 [12800/60000 (21%)]\tLoss: 1.007651\t Accuracy:45.340%\n",
      "Epoch : 2 [19200/60000 (32%)]\tLoss: 0.656016\t Accuracy:45.544%\n",
      "Epoch : 2 [25600/60000 (43%)]\tLoss: 0.611650\t Accuracy:46.618%\n",
      "Epoch : 2 [32000/60000 (53%)]\tLoss: 0.613471\t Accuracy:47.140%\n",
      "Epoch : 2 [38400/60000 (64%)]\tLoss: 0.617596\t Accuracy:48.293%\n",
      "Epoch : 2 [44800/60000 (75%)]\tLoss: 0.550123\t Accuracy:48.932%\n",
      "Epoch : 2 [51200/60000 (85%)]\tLoss: 0.514397\t Accuracy:49.493%\n",
      "Epoch : 2 [57600/60000 (96%)]\tLoss: 0.466993\t Accuracy:49.853%\n",
      "Epoch : 3 [0/60000 (0%)]\tLoss: 0.662479\t Accuracy:43.750%\n",
      "Epoch : 3 [6400/60000 (11%)]\tLoss: 0.714922\t Accuracy:54.089%\n",
      "Epoch : 3 [12800/60000 (21%)]\tLoss: 0.852117\t Accuracy:54.317%\n",
      "Epoch : 3 [19200/60000 (32%)]\tLoss: 0.512686\t Accuracy:54.025%\n",
      "Epoch : 3 [25600/60000 (43%)]\tLoss: 0.498418\t Accuracy:54.405%\n",
      "Epoch : 3 [32000/60000 (53%)]\tLoss: 0.568909\t Accuracy:54.249%\n",
      "Epoch : 3 [38400/60000 (64%)]\tLoss: 0.597784\t Accuracy:54.390%\n",
      "Epoch : 3 [44800/60000 (75%)]\tLoss: 0.452367\t Accuracy:54.619%\n",
      "Epoch : 3 [51200/60000 (85%)]\tLoss: 0.441858\t Accuracy:54.821%\n",
      "Epoch : 3 [57600/60000 (96%)]\tLoss: 0.440267\t Accuracy:54.895%\n",
      "Epoch : 4 [0/60000 (0%)]\tLoss: 0.603873\t Accuracy:37.500%\n",
      "Epoch : 4 [6400/60000 (11%)]\tLoss: 0.648780\t Accuracy:56.514%\n",
      "Epoch : 4 [12800/60000 (21%)]\tLoss: 0.790738\t Accuracy:57.325%\n",
      "Epoch : 4 [19200/60000 (32%)]\tLoss: 0.466781\t Accuracy:57.763%\n",
      "Epoch : 4 [25600/60000 (43%)]\tLoss: 0.513858\t Accuracy:57.963%\n",
      "Epoch : 4 [32000/60000 (53%)]\tLoss: 0.494195\t Accuracy:57.786%\n",
      "Epoch : 4 [38400/60000 (64%)]\tLoss: 0.568906\t Accuracy:57.783%\n",
      "Epoch : 4 [44800/60000 (75%)]\tLoss: 0.438708\t Accuracy:58.117%\n",
      "Epoch : 4 [51200/60000 (85%)]\tLoss: 0.400430\t Accuracy:58.422%\n",
      "Epoch : 4 [57600/60000 (96%)]\tLoss: 0.408588\t Accuracy:58.601%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "smlp = StudentMLP()\n",
    "distill(mlp, smlp, train_loader, temperature=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.576% \n"
     ]
    }
   ],
   "source": [
    "evaluate(smlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также можем попробовать изменить веса для слагаемых в ошибке.\n",
    "Возьмем модель с T = 5 и попробуем изменением весов ее улучшить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 [0/60000 (0%)]\tLoss: 28.813070\t Accuracy:9.375%\n",
      "Epoch : 0 [6400/60000 (11%)]\tLoss: 1.663249\t Accuracy:21.797%\n",
      "Epoch : 0 [12800/60000 (21%)]\tLoss: 1.506650\t Accuracy:26.675%\n",
      "Epoch : 0 [19200/60000 (32%)]\tLoss: 1.278597\t Accuracy:30.891%\n",
      "Epoch : 0 [25600/60000 (43%)]\tLoss: 1.045085\t Accuracy:33.630%\n",
      "Epoch : 0 [32000/60000 (53%)]\tLoss: 1.188174\t Accuracy:35.589%\n",
      "Epoch : 0 [38400/60000 (64%)]\tLoss: 2.018281\t Accuracy:37.940%\n",
      "Epoch : 0 [44800/60000 (75%)]\tLoss: 1.068962\t Accuracy:39.425%\n",
      "Epoch : 0 [51200/60000 (85%)]\tLoss: 0.856626\t Accuracy:40.748%\n",
      "Epoch : 0 [57600/60000 (96%)]\tLoss: 0.808763\t Accuracy:41.850%\n",
      "Epoch : 1 [0/60000 (0%)]\tLoss: 0.870942\t Accuracy:65.625%\n",
      "Epoch : 1 [6400/60000 (11%)]\tLoss: 1.193262\t Accuracy:52.752%\n",
      "Epoch : 1 [12800/60000 (21%)]\tLoss: 1.201379\t Accuracy:53.647%\n",
      "Epoch : 1 [19200/60000 (32%)]\tLoss: 0.740859\t Accuracy:54.435%\n",
      "Epoch : 1 [25600/60000 (43%)]\tLoss: 0.811783\t Accuracy:54.877%\n",
      "Epoch : 1 [32000/60000 (53%)]\tLoss: 0.841314\t Accuracy:55.176%\n",
      "Epoch : 1 [38400/60000 (64%)]\tLoss: 0.928411\t Accuracy:55.573%\n",
      "Epoch : 1 [44800/60000 (75%)]\tLoss: 0.926705\t Accuracy:55.860%\n",
      "Epoch : 1 [51200/60000 (85%)]\tLoss: 0.698444\t Accuracy:56.201%\n",
      "Epoch : 1 [57600/60000 (96%)]\tLoss: 0.628376\t Accuracy:56.297%\n",
      "Epoch : 2 [0/60000 (0%)]\tLoss: 0.783548\t Accuracy:65.625%\n",
      "Epoch : 2 [6400/60000 (11%)]\tLoss: 0.838629\t Accuracy:57.618%\n",
      "Epoch : 2 [12800/60000 (21%)]\tLoss: 1.106815\t Accuracy:59.500%\n",
      "Epoch : 2 [19200/60000 (32%)]\tLoss: 0.700526\t Accuracy:61.210%\n",
      "Epoch : 2 [25600/60000 (43%)]\tLoss: 0.728684\t Accuracy:62.399%\n",
      "Epoch : 2 [32000/60000 (53%)]\tLoss: 0.795604\t Accuracy:62.628%\n",
      "Epoch : 2 [38400/60000 (64%)]\tLoss: 0.662509\t Accuracy:63.525%\n",
      "Epoch : 2 [44800/60000 (75%)]\tLoss: 0.719456\t Accuracy:64.173%\n",
      "Epoch : 2 [51200/60000 (85%)]\tLoss: 0.402946\t Accuracy:64.831%\n",
      "Epoch : 2 [57600/60000 (96%)]\tLoss: 0.630952\t Accuracy:65.311%\n",
      "Epoch : 3 [0/60000 (0%)]\tLoss: 0.616608\t Accuracy:53.125%\n",
      "Epoch : 3 [6400/60000 (11%)]\tLoss: 0.624313\t Accuracy:69.434%\n",
      "Epoch : 3 [12800/60000 (21%)]\tLoss: 0.861793\t Accuracy:69.802%\n",
      "Epoch : 3 [19200/60000 (32%)]\tLoss: 0.421276\t Accuracy:69.910%\n",
      "Epoch : 3 [25600/60000 (43%)]\tLoss: 0.562684\t Accuracy:70.194%\n",
      "Epoch : 3 [32000/60000 (53%)]\tLoss: 0.778577\t Accuracy:69.840%\n",
      "Epoch : 3 [38400/60000 (64%)]\tLoss: 0.571401\t Accuracy:69.775%\n",
      "Epoch : 3 [44800/60000 (75%)]\tLoss: 0.585106\t Accuracy:69.990%\n",
      "Epoch : 3 [51200/60000 (85%)]\tLoss: 0.358057\t Accuracy:70.105%\n",
      "Epoch : 3 [57600/60000 (96%)]\tLoss: 0.394737\t Accuracy:70.150%\n",
      "Epoch : 4 [0/60000 (0%)]\tLoss: 0.626922\t Accuracy:59.375%\n",
      "Epoch : 4 [6400/60000 (11%)]\tLoss: 0.658308\t Accuracy:70.802%\n",
      "Epoch : 4 [12800/60000 (21%)]\tLoss: 0.765742\t Accuracy:71.314%\n",
      "Epoch : 4 [19200/60000 (32%)]\tLoss: 0.361865\t Accuracy:71.345%\n",
      "Epoch : 4 [25600/60000 (43%)]\tLoss: 0.530483\t Accuracy:71.419%\n",
      "Epoch : 4 [32000/60000 (53%)]\tLoss: 0.695222\t Accuracy:71.035%\n",
      "Epoch : 4 [38400/60000 (64%)]\tLoss: 0.458204\t Accuracy:70.917%\n",
      "Epoch : 4 [44800/60000 (75%)]\tLoss: 0.580970\t Accuracy:71.199%\n",
      "Epoch : 4 [51200/60000 (85%)]\tLoss: 0.349195\t Accuracy:71.266%\n",
      "Epoch : 4 [57600/60000 (96%)]\tLoss: 0.385422\t Accuracy:71.275%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "smlp = StudentMLP()\n",
    "distill(mlp, smlp, train_loader, temperature=5.0, alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.710% \n"
     ]
    }
   ],
   "source": [
    "evaluate(smlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 [0/60000 (0%)]\tLoss: 4.137208\t Accuracy:9.375%\n",
      "Epoch : 0 [6400/60000 (11%)]\tLoss: 0.231639\t Accuracy:19.885%\n",
      "Epoch : 0 [12800/60000 (21%)]\tLoss: 0.226190\t Accuracy:25.553%\n",
      "Epoch : 0 [19200/60000 (32%)]\tLoss: 0.209725\t Accuracy:28.063%\n",
      "Epoch : 0 [25600/60000 (43%)]\tLoss: 0.185707\t Accuracy:31.164%\n",
      "Epoch : 0 [32000/60000 (53%)]\tLoss: 0.201776\t Accuracy:33.348%\n",
      "Epoch : 0 [38400/60000 (64%)]\tLoss: 0.190182\t Accuracy:35.681%\n",
      "Epoch : 0 [44800/60000 (75%)]\tLoss: 0.149032\t Accuracy:37.734%\n",
      "Epoch : 0 [51200/60000 (85%)]\tLoss: 0.126203\t Accuracy:39.616%\n",
      "Epoch : 0 [57600/60000 (96%)]\tLoss: 0.117845\t Accuracy:41.654%\n",
      "Epoch : 1 [0/60000 (0%)]\tLoss: 0.118269\t Accuracy:68.750%\n",
      "Epoch : 1 [6400/60000 (11%)]\tLoss: 0.145554\t Accuracy:60.759%\n",
      "Epoch : 1 [12800/60000 (21%)]\tLoss: 0.192144\t Accuracy:62.718%\n",
      "Epoch : 1 [19200/60000 (32%)]\tLoss: 0.080345\t Accuracy:63.472%\n",
      "Epoch : 1 [25600/60000 (43%)]\tLoss: 0.095022\t Accuracy:63.955%\n",
      "Epoch : 1 [32000/60000 (53%)]\tLoss: 0.093581\t Accuracy:64.351%\n",
      "Epoch : 1 [38400/60000 (64%)]\tLoss: 0.082338\t Accuracy:64.948%\n",
      "Epoch : 1 [44800/60000 (75%)]\tLoss: 0.087700\t Accuracy:65.774%\n",
      "Epoch : 1 [51200/60000 (85%)]\tLoss: 0.075883\t Accuracy:66.464%\n",
      "Epoch : 1 [57600/60000 (96%)]\tLoss: 0.058990\t Accuracy:67.056%\n",
      "Epoch : 2 [0/60000 (0%)]\tLoss: 0.088551\t Accuracy:65.625%\n",
      "Epoch : 2 [6400/60000 (11%)]\tLoss: 0.062375\t Accuracy:72.295%\n",
      "Epoch : 2 [12800/60000 (21%)]\tLoss: 0.131630\t Accuracy:72.257%\n",
      "Epoch : 2 [19200/60000 (32%)]\tLoss: 0.058842\t Accuracy:72.645%\n",
      "Epoch : 2 [25600/60000 (43%)]\tLoss: 0.077669\t Accuracy:72.819%\n",
      "Epoch : 2 [32000/60000 (53%)]\tLoss: 0.088063\t Accuracy:72.865%\n",
      "Epoch : 2 [38400/60000 (64%)]\tLoss: 0.073031\t Accuracy:72.796%\n",
      "Epoch : 2 [44800/60000 (75%)]\tLoss: 0.094738\t Accuracy:72.943%\n",
      "Epoch : 2 [51200/60000 (85%)]\tLoss: 0.069978\t Accuracy:72.991%\n",
      "Epoch : 2 [57600/60000 (96%)]\tLoss: 0.048257\t Accuracy:73.081%\n",
      "Epoch : 3 [0/60000 (0%)]\tLoss: 0.070212\t Accuracy:65.625%\n",
      "Epoch : 3 [6400/60000 (11%)]\tLoss: 0.058140\t Accuracy:74.689%\n",
      "Epoch : 3 [12800/60000 (21%)]\tLoss: 0.120410\t Accuracy:74.353%\n",
      "Epoch : 3 [19200/60000 (32%)]\tLoss: 0.055320\t Accuracy:74.470%\n",
      "Epoch : 3 [25600/60000 (43%)]\tLoss: 0.071735\t Accuracy:74.571%\n",
      "Epoch : 3 [32000/60000 (53%)]\tLoss: 0.079634\t Accuracy:74.454%\n",
      "Epoch : 3 [38400/60000 (64%)]\tLoss: 0.057941\t Accuracy:74.279%\n",
      "Epoch : 3 [44800/60000 (75%)]\tLoss: 0.091507\t Accuracy:74.525%\n",
      "Epoch : 3 [51200/60000 (85%)]\tLoss: 0.062992\t Accuracy:74.957%\n",
      "Epoch : 3 [57600/60000 (96%)]\tLoss: 0.042328\t Accuracy:75.031%\n",
      "Epoch : 4 [0/60000 (0%)]\tLoss: 0.061573\t Accuracy:84.375%\n",
      "Epoch : 4 [6400/60000 (11%)]\tLoss: 0.079543\t Accuracy:77.767%\n",
      "Epoch : 4 [12800/60000 (21%)]\tLoss: 0.100993\t Accuracy:77.681%\n",
      "Epoch : 4 [19200/60000 (32%)]\tLoss: 0.043108\t Accuracy:77.647%\n",
      "Epoch : 4 [25600/60000 (43%)]\tLoss: 0.070574\t Accuracy:77.723%\n",
      "Epoch : 4 [32000/60000 (53%)]\tLoss: 0.075969\t Accuracy:77.566%\n",
      "Epoch : 4 [38400/60000 (64%)]\tLoss: 0.055659\t Accuracy:77.581%\n",
      "Epoch : 4 [44800/60000 (75%)]\tLoss: 0.078140\t Accuracy:77.737%\n",
      "Epoch : 4 [51200/60000 (85%)]\tLoss: 0.050270\t Accuracy:77.848%\n",
      "Epoch : 4 [57600/60000 (96%)]\tLoss: 0.043416\t Accuracy:77.849%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "smlp = StudentMLP()\n",
    "distill(mlp, smlp, train_loader, temperature=5, alpha=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.786% \n"
     ]
    }
   ],
   "source": [
    "evaluate(smlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что обучение чувствительно к тому, на что должна смотреть модель - на правильный ответ или на \"интуицию\" большой модели.\n",
    "\n",
    "В итоге, у нас получилось обучить модель в 20 раз меньше оригинальной при этом не слишком сильно потерять в качестве."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Квантизация нейронных сетей\n",
    "\n",
    "В этой лабораторной попробуем уменьшить размер нейронной сети, а также усторить ее работу, заменяя float на int внутри сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import copy\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.quantization import QuantStub, DeQuantStub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x133dbccd0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED=9876\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вновь пробуем обучить сеть из трех полносвязных слоев на fashion mnist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv(\"fashion-mnist_train.csv\")\n",
    "test_csv = pd.read_csv(\"fashion-mnist_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      2       0       0       0       0       0       0       0       0   \n",
       "1      9       0       0       0       0       0       0       0       0   \n",
       "2      6       0       0       0       0       0       0       0       5   \n",
       "3      0       0       0       0       1       2       0       0       0   \n",
       "4      3       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0        30        43         0   \n",
       "3       0  ...         3         0         0         0         0         1   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_csv['label'].values\n",
    "X_train = train_csv.drop(['label'],axis=1).values\n",
    "\n",
    "y_test = test_csv['label'].values\n",
    "X_test = test_csv.drop(['label'],axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14a8932b0>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[3].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14a95cd90>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[4].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "torch_X_train = torch.from_numpy(X_train).type(torch.LongTensor)\n",
    "torch_y_train = torch.from_numpy(y_train).type(torch.LongTensor)\n",
    "torch_X_test = torch.from_numpy(X_test).type(torch.LongTensor)\n",
    "torch_y_test = torch.from_numpy(y_test).type(torch.LongTensor)\n",
    "\n",
    "train = torch.utils.data.TensorDataset(torch_X_train,torch_y_train)\n",
    "test = torch.utils.data.TensorDataset(torch_X_test,torch_y_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size = BATCH_SIZE, shuffle = False)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size = BATCH_SIZE, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (linear1): Linear(in_features=784, out_features=250, bias=True)\n",
      "  (linear2): Linear(in_features=250, out_features=100, bias=True)\n",
      "  (linear3): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(784,250)\n",
    "        self.linear2 = nn.Linear(250,100)\n",
    "        self.linear3 = nn.Linear(100,10)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        X = F.relu(self.linear1(X))\n",
    "        X = F.relu(self.linear2(X))\n",
    "        X = self.linear3(X)\n",
    "        return X\n",
    "\n",
    "mlp = MLP()\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, train_loader, epoch_number=5):\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    error = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epoch_number):\n",
    "        correct = 0\n",
    "        \n",
    "        for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "            var_X_batch = Variable(X_batch).float()\n",
    "            var_y_batch = Variable(y_batch)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(var_X_batch)\n",
    "            loss = error(output, var_y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            predicted = torch.max(output.data, 1)[1] \n",
    "            correct += (predicted == var_y_batch).sum()\n",
    "            if batch_idx % 200 == 0:\n",
    "                print('Epoch : {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Accuracy:{:.3f}%'.format(\n",
    "                    epoch, batch_idx*len(X_batch), len(train_loader.dataset), 100.*batch_idx / len(train_loader), loss.data, float(correct*100) / float(BATCH_SIZE*(batch_idx+1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader=test_loader):\n",
    "    correct = 0 \n",
    "    for test_imgs, test_labels in loader:\n",
    "        test_imgs = Variable(test_imgs).float()\n",
    "        \n",
    "        output = model(test_imgs)\n",
    "        predicted = torch.max(output,1)[1]\n",
    "        correct += (predicted == test_labels).sum()\n",
    "    print(\"Test accuracy:{:.3f}% \".format( float(correct) / (len(loader)*BATCH_SIZE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 [0/60000 (0%)]\tLoss: 17.452320\t Accuracy:3.125%\n",
      "Epoch : 0 [6400/60000 (11%)]\tLoss: 0.777609\t Accuracy:69.045%\n",
      "Epoch : 0 [12800/60000 (21%)]\tLoss: 0.971888\t Accuracy:72.958%\n",
      "Epoch : 0 [19200/60000 (32%)]\tLoss: 0.423758\t Accuracy:75.078%\n",
      "Epoch : 0 [25600/60000 (43%)]\tLoss: 0.300038\t Accuracy:76.381%\n",
      "Epoch : 0 [32000/60000 (53%)]\tLoss: 0.521944\t Accuracy:77.301%\n",
      "Epoch : 0 [38400/60000 (64%)]\tLoss: 0.464210\t Accuracy:78.047%\n",
      "Epoch : 0 [44800/60000 (75%)]\tLoss: 0.300150\t Accuracy:78.589%\n",
      "Epoch : 0 [51200/60000 (85%)]\tLoss: 0.275364\t Accuracy:79.042%\n",
      "Epoch : 0 [57600/60000 (96%)]\tLoss: 0.241909\t Accuracy:79.402%\n",
      "Epoch : 1 [0/60000 (0%)]\tLoss: 0.488709\t Accuracy:84.375%\n",
      "Epoch : 1 [6400/60000 (11%)]\tLoss: 0.477927\t Accuracy:83.116%\n",
      "Epoch : 1 [12800/60000 (21%)]\tLoss: 0.779009\t Accuracy:83.588%\n",
      "Epoch : 1 [19200/60000 (32%)]\tLoss: 0.315745\t Accuracy:83.709%\n",
      "Epoch : 1 [25600/60000 (43%)]\tLoss: 0.278790\t Accuracy:83.813%\n",
      "Epoch : 1 [32000/60000 (53%)]\tLoss: 0.567111\t Accuracy:83.719%\n",
      "Epoch : 1 [38400/60000 (64%)]\tLoss: 0.415207\t Accuracy:83.857%\n",
      "Epoch : 1 [44800/60000 (75%)]\tLoss: 0.235364\t Accuracy:84.031%\n",
      "Epoch : 1 [51200/60000 (85%)]\tLoss: 0.233183\t Accuracy:84.141%\n",
      "Epoch : 1 [57600/60000 (96%)]\tLoss: 0.192994\t Accuracy:84.160%\n",
      "Epoch : 2 [0/60000 (0%)]\tLoss: 0.436427\t Accuracy:87.500%\n",
      "Epoch : 2 [6400/60000 (11%)]\tLoss: 0.415828\t Accuracy:84.826%\n",
      "Epoch : 2 [12800/60000 (21%)]\tLoss: 0.782725\t Accuracy:84.702%\n",
      "Epoch : 2 [19200/60000 (32%)]\tLoss: 0.212440\t Accuracy:84.853%\n",
      "Epoch : 2 [25600/60000 (43%)]\tLoss: 0.245404\t Accuracy:84.886%\n",
      "Epoch : 2 [32000/60000 (53%)]\tLoss: 0.647482\t Accuracy:84.856%\n",
      "Epoch : 2 [38400/60000 (64%)]\tLoss: 0.663159\t Accuracy:84.916%\n",
      "Epoch : 2 [44800/60000 (75%)]\tLoss: 0.218280\t Accuracy:85.129%\n",
      "Epoch : 2 [51200/60000 (85%)]\tLoss: 0.166506\t Accuracy:85.228%\n",
      "Epoch : 2 [57600/60000 (96%)]\tLoss: 0.118439\t Accuracy:85.158%\n",
      "Epoch : 3 [0/60000 (0%)]\tLoss: 0.363178\t Accuracy:87.500%\n",
      "Epoch : 3 [6400/60000 (11%)]\tLoss: 0.443698\t Accuracy:85.759%\n",
      "Epoch : 3 [12800/60000 (21%)]\tLoss: 0.668413\t Accuracy:85.832%\n",
      "Epoch : 3 [19200/60000 (32%)]\tLoss: 0.230223\t Accuracy:85.727%\n",
      "Epoch : 3 [25600/60000 (43%)]\tLoss: 0.204274\t Accuracy:85.861%\n",
      "Epoch : 3 [32000/60000 (53%)]\tLoss: 0.471792\t Accuracy:85.858%\n",
      "Epoch : 3 [38400/60000 (64%)]\tLoss: 0.484164\t Accuracy:85.884%\n",
      "Epoch : 3 [44800/60000 (75%)]\tLoss: 0.194284\t Accuracy:86.043%\n",
      "Epoch : 3 [51200/60000 (85%)]\tLoss: 0.154474\t Accuracy:86.124%\n",
      "Epoch : 3 [57600/60000 (96%)]\tLoss: 0.142835\t Accuracy:86.055%\n",
      "Epoch : 4 [0/60000 (0%)]\tLoss: 0.355323\t Accuracy:81.250%\n",
      "Epoch : 4 [6400/60000 (11%)]\tLoss: 0.369167\t Accuracy:86.194%\n",
      "Epoch : 4 [12800/60000 (21%)]\tLoss: 0.708802\t Accuracy:86.152%\n",
      "Epoch : 4 [19200/60000 (32%)]\tLoss: 0.169659\t Accuracy:86.361%\n",
      "Epoch : 4 [25600/60000 (43%)]\tLoss: 0.423034\t Accuracy:86.333%\n",
      "Epoch : 4 [32000/60000 (53%)]\tLoss: 0.470072\t Accuracy:86.239%\n",
      "Epoch : 4 [38400/60000 (64%)]\tLoss: 0.454010\t Accuracy:86.285%\n",
      "Epoch : 4 [44800/60000 (75%)]\tLoss: 0.210974\t Accuracy:86.525%\n",
      "Epoch : 4 [51200/60000 (85%)]\tLoss: 0.153560\t Accuracy:86.557%\n",
      "Epoch : 4 [57600/60000 (96%)]\tLoss: 0.131629\t Accuracy:86.504%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "fit(mlp, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.854% \n"
     ]
    }
   ],
   "source": [
    "evaluate(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уменьшать количество самих весов в этот раз мы не будем, поэтому в качестве размера нейронной сети будет использовать буквально количество памяти, которая она занимает."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_size(model):\n",
    "    torch.save(model.state_dict(), \"/tmp/model.p\")\n",
    "    size=os.path.getsize(\"/tmp/model.p\")\n",
    "    os.remove('/tmp/model.p')\n",
    "    return \"{:.3f} KB\".format(size / 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'870.729 KB'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_size(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полученная сеть весит почти мегабайт.\n",
    "\n",
    "Посмотрим на веса, которые используются внутри нашей сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0328,  0.0293,  0.0301,  ..., -0.0090,  0.0263,  0.0303],\n",
       "        [-0.0152,  0.0219, -0.0283,  ...,  0.0117,  0.0584,  0.0334],\n",
       "        [-0.0097, -0.0417,  0.0250,  ..., -0.0420,  0.0145,  0.0303],\n",
       "        ...,\n",
       "        [-0.0203, -0.0009,  0.0205,  ...,  0.0123,  0.0195,  0.0143],\n",
       "        [-0.0187, -0.0666,  0.0168,  ...,  0.0614,  0.0164, -0.0347],\n",
       "        [-0.0037,  0.0156,  0.0021,  ..., -0.0133, -0.0365,  0.0120]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.linear1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (linear1): Linear(in_features=784, out_features=250, bias=True)\n",
       "  (linear2): Linear(in_features=250, out_features=100, bias=True)\n",
       "  (linear3): Linear(in_features=100, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем дополнительно, за сколько в среднем она делает предсказания. Для замеров по времени будем использовать один поток, чтобы все сети были в одинаковых условиях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def single_thread():  \n",
    "    num = torch.get_num_threads()\n",
    "    torch.set_num_threads(1)\n",
    "    yield\n",
    "    torch.set_num_threads(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.864% \n",
      "Test accuracy:0.864% \n",
      "Test accuracy:0.864% \n",
      "Test accuracy:0.864% \n",
      "Test accuracy:0.864% \n",
      "Test accuracy:0.864% \n",
      "Test accuracy:0.864% \n",
      "Test accuracy:0.864% \n",
      "Test accuracy:0.864% \n",
      "Test accuracy:0.864% \n",
      "Test accuracy:0.864% \n",
      "1.43 s ± 9.38 ms per loop (mean ± std. dev. of 10 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r10\n",
    "\n",
    "with single_thread():\n",
    "    evaluate(mlp, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Динамическая квантизация\n",
    "\n",
    "Динамическая квантизация - пожалуй самый простой способ квантизации. \n",
    "\n",
    "Все веса из float мы сразу переводим в int, а вот активации мы пересчитываем на лету во время работы сети. Для каждого примера в нейроне мы аккумулируем взвешенную сумму по-честному во float, подбираем для конкретного получившегося числа лучшие параметры квантизации, квантуем и отправляем дальше по сети.\n",
    "\n",
    "Таким образом, из-за этих автоматических квантований во время работы сети сеть все еще может работать медленно.\n",
    "\n",
    "Попробуем применить динамическую квантизацию к нашей модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "qd_mlp = torch.quantization.quantize_dynamic(\n",
    "    mlp, {nn.Linear}, dtype=torch.qint8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (linear1): DynamicQuantizedLinear(in_features=784, out_features=250, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "  (linear2): DynamicQuantizedLinear(in_features=250, out_features=100, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "  (linear3): DynamicQuantizedLinear(in_features=100, out_features=10, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       ")"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qd_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'222.351 KB'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_size(qd_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что получилось уменьшить размер сети почти в 4 раза. Посмотрим, что случилось с качеством полученной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.863% \n"
     ]
    }
   ],
   "source": [
    "evaluate(qd_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество осталось в точности таким же.\n",
    "\n",
    "Заглянем в то, какие теперь веса используются внутри."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  7,   6,   6,  ...,  -2,   5,   6],\n",
       "        [ -3,   5,  -6,  ...,   2,  12,   7],\n",
       "        [ -2,  -9,   5,  ...,  -9,   3,   6],\n",
       "        ...,\n",
       "        [ -4,   0,   4,  ...,   3,   4,   3],\n",
       "        [ -4, -14,   3,  ...,  13,   3,  -7],\n",
       "        [ -1,   3,   0,  ...,  -3,  -8,   2]], dtype=torch.int8)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qd_mlp.linear1.weight().int_repr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0328,  0.0293,  0.0301,  ..., -0.0090,  0.0263,  0.0303],\n",
       "        [-0.0152,  0.0219, -0.0283,  ...,  0.0117,  0.0584,  0.0334],\n",
       "        [-0.0097, -0.0417,  0.0250,  ..., -0.0420,  0.0145,  0.0303],\n",
       "        ...,\n",
       "        [-0.0203, -0.0009,  0.0205,  ...,  0.0123,  0.0195,  0.0143],\n",
       "        [-0.0187, -0.0666,  0.0168,  ...,  0.0614,  0.0164, -0.0347],\n",
       "        [-0.0037,  0.0156,  0.0021,  ..., -0.0133, -0.0365,  0.0120]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.linear1.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помотрим, насколько быстро получается делать предсказания квантизированной моделью."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.871% \n",
      "Test accuracy:0.871% \n",
      "Test accuracy:0.871% \n",
      "Test accuracy:0.871% \n",
      "Test accuracy:0.871% \n",
      "Test accuracy:0.871% \n",
      "Test accuracy:0.871% \n",
      "Test accuracy:0.871% \n",
      "Test accuracy:0.871% \n",
      "Test accuracy:0.871% \n",
      "Test accuracy:0.871% \n",
      "1.25 s ± 13.3 ms per loop (mean ± std. dev. of 10 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r10\n",
    "\n",
    "with single_thread():\n",
    "    evaluate(qd_mlp, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На моем компьютере получился прирост примерно в 10-15%. Таким образом динамическая квантизация - достаточно простой прием, который не дает большого проигрышал по качеству, при этом уменьшает размер сети и ускоряет предсказания.\n",
    "\n",
    "Однако это не единственный подход для квантования. \n",
    "\n",
    "### Статическая квантизация\n",
    "\n",
    "Статическая квантизация позволяет сразу все операции перевести в int, без необходимости дополнительно что-то расчитывать в процессе предсказания.\n",
    "\n",
    "Для того, чтобы при этом качество не сильно пострадало, параметры квантования для разных слоем настраиваются по обучающей выборке.\n",
    "\n",
    "Таким образом для того, чтобы статически квантизировать сеть, необходимо вначале подключить к ней модуль подсчета параметров (Observer), который будет для каждого слоя расчитывать необходимые параметры квантования по обучающей выборке. После этого один раз необходимо всю выборку прогнать через сеть, чтобы эти модули смогли подсчитать нужные параметры. После чего можно фиксировать полученные параметры и итоговую квантизированную сеть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QuantizedMLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(784,250)\n",
    "        self.linear2 = nn.Linear(250,100)\n",
    "        self.linear3 = nn.Linear(100,10)\n",
    "        # Так как теперь квантизация не происходит динамически, необходимо дополнительно \n",
    "        # руками квантовать входные данные и деквантовать ответ\n",
    "        self.quant = QuantStub()\n",
    "        self.dequant = DeQuantStub()\n",
    "    \n",
    "    def forward(self,X):\n",
    "        # Квантуем входные данные\n",
    "        X = self.quant(X)\n",
    "        X = F.relu(self.linear1(X))\n",
    "        X = F.relu(self.linear2(X))\n",
    "        X = self.linear3(X)\n",
    "        # Деквантуем ответ\n",
    "        X = self.dequant(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptq_mlp = QuantizedMLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 [0/60000 (0%)]\tLoss: 11.684026\t Accuracy:18.750%\n",
      "Epoch : 0 [6400/60000 (11%)]\tLoss: 0.835062\t Accuracy:69.450%\n",
      "Epoch : 0 [12800/60000 (21%)]\tLoss: 0.927683\t Accuracy:74.158%\n",
      "Epoch : 0 [19200/60000 (32%)]\tLoss: 0.419413\t Accuracy:75.499%\n",
      "Epoch : 0 [25600/60000 (43%)]\tLoss: 0.377385\t Accuracy:76.701%\n",
      "Epoch : 0 [32000/60000 (53%)]\tLoss: 0.569878\t Accuracy:77.644%\n",
      "Epoch : 0 [38400/60000 (64%)]\tLoss: 0.492238\t Accuracy:78.427%\n",
      "Epoch : 0 [44800/60000 (75%)]\tLoss: 0.430269\t Accuracy:79.044%\n",
      "Epoch : 0 [51200/60000 (85%)]\tLoss: 0.268351\t Accuracy:79.493%\n",
      "Epoch : 0 [57600/60000 (96%)]\tLoss: 0.265719\t Accuracy:79.846%\n",
      "Epoch : 1 [0/60000 (0%)]\tLoss: 0.743126\t Accuracy:75.000%\n",
      "Epoch : 1 [6400/60000 (11%)]\tLoss: 0.498857\t Accuracy:84.126%\n",
      "Epoch : 1 [12800/60000 (21%)]\tLoss: 0.798827\t Accuracy:84.305%\n",
      "Epoch : 1 [19200/60000 (32%)]\tLoss: 0.250168\t Accuracy:84.354%\n",
      "Epoch : 1 [25600/60000 (43%)]\tLoss: 0.277832\t Accuracy:84.508%\n",
      "Epoch : 1 [32000/60000 (53%)]\tLoss: 0.541766\t Accuracy:84.547%\n",
      "Epoch : 1 [38400/60000 (64%)]\tLoss: 0.423012\t Accuracy:84.516%\n",
      "Epoch : 1 [44800/60000 (75%)]\tLoss: 0.314181\t Accuracy:84.560%\n",
      "Epoch : 1 [51200/60000 (85%)]\tLoss: 0.177072\t Accuracy:84.637%\n",
      "Epoch : 1 [57600/60000 (96%)]\tLoss: 0.156936\t Accuracy:84.644%\n",
      "Epoch : 2 [0/60000 (0%)]\tLoss: 0.383423\t Accuracy:87.500%\n",
      "Epoch : 2 [6400/60000 (11%)]\tLoss: 0.443949\t Accuracy:85.339%\n",
      "Epoch : 2 [12800/60000 (21%)]\tLoss: 0.702213\t Accuracy:85.069%\n",
      "Epoch : 2 [19200/60000 (32%)]\tLoss: 0.300388\t Accuracy:85.108%\n",
      "Epoch : 2 [25600/60000 (43%)]\tLoss: 0.262349\t Accuracy:85.183%\n",
      "Epoch : 2 [32000/60000 (53%)]\tLoss: 0.577158\t Accuracy:85.184%\n",
      "Epoch : 2 [38400/60000 (64%)]\tLoss: 0.349054\t Accuracy:85.184%\n",
      "Epoch : 2 [44800/60000 (75%)]\tLoss: 0.248039\t Accuracy:85.209%\n",
      "Epoch : 2 [51200/60000 (85%)]\tLoss: 0.130468\t Accuracy:85.331%\n",
      "Epoch : 2 [57600/60000 (96%)]\tLoss: 0.224467\t Accuracy:85.326%\n",
      "Epoch : 3 [0/60000 (0%)]\tLoss: 0.348412\t Accuracy:87.500%\n",
      "Epoch : 3 [6400/60000 (11%)]\tLoss: 0.365980\t Accuracy:86.101%\n",
      "Epoch : 3 [12800/60000 (21%)]\tLoss: 0.499289\t Accuracy:85.879%\n",
      "Epoch : 3 [19200/60000 (32%)]\tLoss: 0.293281\t Accuracy:86.013%\n",
      "Epoch : 3 [25600/60000 (43%)]\tLoss: 0.321457\t Accuracy:86.103%\n",
      "Epoch : 3 [32000/60000 (53%)]\tLoss: 0.566689\t Accuracy:86.008%\n",
      "Epoch : 3 [38400/60000 (64%)]\tLoss: 0.363010\t Accuracy:86.045%\n",
      "Epoch : 3 [44800/60000 (75%)]\tLoss: 0.296919\t Accuracy:86.035%\n",
      "Epoch : 3 [51200/60000 (85%)]\tLoss: 0.165712\t Accuracy:86.009%\n",
      "Epoch : 3 [57600/60000 (96%)]\tLoss: 0.173901\t Accuracy:86.020%\n",
      "Epoch : 4 [0/60000 (0%)]\tLoss: 0.389307\t Accuracy:84.375%\n",
      "Epoch : 4 [6400/60000 (11%)]\tLoss: 0.380595\t Accuracy:86.878%\n",
      "Epoch : 4 [12800/60000 (21%)]\tLoss: 0.518744\t Accuracy:86.767%\n",
      "Epoch : 4 [19200/60000 (32%)]\tLoss: 0.243441\t Accuracy:86.710%\n",
      "Epoch : 4 [25600/60000 (43%)]\tLoss: 0.184023\t Accuracy:86.728%\n",
      "Epoch : 4 [32000/60000 (53%)]\tLoss: 0.614309\t Accuracy:86.654%\n",
      "Epoch : 4 [38400/60000 (64%)]\tLoss: 0.329834\t Accuracy:86.673%\n",
      "Epoch : 4 [44800/60000 (75%)]\tLoss: 0.298753\t Accuracy:86.800%\n",
      "Epoch : 4 [51200/60000 (85%)]\tLoss: 0.147100\t Accuracy:86.809%\n",
      "Epoch : 4 [57600/60000 (96%)]\tLoss: 0.182466\t Accuracy:86.837%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "fit(ptq_mlp, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.862% \n"
     ]
    }
   ],
   "source": [
    "evaluate(ptq_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для моделей мы также можем указать конфиг квантования,\n",
    "# где в частности может указать библиотеку для работы с квантованными значениями\n",
    "\n",
    "ptq_mlp.qconfig = torch.quantization.get_default_qconfig('fbgemm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/AlexHome/vms/lsml-internal/venv/lib/python3.9/site-packages/torch/quantization/observer.py:119: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QuantizedMLP(\n",
       "  (linear1): Linear(\n",
       "    in_features=784, out_features=250, bias=True\n",
       "    (activation_post_process): HistogramObserver()\n",
       "  )\n",
       "  (linear2): Linear(\n",
       "    in_features=250, out_features=100, bias=True\n",
       "    (activation_post_process): HistogramObserver()\n",
       "  )\n",
       "  (linear3): Linear(\n",
       "    in_features=100, out_features=10, bias=True\n",
       "    (activation_post_process): HistogramObserver()\n",
       "  )\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): HistogramObserver()\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Устанавливаем модули подсчета параметров квантования. По умолчанию исползуется HistogramObserver, то есть\n",
    "# модуль, который рассчтывает параметры на основе гистрограммы распределения значнеий для конкретного слоя\n",
    "torch.quantization.prepare(ptq_mlp, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.870% \n"
     ]
    }
   ],
   "source": [
    "# Прогоняем всю обучающую выборку через сеть. Для этого просто считаем качество на данных обучающей выборки\n",
    "# Само значение нам не интересно, нам важно, чтобы посчитались параметры\n",
    "evaluate(ptq_mlp, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantizedMLP(\n",
       "  (linear1): QuantizedLinear(in_features=784, out_features=250, scale=46.39590072631836, zero_point=91, qscheme=torch.per_channel_affine)\n",
       "  (linear2): QuantizedLinear(in_features=250, out_features=100, scale=9.414811134338379, zero_point=85, qscheme=torch.per_channel_affine)\n",
       "  (linear3): QuantizedLinear(in_features=100, out_features=10, scale=3.005615472793579, zero_point=93, qscheme=torch.per_channel_affine)\n",
       "  (quant): Quantize(scale=tensor([2.0069]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Фиксируем полученные веса и параметры квантизации\n",
    "torch.quantization.convert(ptq_mlp, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно видеть, какие именно параметры (а именно scale и zero_point) подсчитались для каждого слоя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'229.991 KB'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_size(ptq_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  -6,  -18,  -15,  ...,   51,  -16,  -30],\n",
       "        [  -5,  -33,  -10,  ...,   19,  -31,  -25],\n",
       "        [ -42,  -47,   61,  ...,   23,   68,   57],\n",
       "        ...,\n",
       "        [  35,   -8,  -10,  ...,  -51,   25,  -72],\n",
       "        [ -39,   79,    8,  ...,   58, -102,  -29],\n",
       "        [ -84,   66,   68,  ...,  -35,   67,   86]], dtype=torch.int8)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptq_mlp.linear1.weight().int_repr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.813% \n"
     ]
    }
   ],
   "source": [
    "evaluate(ptq_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что модель все еще хорошо сжалась, при этом качество немного упало. \n",
    "\n",
    "Посмотрим, что по скорости выполнения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.822% \n",
      "Test accuracy:0.822% \n",
      "Test accuracy:0.822% \n",
      "Test accuracy:0.822% \n",
      "Test accuracy:0.822% \n",
      "Test accuracy:0.822% \n",
      "Test accuracy:0.822% \n",
      "Test accuracy:0.822% \n",
      "Test accuracy:0.822% \n",
      "Test accuracy:0.822% \n",
      "Test accuracy:0.822% \n",
      "1.31 s ± 33.1 ms per loop (mean ± std. dev. of 10 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r10\n",
    "\n",
    "with single_thread():\n",
    "    evaluate(ptq_mlp, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что с текущей моделью статическая квантизация работает не так хорошо - качество немного просело, а значительного прироста по времени не наблюдается.\n",
    "\n",
    "Вместо простых схем квантизации, когда уже обученная модель квантуется, есть и более продвинутые схемы.\n",
    "\n",
    "### Квантизация в процессе обучения\n",
    "\n",
    "Этот метод заключается в том, что квантование происходит на каждом шаге градиентного спуска. Теоретически это должно делать более аккуратными относительно квантованных параметров и тем самым получать качество лучше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_mlp = QuantizedMLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantized_fit(model, train_loader, epoch_number=5):\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "    # Ничем особенным процесс обучения не отличается\n",
    "    # Добавляем конфигурацию, после чего подготавливаем модель для обучения с квантованием\n",
    "    # Модель внутри себя автоматически будет обновлять веса с учетом квантования\n",
    "    model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "    torch.quantization.prepare_qat(model, inplace=True)\n",
    "    \n",
    "    error = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epoch_number):\n",
    "        correct = 0\n",
    "        \n",
    "        for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "            var_X_batch = Variable(X_batch).float()\n",
    "            var_y_batch = Variable(y_batch)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(var_X_batch)\n",
    "            loss = error(output, var_y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            predicted = torch.max(output.data, 1)[1] \n",
    "            correct += (predicted == var_y_batch).sum()\n",
    "            if batch_idx % 200 == 0:\n",
    "                print('Epoch : {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Accuracy:{:.3f}%'.format(\n",
    "                    epoch, batch_idx*len(X_batch), len(train_loader.dataset), 100.*batch_idx / len(train_loader), loss.data, float(correct*100) / float(BATCH_SIZE*(batch_idx+1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 [0/60000 (0%)]\tLoss: 19.953337\t Accuracy:3.125%\n",
      "Epoch : 0 [6400/60000 (11%)]\tLoss: 1.015618\t Accuracy:68.470%\n",
      "Epoch : 0 [12800/60000 (21%)]\tLoss: 1.126122\t Accuracy:72.919%\n",
      "Epoch : 0 [19200/60000 (32%)]\tLoss: 0.382234\t Accuracy:74.698%\n",
      "Epoch : 0 [25600/60000 (43%)]\tLoss: 0.263492\t Accuracy:75.964%\n",
      "Epoch : 0 [32000/60000 (53%)]\tLoss: 0.520338\t Accuracy:76.795%\n",
      "Epoch : 0 [38400/60000 (64%)]\tLoss: 0.406278\t Accuracy:77.542%\n",
      "Epoch : 0 [44800/60000 (75%)]\tLoss: 0.436573\t Accuracy:78.083%\n",
      "Epoch : 0 [51200/60000 (85%)]\tLoss: 0.301557\t Accuracy:78.609%\n",
      "Epoch : 0 [57600/60000 (96%)]\tLoss: 0.401963\t Accuracy:79.013%\n",
      "Epoch : 1 [0/60000 (0%)]\tLoss: 0.458184\t Accuracy:87.500%\n",
      "Epoch : 1 [6400/60000 (11%)]\tLoss: 0.491694\t Accuracy:83.411%\n",
      "Epoch : 1 [12800/60000 (21%)]\tLoss: 0.751592\t Accuracy:83.619%\n",
      "Epoch : 1 [19200/60000 (32%)]\tLoss: 0.368394\t Accuracy:83.663%\n",
      "Epoch : 1 [25600/60000 (43%)]\tLoss: 0.291238\t Accuracy:83.770%\n",
      "Epoch : 1 [32000/60000 (53%)]\tLoss: 0.506257\t Accuracy:83.694%\n",
      "Epoch : 1 [38400/60000 (64%)]\tLoss: 0.487764\t Accuracy:83.800%\n",
      "Epoch : 1 [44800/60000 (75%)]\tLoss: 0.255858\t Accuracy:83.808%\n",
      "Epoch : 1 [51200/60000 (85%)]\tLoss: 0.246901\t Accuracy:83.916%\n",
      "Epoch : 1 [57600/60000 (96%)]\tLoss: 0.169946\t Accuracy:83.905%\n",
      "Epoch : 2 [0/60000 (0%)]\tLoss: 0.338304\t Accuracy:87.500%\n",
      "Epoch : 2 [6400/60000 (11%)]\tLoss: 0.423759\t Accuracy:84.670%\n",
      "Epoch : 2 [12800/60000 (21%)]\tLoss: 0.726992\t Accuracy:84.593%\n",
      "Epoch : 2 [19200/60000 (32%)]\tLoss: 0.236238\t Accuracy:84.593%\n",
      "Epoch : 2 [25600/60000 (43%)]\tLoss: 0.293010\t Accuracy:84.648%\n",
      "Epoch : 2 [32000/60000 (53%)]\tLoss: 0.552295\t Accuracy:84.606%\n",
      "Epoch : 2 [38400/60000 (64%)]\tLoss: 0.429808\t Accuracy:84.607%\n",
      "Epoch : 2 [44800/60000 (75%)]\tLoss: 0.463073\t Accuracy:84.741%\n",
      "Epoch : 2 [51200/60000 (85%)]\tLoss: 0.199725\t Accuracy:84.875%\n",
      "Epoch : 2 [57600/60000 (96%)]\tLoss: 0.099700\t Accuracy:84.823%\n",
      "Epoch : 3 [0/60000 (0%)]\tLoss: 0.392204\t Accuracy:84.375%\n",
      "Epoch : 3 [6400/60000 (11%)]\tLoss: 0.415213\t Accuracy:85.230%\n",
      "Epoch : 3 [12800/60000 (21%)]\tLoss: 0.687019\t Accuracy:85.240%\n",
      "Epoch : 3 [19200/60000 (32%)]\tLoss: 0.301354\t Accuracy:85.353%\n",
      "Epoch : 3 [25600/60000 (43%)]\tLoss: 0.321496\t Accuracy:85.385%\n",
      "Epoch : 3 [32000/60000 (53%)]\tLoss: 0.544561\t Accuracy:85.209%\n",
      "Epoch : 3 [38400/60000 (64%)]\tLoss: 0.409837\t Accuracy:85.327%\n",
      "Epoch : 3 [44800/60000 (75%)]\tLoss: 0.290352\t Accuracy:85.397%\n",
      "Epoch : 3 [51200/60000 (85%)]\tLoss: 0.204784\t Accuracy:85.450%\n",
      "Epoch : 3 [57600/60000 (96%)]\tLoss: 0.198320\t Accuracy:85.449%\n",
      "Epoch : 4 [0/60000 (0%)]\tLoss: 0.393940\t Accuracy:87.500%\n",
      "Epoch : 4 [6400/60000 (11%)]\tLoss: 0.427149\t Accuracy:85.557%\n",
      "Epoch : 4 [12800/60000 (21%)]\tLoss: 0.591541\t Accuracy:85.575%\n",
      "Epoch : 4 [19200/60000 (32%)]\tLoss: 0.237643\t Accuracy:85.566%\n",
      "Epoch : 4 [25600/60000 (43%)]\tLoss: 0.280864\t Accuracy:85.803%\n",
      "Epoch : 4 [32000/60000 (53%)]\tLoss: 0.463835\t Accuracy:85.761%\n",
      "Epoch : 4 [38400/60000 (64%)]\tLoss: 0.426036\t Accuracy:85.764%\n",
      "Epoch : 4 [44800/60000 (75%)]\tLoss: 0.251716\t Accuracy:85.834%\n",
      "Epoch : 4 [51200/60000 (85%)]\tLoss: 0.176312\t Accuracy:85.872%\n",
      "Epoch : 4 [57600/60000 (96%)]\tLoss: 0.158091\t Accuracy:85.801%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "quantized_fit(qa_mlp, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно заметить, что обучение идет немного дольше чем обычно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PostTrainedQuantizedMLP(\n",
       "  (linear1): QuantizedLinear(in_features=784, out_features=250, scale=35.50273513793945, zero_point=88, qscheme=torch.per_channel_affine)\n",
       "  (linear2): QuantizedLinear(in_features=250, out_features=100, scale=5.187557697296143, zero_point=71, qscheme=torch.per_channel_affine)\n",
       "  (linear3): QuantizedLinear(in_features=100, out_features=10, scale=0.8892423510551453, zero_point=74, qscheme=torch.per_channel_affine)\n",
       "  (quant): Quantize(scale=tensor([2.0079]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# После обучения с квантованием, фиксируем квантованные веса и параметры\n",
    "quantized_model = torch.quantization.convert(qa_mlp, inplace=False)\n",
    "quantized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.865% \n"
     ]
    }
   ],
   "source": [
    "evaluate(quantized_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На моем компьютере качество получилось даже чуть выше, чем у оригинальной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'229.746 KB'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_size(quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  -9,   13,  -17,  ...,  -34,  -34,  -65],\n",
       "        [   9,    4,  -45,  ...,  -13,  -24,   65],\n",
       "        [ -15,  -88,   29,  ...,  -72,   52,   53],\n",
       "        ...,\n",
       "        [  74,   50,  106,  ...,   75,  101,   31],\n",
       "        [ -58,  -68,   57,  ..., -106,  -79,    8],\n",
       "        [ -15,   58,  -44,  ...,  115,   37,  -63]], dtype=torch.int8)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_model.linear1.weight().int_repr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.876% \n",
      "Test accuracy:0.876% \n",
      "Test accuracy:0.876% \n",
      "Test accuracy:0.876% \n",
      "Test accuracy:0.876% \n",
      "Test accuracy:0.876% \n",
      "Test accuracy:0.876% \n",
      "Test accuracy:0.876% \n",
      "Test accuracy:0.876% \n",
      "Test accuracy:0.876% \n",
      "Test accuracy:0.876% \n",
      "1.3 s ± 6.6 ms per loop (mean ± std. dev. of 10 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r10\n",
    "\n",
    "with single_thread():\n",
    "    evaluate(quantized_model, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По итогу этих экспериментов можно сказать следующее - универсального метода квантизации не существует, в каждом конкретном случае нужно искать свой подход.\n",
    "\n",
    "Однако все схемы показывают стабильное уменьшение размера модели при небольшом изменении метрики качества и ускорении расчета предсказаний."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
